1
00:00:00,420 --> 00:00:03,930
您好，欢迎来到Arta Tauriel和Embree使用厕所。

2
00:00:03,930 --> 00:00:09,470
我们已经实现了适合训练集的多元回归模型。

3
00:00:09,480 --> 00:00:14,340
但是，当我们退后一步时，您认为这实际上是我们可以使用

4
00:00:14,340 --> 00:00:15,780
我们这里的数据集。

5
00:00:15,930 --> 00:00:21,290
您知道，因为在构建此模型时，我们实际上使用了所有自变量。

6
00:00:21,450 --> 00:00:26,640
但是，如果这些自变量中有一些在统计上具有高度意义

7
00:00:26,820 --> 00:00:34,290
对因变量利润产生重大影响或影响，而一些在统计上没有

8
00:00:34,290 --> 00:00:35,510
意义重大。

9
00:00:35,520 --> 00:00:41,700
这意味着，如果我们从道德世界中删除这些不粘手明显重要的变量，我们

10
00:00:41,700 --> 00:00:44,670
仍然会得到一些惊人的预测。

11
00:00:44,670 --> 00:00:51,210
因此，本教程的目标是实际找到一个最佳的独立变量团队，以便

12
00:00:51,210 --> 00:00:57,570
团队的每个自变量对因变量利润有很大影响，即

13
00:00:57,720 --> 00:01:03,060
团队中的每个独立变量都是一个有力的预测器，具有很高的统计意义

14
00:01:03,270 --> 00:01:08,630
肯定会对因变量利润产生影响，并且这种影响可能是积极的。

15
00:01:08,700 --> 00:01:12,010
也就是说，自变量增加了1个单位。

16
00:01:12,030 --> 00:01:14,710
利润将增加或可能为负。

17
00:01:14,760 --> 00:01:17,800
也就是说，自变量增加了1个单位。

18
00:01:17,820 --> 00:01:24,060
利润将减少，因此我们将划分构建最佳模型的最后一步

19
00:01:24,060 --> 00:01:29,890
在我们现在所拥有的第一个教程中，将向后消除方法分为两个教程。

20
00:01:29,910 --> 00:01:35,460
我将向您介绍后门消除算法，而无需完成它直到最后

21
00:01:35,460 --> 00:01:35,960
。

22
00:01:36,030 --> 00:01:42,540
这意味着在Tauriel结束时，您将获得一份作业，其中包括完成

23
00:01:42,540 --> 00:01:44,370
我们从向后淘汰开始。

24
00:01:44,400 --> 00:01:49,260
因此，我相信我们不会有任何问题，因为我将带您逐步了解反向操作的介绍。

25
00:01:49,290 --> 00:01:55,290
消除，使您可以了解所有内容并拥有完成作业所需的所有工具。

26
00:01:55,500 --> 00:02:00,240
然后在下一个教程中，我将为您提供此作业的解决方案，我们将一起完成

27
00:02:00,240 --> 00:02:01,960
向后淘汰。

28
00:02:02,190 --> 00:02:03,730
所以我希望你很兴奋。

29
00:02:03,960 --> 00:02:04,980
让我们现在开始。

30
00:02:04,980 --> 00:02:07,320
向后淘汰。

31
00:02:07,440 --> 00:02:12,690
因此，对于那些遵循Python教程的人来说，您会发现它实际上还多了一些

32
00:02:12,690 --> 00:02:19,020
这很简单，因为在PIF反托里游戏中，我们不得不使用另一个库和另一个回归模型

33
00:02:19,170 --> 00:02:21,830
实施向后淘汰。

34
00:02:22,020 --> 00:02:27,870
这次，我们将简单地采用在此创建的模型，并将使用此惊人的功能

35
00:02:27,870 --> 00:02:35,400
我们的摘要，返回了大量的统计信息，可以帮助使我们的模型更加完善

36
00:02:35,400 --> 00:02:36,570
强大的。

37
00:02:36,570 --> 00:02:42,720
然后与Python中一样，我们将非常简单地执行一些复制粘贴，直到最终

38
00:02:42,720 --> 00:02:45,260
我们的自变量团队。

39
00:02:45,330 --> 00:02:46,500
因此，让我们这样做。

40
00:02:46,620 --> 00:02:48,470
您将看到它确实非常快捷。

41
00:02:48,480 --> 00:02:50,230
我们将非常有效地做到这一点。

42
00:02:50,340 --> 00:02:56,160
第一步是实际采用我们的模型，因为正如我刚才提到的，

43
00:02:56,160 --> 00:03:00,110
使用相同的模型来实现向后消除。

44
00:03:00,180 --> 00:03:06,660
所以在这里我只是复制了模型，然后将其粘贴到此处，现在我们将要更改两个

45
00:03:06,660 --> 00:03:07,590
东西。

46
00:03:07,620 --> 00:03:13,470
首先要注意的是，这代表了所有独立的

47
00:03:13,470 --> 00:03:20,130
变量，我们将编写所有由加号分隔的独立变量，因为您知道

48
00:03:20,130 --> 00:03:26,550
向后消除的原理是，我们将删除统计上没有的每个自变量

49
00:03:26,550 --> 00:03:28,600
一件一件的重要。

50
00:03:28,650 --> 00:03:34,950
因此，我们需要在此处编写每个独立变量，以便在复制时在此处粘贴此模型

51
00:03:35,220 --> 00:03:40,990
我们只需要从此等式中删除非统计上显着的变量。

52
00:03:41,280 --> 00:03:41,490
好。

53
00:03:41,490 --> 00:03:42,660
因此，让我们首先这样做。

54
00:03:42,660 --> 00:03:47,340
我将在这里使用我们的数据集来查看自变量。

55
00:03:47,340 --> 00:03:49,690
因此，第一个自变量是Arty's Ben。

56
00:03:49,710 --> 00:03:51,450
所以我们在这里添加它。

57
00:03:51,720 --> 00:03:58,650
所以我们的点D是提醒您，这里有一个点，因为这个独立的原始名称

58
00:03:58,650 --> 00:04:04,510
变量是我们的空间D，这里只是用点代替了空间。

59
00:04:04,800 --> 00:04:09,660
因此，很高兴知道您是否正在使用are以及是否有一些带有空格的数据集

60
00:04:09,660 --> 00:04:11,780
列名，然后单击确定。

61
00:04:11,790 --> 00:04:14,670
那点叫什么呢。

62
00:04:14,700 --> 00:04:18,070
另一个女儿意味着所有的空间都是零花钱。

63
00:04:18,210 --> 00:04:20,460
好的，这是第一个独立的可行方案。

64
00:04:20,460 --> 00:04:22,170
现在让我们添加第二个。

65
00:04:22,170 --> 00:04:24,770
因此，我们需要将它们分开。

66
00:04:24,870 --> 00:04:25,590
好。

67
00:04:25,770 --> 00:04:27,360
第二个是什么。

68
00:04:27,450 --> 00:04:29,440
第二个是行政管理。

69
00:04:29,520 --> 00:04:30,420
好。

70
00:04:30,810 --> 00:04:35,670
因此，毫无疑问，这里的拼写正确就可以了。

71
00:04:35,670 --> 00:04:37,860
行政加

72
00:04:40,900 --> 00:04:41,860
营销支出

73
00:04:44,830 --> 00:04:52,360
加号，我们有一个倒数第二个非常独立的州，所以我们不需要创建一个

74
00:04:52,460 --> 00:05:00,220
虚拟变量，就像我们在Python中所做的那样，因为请记住我们在这里使用了编码

75
00:05:00,370 --> 00:05:07,570
这个状态分类变量分为一到三个因子，没有关系顺序

76
00:05:07,600 --> 00:05:09,630
在这些类别之间。

77
00:05:09,670 --> 00:05:11,000
所以一切都很好。

78
00:05:11,050 --> 00:05:13,540
我们不需要创建任何虚拟变量。

79
00:05:13,610 --> 00:05:15,120
这就是我们的美丽。

80
00:05:15,340 --> 00:05:21,370
因此，这就是说我们不需要求和以分离出虚拟变量，我们可以采用原始的独立变量。

81
00:05:21,370 --> 00:05:22,560
可变状态。

82
00:05:22,870 --> 00:05:25,930
好的，正如我提到的，我们要在此处更改两件事。

83
00:05:25,930 --> 00:05:31,750
第一件事是用加号分隔的所有这些自变量替换点。

84
00:05:31,930 --> 00:05:36,400
现在，我们想做的第二件事不是强制性的，只是因为我想

85
00:05:36,400 --> 00:05:44,420
使用所有数据集来查看相关性就是用我们的数据集代替您的训练集

86
00:05:44,430 --> 00:05:45,150
。

87
00:05:45,190 --> 00:05:46,750
因此，这不是强制性的。

88
00:05:46,750 --> 00:05:52,620
我们实际上可以使用训练集进行向后消除，但我们只是获取整个数据

89
00:05:52,620 --> 00:05:58,660
进行设置，以获取有关哪些自变量在统计上有意义的完整信息

90
00:05:58,900 --> 00:06:02,060
以及哪些自变量不合适。

91
00:06:02,080 --> 00:06:04,390
现在实际上我们已经准备好了。

92
00:06:04,450 --> 00:06:09,670
我们只需要使用我们之前实际已经使用过的摘要功能即可。

93
00:06:09,850 --> 00:06:14,830
没有什么比使用摘要功能更简单了，我们只需要使用摘要功能

94
00:06:14,830 --> 00:06:19,520
在这里，然后在括号中，我们输入的是回归变量。

95
00:06:19,600 --> 00:06:20,470
这里是。

96
00:06:20,500 --> 00:06:26,050
现在已经准备好了，我们实际上已经准备好开始向后淘汰的第一步

97
00:06:26,050 --> 00:06:26,080
。

98
00:06:26,080 --> 00:06:31,930
好说向后消除让我们看看直觉中基里尔看到的幻灯片

99
00:06:31,930 --> 00:06:34,040
教程和幻灯片。

100
00:06:34,150 --> 00:06:37,680
因此，让我们快速提醒一下此处的五个步骤。

101
00:06:37,690 --> 00:06:43,600
因此，第一步是选择显着性水平，该水平是我们P值的阈值，以便

102
00:06:43,750 --> 00:06:49,420
如果自变量的p值低于显着性水平，则该自变量

103
00:06:49,430 --> 00:06:55,210
留在模型中，并且自变量的p值高于显着性水平

104
00:06:55,510 --> 00:06:57,120
那么它将不会停留在模型中。

105
00:06:57,130 --> 00:06:58,650
我们将其删除。

106
00:06:59,000 --> 00:07:04,480
所以第一步只是选择一个意义，我们在这里不必对独立

107
00:07:04,480 --> 00:07:04,900
变量。

108
00:07:04,900 --> 00:07:09,230
我们只需要选择一个，我们将选择5％0.05。

109
00:07:09,480 --> 00:07:09,940
好的。

110
00:07:09,960 --> 00:07:15,040
现在，第二步到第二步是使整个模型与所有可能的预测变量匹配。

111
00:07:15,040 --> 00:07:20,500
因此，这就是我们刚刚通过将所有自变量纳入回归变量来了解的内容

112
00:07:20,500 --> 00:07:26,390
使用实际上使整个模型与所有可能的预测变量匹配的LN函数

113
00:07:26,390 --> 00:07:28,130
所有自变量。

114
00:07:28,420 --> 00:07:30,310
好的，做完了。

115
00:07:30,370 --> 00:07:35,780
现在，第三步是什么第三步是查看具有最高P值的预测变量。

116
00:07:36,100 --> 00:07:39,940
因此，由于我们的汇总功能，我们将找到它。

117
00:07:40,240 --> 00:07:45,360
如果p值高于显着性水平（即高于5％），则

118
00:07:45,360 --> 00:07:46,520
我们将转到步骤4。

119
00:07:46,720 --> 00:07:52,950
如果不是这种情况，我们的模型实际上已经准备就绪，但是请不要担心那不会那么快。

120
00:07:53,080 --> 00:07:59,170
因此，实际上我们假设发现最高值高于5％的显着性水平

121
00:07:59,590 --> 00:08:05,110
那么我们需要继续执行第4步，而第4步实际上是删除

122
00:08:05,110 --> 00:08:07,000
具有最高的价值。

123
00:08:07,420 --> 00:08:13,270
一旦我们删除了预测变量，我们就可以继续进行第五步了

124
00:08:13,270 --> 00:08:14,340
这个可行。

125
00:08:14,410 --> 00:08:20,440
这就是为什么您知道我们将所有独立变量一一写成，用加号隔开的原因，因为

126
00:08:20,740 --> 00:08:26,950
你知道一旦我们到达第五步，我们将复制粘贴回归或摘要功能

127
00:08:27,160 --> 00:08:33,960
并从回归中删除价值最高的圣经，或建立新的回归或

128
00:08:33,970 --> 00:08:35,310
没有这个可行性。

129
00:08:35,350 --> 00:08:43,360
这将适合该模型而不会导致无效，一旦完成，我们将返回此处的第3步

130
00:08:43,480 --> 00:08:49,480
同样的途径，我们将再次寻找新的独立变量

131
00:08:49,480 --> 00:08:54,130
没有我们刚刚删除的自变量的自变量团队。

132
00:08:54,130 --> 00:08:58,990
因此，我们将寻找具有最高值的自变量，并说出p值是否

133
00:08:58,990 --> 00:09:04,070
高于显着性水平，我们将转到步骤4，否则我们的模型已准备就绪。

134
00:09:04,480 --> 00:09:11,920
因此，让我们执行此操作，我们已经选择了5％的显着水平，并且已经为第一个步骤完成了第1步

135
00:09:11,920 --> 00:09:14,940
步骤2我们实际上将所有可能的预测变量拟合到模型中。

136
00:09:14,950 --> 00:09:17,920
好吧，我们当然需要执行代码。

137
00:09:18,100 --> 00:09:23,800
现在我们进入第三步，其中包括寻找

138
00:09:23,800 --> 00:09:25,100
具有最高的P值。

139
00:09:25,300 --> 00:09:26,490
因此，让我们立即执行此操作。

140
00:09:27,290 --> 00:09:27,880
好。

141
00:09:28,010 --> 00:09:35,550
因此，正如我刚才提到的，我们需要执行此操作以使用所有独立变量构建回归器

142
00:09:35,550 --> 00:09:35,580
。

143
00:09:35,580 --> 00:09:40,940
好吧，实际上我们并不需要执行此操作，因为我们实际上在这里执行了此代码部分

144
00:09:40,950 --> 00:09:43,370
建立完全相同的回归或。

145
00:09:43,400 --> 00:09:49,220
但是只是为了完成本教程中的所有步骤，让我们再次执行该操作，不会导致任何问题。

146
00:09:49,230 --> 00:09:49,870
问题。

147
00:09:49,980 --> 00:09:53,950
所以我要按Command Control按Enter执行。

148
00:09:54,090 --> 00:09:54,960
现在我们开始。

149
00:09:55,080 --> 00:10:01,890
使用不同的语法会再次产生相同的遗憾，这是因为我们要删除不重要的内容

150
00:10:01,880 --> 00:10:04,090
钟一一不变。

151
00:10:04,380 --> 00:10:05,020
大。

152
00:10:05,100 --> 00:10:07,830
这样就完成了第二步。

153
00:10:08,120 --> 00:10:13,170
现在让我们继续进行第三步，寻找具有最高的自变量

154
00:10:13,170 --> 00:10:20,670
P值，为此，我们将选择内部包含回归器的摘要函数，然后按

155
00:10:20,670 --> 00:10:23,230
命令和控制我们并执行。

156
00:10:23,970 --> 00:10:26,610
让我们将其向上移动一点。

157
00:10:26,700 --> 00:10:32,610
因此，当我们要构建健壮的模型时，这些信息是非常重要的信息。

158
00:10:32,820 --> 00:10:38,250
这不仅要感谢此处有关P值的信息，这将有助于选择最佳值。

159
00:10:38,250 --> 00:10:43,080
团队的自变量，因为在下面我们也有这个多个R平方。

160
00:10:43,110 --> 00:10:49,320
我只添加R平方，这将帮助我们建立比我们要建立的模型更强大的模型

161
00:10:49,320 --> 00:10:54,420
在接下来的两个教程中进行介绍，因为在本部分的末尾有一个名为“评估”的部分

162
00:10:54,410 --> 00:10:58,520
建模性能以实际改善模型性能。

163
00:10:58,760 --> 00:11:03,990
在这部分中，我们实际上将使用多个R平方和调整后的R平方来最终确定我们的

164
00:11:03,990 --> 00:11:09,360
迈向最强大的模型，您将完全理解为什么在本部分结尾

165
00:11:10,080 --> 00:11:15,600
但是现在让我们关注P值，所以p值实际上在此列中，然后是

166
00:11:15,610 --> 00:11:19,080
实际上是查看统计意义的捷径。

167
00:11:19,110 --> 00:11:24,080
这是这最后一列，而此Nasscom没有名称，但您需要查看星星

168
00:11:24,090 --> 00:11:30,980
这里是因为提醒一下，该值越是低于5％，则显着性水平就越高

169
00:11:30,990 --> 00:11:36,240
自变量对于因变量利润和

170
00:11:36,240 --> 00:11:42,360
值高于显着性水平5％时，统计学上的显着性越低

171
00:11:42,360 --> 00:11:44,010
自变量将是。

172
00:11:44,310 --> 00:11:50,970
简而言之，值越低，您的自变量对受抚养人的影响越大

173
00:11:50,970 --> 00:11:57,180
变量，该值越高，对自变量的影响越小

174
00:11:57,360 --> 00:11:58,760
在因变量上。

175
00:11:59,150 --> 00:12:06,830
这里的提醒是，如果P值介于0％和0.1％之间，则

176
00:12:06,840 --> 00:12:13,530
3星，表示与p值在0.1％之间时相比，具有很高的统计意义。

177
00:12:13,680 --> 00:12:19,620
1％则是2星，这表示它在统计上非常显着，但不那么显着

178
00:12:19,620 --> 00:12:21,060
比有三颗星的时候。

179
00:12:21,060 --> 00:12:27,170
然后，如果您让我们获得的百分比介于1％和5％之间，那么这在统计上就非常重要。

180
00:12:27,180 --> 00:12:31,810
那就是你的自变量仍然对因变量有一些好的影响。

181
00:12:32,150 --> 00:12:36,770
然后，如果该值在5％到10％之间，则为点。

182
00:12:36,840 --> 00:12:42,780
这意味着在一定程度上具有统计意义，

183
00:12:42,780 --> 00:12:50,370
变量对您的因变量有一定影响，但绝对不如其他独立变量大

184
00:12:50,370 --> 00:12:53,980
这些变量在这里特别针对此类别。

185
00:12:54,330 --> 00:12:58,340
最后，如果您的p值介于10％和1之间。

186
00:12:58,500 --> 00:13:02,540
好吧，绝对没有统计意义。

187
00:13:02,550 --> 00:13:05,320
因此，这意味着我们首先在这里观察到。

188
00:13:05,490 --> 00:13:12,870
好吧，我们可以看到R＆D支出在统计上非常重要，但其余的似乎并不多

189
00:13:12,870 --> 00:13:13,670
重大。

190
00:13:13,830 --> 00:13:20,540
但是，让我们等待完成向后淘汰，以了解我们的最终团队是否真正

191
00:13:20,550 --> 00:13:26,250
仅由Ardie支出组成，因为在此处删除了一些自变量，这将删除一些

192
00:13:26,250 --> 00:13:30,390
想要一些自变量的可能偏差被消除。

193
00:13:30,380 --> 00:13:35,870
实际上，我们可以找到一个自变量，其统计意义远比它看起来的要重要。

194
00:13:35,880 --> 00:13:37,280
刚开始。

195
00:13:37,290 --> 00:13:39,030
这是这里的第一步。

196
00:13:39,260 --> 00:13:40,610
因此，让我们了解一下。

197
00:13:40,640 --> 00:13:45,730
实际上，您会发现自己，因为这将是家庭作业的主题

198
00:13:45,740 --> 00:13:45,800
。

199
00:13:45,870 --> 00:13:51,210
但是请放心，我将引导您完成向后淘汰的第一步，您将完成

200
00:13:51,210 --> 00:13:52,020
它自己。

201
00:13:52,160 --> 00:13:56,440
然后，下一个教程当然将提供解决方案，我们将一起完成它。

202
00:13:56,550 --> 00:13:59,180
因此，我期待看到我们能否获得相同的结果。

203
00:13:59,580 --> 00:14:02,970
好的，现在让我们继续进行向后消除。

204
00:14:03,060 --> 00:14:05,270
因此，请记住我们处于第三步。

205
00:14:05,340 --> 00:14:11,520
第三步实际上是寻找具有最高P值的自变量，我们可以

206
00:14:11,520 --> 00:14:12,860
很容易找到它。

207
00:14:12,890 --> 00:14:20,010
实际上就是这一行，因为确实是P值打开99，即99％。

208
00:14:20,070 --> 00:14:22,400
因此，这实际上是一个很高的价值。

209
00:14:22,560 --> 00:14:26,320
而且我们远远超过了5％的显着性水平。

210
00:14:26,520 --> 00:14:33,700
因此，状态状态2的这个虚拟变量绝对没有统计意义。

211
00:14:33,720 --> 00:14:37,720
它对因变量利润绝对没有影响。

212
00:14:37,910 --> 00:14:43,240
顺便说一下，我们还发现阶段3的p值为94％。

213
00:14:43,400 --> 00:14:49,920
而且，如果我们将状态移到井底，这是不可能的，那么价值将降低到5％以下

214
00:14:49,910 --> 00:14:51,240
显着性水平。

215
00:14:51,240 --> 00:14:57,330
所以我们实际上也可以删除该状态的三个自变量，因为显然状态具有

216
00:14:57,330 --> 00:15:01,200
对因变量利润无影响。

217
00:15:01,400 --> 00:15:06,770
所以我实际上会在这里做一些捷径，而不是删除自变量

218
00:15:06,780 --> 00:15:13,130
具有最高P值即状态2的值实际上将删除这两个状态的虚拟变量

219
00:15:13,590 --> 00:15:17,410
因为绝对状态在统计上并不重要。

220
00:15:17,580 --> 00:15:21,870
因此，让我们执行此操作，让我们从方程式中删除状态变量。

221
00:15:21,920 --> 00:15:24,440
所以我要把它放下来。

222
00:15:25,290 --> 00:15:33,350
好的，正如我告诉您的那样，这非常简单，我们将复制并粘贴到此处。

223
00:15:33,380 --> 00:15:34,740
因此，我们现在要做的是。

224
00:15:34,880 --> 00:15:43,490
我们只需要在此处从方程式中删除变量中的状态，然后完成

225
00:15:43,640 --> 00:15:50,610
第四步，因为如果回到幻灯片，则第四步实际上是删除预测变量等级，

226
00:15:50,610 --> 00:15:55,830
现在我们可以进入第五步，它适合没有独立变量的多元回归模型

227
00:15:55,830 --> 00:15:58,240
我们刚刚删除的可行状态。

228
00:15:58,560 --> 00:15:58,810
好。

229
00:15:58,830 --> 00:16:07,740
因此，我们删除了状态，因此第4步完成了，现在第5步实际上是在没有这种独立性的情况下拟合模型

230
00:16:07,740 --> 00:16:09,420
我们刚刚删除的可行状态。

231
00:16:09,410 --> 00:16:10,410
因此，让我们这样做。

232
00:16:10,400 --> 00:16:15,520
我们只需要选择此press命令并控制它并执行即可。

233
00:16:15,600 --> 00:16:21,050
在这里，我们的新回归者已经准备就绪，没有独立于国家的可行性。

234
00:16:21,060 --> 00:16:26,460
因此，现在我们有一个由三个自变量组成的团队，我们拭目以待。

235
00:16:26,460 --> 00:16:27,810
被踢出球队。

236
00:16:28,050 --> 00:16:33,770
说到这里，我将让您独自一人做作业，但不要担心您

237
00:16:33,780 --> 00:16:35,800
在下一个教程中将有解决方案。

238
00:16:35,900 --> 00:16:41,360
但是，真正尝试自己实施此完全反向淘汰直到最后。

239
00:16:41,700 --> 00:16:44,310
看看我们是否能获得相同的结果将非常有趣。

240
00:16:44,310 --> 00:16:48,070
实际上，在消除Backwell的过程中确实要做出一个决定。

241
00:16:48,120 --> 00:16:54,540
因此，我很想知道您是如何做出决定的，因为这两种解决方案实际上都是

242
00:16:54,570 --> 00:16:58,190
很好，我们将在解决方案中进行讨论。

243
00:16:58,470 --> 00:17:00,870
祝你功课顺利。

244
00:17:01,080 --> 00:17:02,180
您将看到它会很好。

245
00:17:02,180 --> 00:17:08,200
因此，基本上，您唯一需要做的就是跟随此向后消除幻灯片，等等。

246
00:17:08,200 --> 00:17:09,930
挪威国家石油公司决定这样做。

247
00:17:10,050 --> 00:17:16,430
现在，您可以看到现在的第五步，您必须回到第三步，并重复执行第三步和第四步

248
00:17:16,450 --> 00:17:24,020
五个与我们刚做的完全一样，直到您发现最高值不高于含义

249
00:17:24,030 --> 00:17:24,510
水平。

250
00:17:24,680 --> 00:17:27,940
在这种情况下，您的模型将准备就绪。

251
00:17:28,400 --> 00:17:28,670
好。

252
00:17:28,670 --> 00:17:31,110
因此，我期待在下一个教程中见到您。

253
00:17:31,110 --> 00:17:37,620
我期待与您比较我的结果，我相信一切都会好的。

254
00:17:37,660 --> 00:17:40,280
向后淘汰对我们来说非常实用。

255
00:17:40,430 --> 00:17:43,450
实际上，这很有趣且容易完成。

256
00:17:43,770 --> 00:17:45,570
因此，谢谢您收看这个故事。

257
00:17:45,740 --> 00:17:49,100
我期待与您在下一个解决方案中见面。

258
00:17:49,130 --> 00:17:51,080
在那之前和机器学习有关。

