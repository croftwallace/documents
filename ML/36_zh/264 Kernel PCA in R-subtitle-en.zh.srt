1
00:00:00,300 --> 00:00:02,580
您好，欢迎来到Tauriel的艺术领域。

2
00:00:02,640 --> 00:00:06,240
因此，现在我们知道了如何实现特征提取技术。

3
00:00:06,240 --> 00:00:11,580
这些是PCa和LDA，但是这些在Lynnie上起作用的特征提取技术是有问题的。

4
00:00:11,580 --> 00:00:14,200
那就是数据是线性可分离的。

5
00:00:14,550 --> 00:00:19,260
在本节中，我们将看到一种新的特征提取技术，但这一次适用于

6
00:00:19,260 --> 00:00:23,150
数据是非线性可分离的非线性问题。

7
00:00:23,160 --> 00:00:25,790
因此，该技术称为PCA上校。

8
00:00:26,160 --> 00:00:32,130
PCa上校是PCa的最新版本，我们在其中使用内核将数据映射到更高维度

9
00:00:32,130 --> 00:00:32,570
特技。

10
00:00:32,670 --> 00:00:37,320
然后从那里提取一些新的主要成分，我们将看到它如何管理

11
00:00:37,320 --> 00:00:39,320
处理非线性问题。

12
00:00:39,570 --> 00:00:44,970
因此，我们不会像上一节那样处理一个数据集。

13
00:00:45,210 --> 00:00:50,250
但是我们将使用与第3部分分类中使用的数据集相同的数据集，因为现在我们

14
00:00:50,250 --> 00:00:50,960
需要视觉效果。

15
00:00:51,000 --> 00:00:58,170
我们需要清楚地看到发生了什么，我们需要看到当前的PCA如何设法提取一些新的独立

16
00:00:58,170 --> 00:01:03,360
即使问题是非线性的或数据不是非线性的，也可以对主要成分进行变量

17
00:01:03,360 --> 00:01:04,510
线性可分离的

18
00:01:04,710 --> 00:01:10,770
在社交网络的第3部分中使用的该数据集将记住

19
00:01:10,770 --> 00:01:15,600
这是一个非线性问题，因为非线性分类器的性能要好得多。

20
00:01:15,600 --> 00:01:21,280
因此，让我们采用此数据集，让我们像PCA一样看一下它将如何处理被提名人。

21
00:01:21,300 --> 00:01:24,780
因此，让我们在工作目录文件夹中找到此数据集。

22
00:01:24,810 --> 00:01:26,380
因此，我们将完成任务。

23
00:01:26,380 --> 00:01:29,410
它是一个文件夹，是九维缩减的一部分。

24
00:01:29,640 --> 00:01:33,900
这是第9部分内核PCA的最后一部分。

25
00:01:33,950 --> 00:01:37,830
这就是您要设置为工作目录的文件夹，请确保您具有社交网络

26
00:01:37,830 --> 00:01:38,980
以及。

27
00:01:39,120 --> 00:01:44,220
如果是这样的话，您真的可以在附近单击更多以将文件夹设置为工作目录。

28
00:01:44,500 --> 00:01:50,700
现在我们要做的是采用这个逻辑回归模型，因为您知道这个逻辑

29
00:01:50,700 --> 00:01:56,340
回归模型是线性分类器，因此不适合我们的问题，因为

30
00:01:56,550 --> 00:01:58,610
我们的数据不是线性可分离的。

31
00:01:58,620 --> 00:02:05,220
所以我们要做的是在这里进行线性分类，但是我们将应用内核Pitsea

32
00:02:05,220 --> 00:02:09,990
在其中查看内核PCA将如何保存情况。

33
00:02:10,020 --> 00:02:12,930
因此，即使我们应用线性模型，您也会看到这一点。

34
00:02:13,050 --> 00:02:19,380
好了，感谢PCa上校，我们设法非线性地提取了适用于此的新主要成分

35
00:02:19,380 --> 00:02:20,440
可分离的数据。

36
00:02:20,550 --> 00:02:22,840
好吧，您将看到它将获得惊人的结果。

37
00:02:23,010 --> 00:02:29,180
现在，让我们从上到下复制整个模型。

38
00:02:29,460 --> 00:02:32,990
让我们在代码PCA文件中面对它。

39
00:02:33,140 --> 00:02:33,960
好吧。

40
00:02:34,200 --> 00:02:40,620
现在，基本上我们唯一要做的就是在正确的位置应用代码PCa。

41
00:02:40,620 --> 00:02:45,840
但是在我们这样做之前，我想再次想像一下为什么这种线性模型不合适

42
00:02:45,990 --> 00:02:47,720
对于此数据集。

43
00:02:47,730 --> 00:02:53,090
所以我们要做的是从这里开始进行所有操作，因为您知道这将使培训形象化

44
00:02:53,090 --> 00:02:56,820
通过绘制预测区域和预测边界得出结果。

45
00:02:56,940 --> 00:03:02,940
因此，我们将从头到尾介绍所有内容，您知道重要的数据集已应用了预处理

46
00:03:02,940 --> 00:03:08,500
阶段过滤逻辑回归到训练集，并最终绘制解决方案。

47
00:03:08,520 --> 00:03:14,040
因此，让我们做起来让我们非常快速地再次可视化它，这将使我们有动力进行应用。

48
00:03:14,040 --> 00:03:17,410
皮切特上校，我们走了。

49
00:03:17,470 --> 00:03:22,180
所有操作都正确执行，以提醒您一点是真实的观察结果。

50
00:03:22,180 --> 00:03:28,420
那就是我们社交网络中的真实客户，由他们的年龄和估计的工资代表。

51
00:03:28,420 --> 00:03:33,920
这就是我们真正的观察点，而我们的预测都由这些区域来表示。

52
00:03:33,940 --> 00:03:36,240
这里的红色区域和这里的绿色区域。

53
00:03:36,310 --> 00:03:42,580
基本上，这个红色区域是我们的模型预测客户不会点击广告的地方

54
00:03:42,940 --> 00:03:47,680
这个绿色区域就是模型预测客户点击该区域的区域

55
00:03:47,680 --> 00:03:49,870
广告并购买SUV。

56
00:03:49,900 --> 00:03:56,500
所以请记住问题是，这里的直线实际上是生成的预测边界

57
00:03:56,500 --> 00:03:58,200
通过逻辑回归模型。

58
00:03:58,360 --> 00:04:03,490
但是由于逻辑回归模型是线性分类器，因此这里必须是一条直线

59
00:04:03,490 --> 00:04:08,320
分离数据，因此请记住问题是它无法做出某种曲线

60
00:04:08,320 --> 00:04:14,360
在这里捕获这些应该位于绿色区域中的绿色用户，现在他们位于红色区域中。

61
00:04:14,460 --> 00:04:19,420
因此，这清楚地表明了我们的数据实际上无法分离的事实，因为我们可以

62
00:04:19,420 --> 00:04:25,270
看到这里的预测边界起着分隔符的作用，应该分开

63
00:04:25,360 --> 00:04:26,360
这两个班。

64
00:04:26,590 --> 00:04:31,540
好吧，它不能正确地将两个类分开，因为您可以看到这些用户不在正确的位置

65
00:04:31,540 --> 00:04:37,690
区域，所以现在我们要做的不是像第三部分那样做非线性分类器

66
00:04:37,840 --> 00:04:41,800
你知道我们什么时候为他建立SBM上校的基础决策树的。

67
00:04:41,950 --> 00:04:48,400
好吧，我们现在要做的是黑色的PCA上校，以便我们将一条直线

68
00:04:48,400 --> 00:04:53,470
分隔符作为仍将作为预测的线性分类器的预测边界

69
00:04:53,470 --> 00:04:55,590
逻辑回归模型的边界。

70
00:04:55,840 --> 00:05:02,320
但是，由于我们将应用内核PCA，所以这将设法应用一些技巧，

71
00:05:02,320 --> 00:05:08,800
实际上是将数据映射到更高维度，然后应用PCA提取新数据的核心技巧。

72
00:05:08,800 --> 00:05:13,050
组件将是可以解释最多变化形式的新维度。

73
00:05:13,090 --> 00:05:19,240
但多亏了Treyc上校，您会发现我们将设法获得一些新的维度，

74
00:05:19,240 --> 00:05:24,420
即使通过线性分类器（如逻辑回归），数据也将是线性可分离的。

75
00:05:24,580 --> 00:05:25,580
因此，让我们现在看看。

76
00:05:25,600 --> 00:05:27,250
我等不及要向您展示。

77
00:05:27,280 --> 00:05:28,540
我要关闭这个。

78
00:05:28,600 --> 00:05:32,920
现在让我们在正确的位置应用内核PCa。

79
00:05:32,950 --> 00:05:34,810
所以您已经知道这个位置是什么。

80
00:05:34,810 --> 00:05:37,600
实际上与以前没有什么不同。

81
00:05:37,600 --> 00:05:43,630
我们需要在数据预处理阶段之前和应用分类器之前就应用内核PCA。

82
00:05:43,630 --> 00:05:46,080
例如逻辑回归到我们的培训中心。

83
00:05:46,150 --> 00:05:56,420
所以基本上我们需要在这里应用当前的PCA，因此在美国使用Kerno PCa，然后我们开始

84
00:05:56,610 --> 00:05:57,560
我们开始做吧。

85
00:05:57,900 --> 00:05:58,190
好吧。

86
00:05:58,200 --> 00:06:03,210
因此，首先我们需要安装一个名为current lab的新软件包，我认为我们不会安装

87
00:06:03,210 --> 00:06:04,800
之前，让我们现在就开始吧。

88
00:06:05,070 --> 00:06:08,850
因此，我们使用命令安装软件包。

89
00:06:08,880 --> 00:06:09,550
开始了。

90
00:06:09,690 --> 00:06:12,450
并在报价kerflop。

91
00:06:12,510 --> 00:06:13,080
好吧。

92
00:06:13,320 --> 00:06:15,650
所以我想我已经安装了它。

93
00:06:15,720 --> 00:06:17,130
让我们来看看。

94
00:06:17,130 --> 00:06:20,410
因此，这里是当前基于内核的实验室。

95
00:06:20,520 --> 00:06:21,980
所以我不会安装它。

96
00:06:21,990 --> 00:06:25,190
但是，如果您要执行此操作，则只需执行此行即可。

97
00:06:25,260 --> 00:06:27,810
因此，我仅将这一行作为评论。

98
00:06:27,840 --> 00:06:28,540
开始了。

99
00:06:28,680 --> 00:06:35,290
但是，由于未导入，因此我将使用库命令Kurla导入。

100
00:06:35,640 --> 00:06:36,290
好吧。

101
00:06:36,390 --> 00:06:38,900
那将导入它。

102
00:06:39,030 --> 00:06:39,670
好吧。

103
00:06:39,680 --> 00:06:41,170
Kurla将导入它。

104
00:06:41,280 --> 00:06:44,590
现在让我们开始应用内核PCA。

105
00:06:44,940 --> 00:06:51,540
因此，请求PCa和LDA，我们将首先创建一个对象，该对象将是

106
00:06:51,540 --> 00:06:55,990
我们将用于将原始数据集转换为该数据集。

107
00:06:56,010 --> 00:07:02,700
在使用了内核技巧之后，我们将这个对象称为K PCa，然后等于，然后就是

108
00:07:02,700 --> 00:07:06,300
使用将创建此内核对象的函数。

109
00:07:06,540 --> 00:07:13,230
因此，此函数也是K PCa，然后加上括号，然后输入不同的参数。

110
00:07:13,230 --> 00:07:18,390
因此，让我们将其删除，让我们按一下此处以查看参数。

111
00:07:18,810 --> 00:07:24,920
因此，第一个参数是X，实际上是数据矩阵或描述模型的公式。

112
00:07:24,930 --> 00:07:30,060
在这里，我将向您提供一些技巧，以非常简单有效地描述模型

113
00:07:30,060 --> 00:07:32,150
只需在此处输入直到。

114
00:07:32,230 --> 00:07:38,700
那样就足以让KPC函数了解公式是什么，因为

115
00:07:38,730 --> 00:07:45,720
我们将添加第二个参数，即数据，它实际上是训练集，但没有

116
00:07:45,720 --> 00:07:51,540
因变量，因为请记住，PCA上校只是一种使用内核技巧的PCA技术

117
00:07:51,750 --> 00:07:57,570
将数据映射到更高的维度，然后应用PCA，因为实际上在这种高维度上，

118
00:07:57,570 --> 00:08:04,380
数据是线性可分离的，因此，由于我们在较高的维度上应用了PCa，因此PCA是不受监督的

119
00:08:04,380 --> 00:08:05,090
技术。

120
00:08:05,160 --> 00:08:10,890
在这里，对于数据参数，我们只需要输入训练集即可，但是没有因变量

121
00:08:11,190 --> 00:08:19,830
因此，要求提供PCa，我们在此处输入数据等于括号中的训练集，以消除相关因素

122
00:08:19,830 --> 00:08:25,170
因为我们只有两个自变量，所以索引为3的变量。

123
00:08:25,200 --> 00:08:25,710
好吧。

124
00:08:25,710 --> 00:08:27,760
然后下一个参数是内核。

125
00:08:27,840 --> 00:08:32,130
因此内核是您要用于应用内核技巧的内核。

126
00:08:32,160 --> 00:08:38,100
请记住，当我们研究内核SVM时，发现有多个内核可以使用内核技巧。

127
00:08:38,100 --> 00:08:43,520
在这里，我们将使用最常见的一种加油机上校，在这里称为RBA。

128
00:08:43,740 --> 00:08:45,880
这是我们的第三个论点。

129
00:08:46,050 --> 00:08:51,960
因此，在这里我们可以将内核等于RDF点。

130
00:08:52,190 --> 00:08:52,850
好吧。

131
00:08:52,920 --> 00:08:58,600
然后，下一个参数是keep的下一个参数是什么，否则我们实际上将不使用该参数。

132
00:08:58,740 --> 00:09:04,950
但是，我们有一个非常重要的论点，它是降维的核心，即

133
00:09:04,950 --> 00:09:10,440
features是要结束的主要部件数。

134
00:09:10,590 --> 00:09:15,960
因此，在这里，当然，谁想可视化将测试结果分为两部分的培训

135
00:09:15,960 --> 00:09:22,230
维度，并在二维中具有此维度，我们需要将两个新提取的独立

136
00:09:22,230 --> 00:09:23,030
变量。

137
00:09:23,040 --> 00:09:28,000
因此，这里的功能数量将要求PCa。

138
00:09:28,260 --> 00:09:32,790
因此，我们将在此处输入特征等于。

139
00:09:33,200 --> 00:09:33,590
好吧。

140
00:09:33,590 --> 00:09:35,770
这就是我们的PCA。

141
00:09:35,800 --> 00:09:41,960
准备好创建对象并将其用于将我们的原始数据集转换为该新数据集

142
00:09:41,960 --> 00:09:45,700
具有从内核PCA派生的新提取的功能。

143
00:09:45,710 --> 00:09:49,330
因此，让我们选择这一行并创建对象。

144
00:09:49,340 --> 00:09:50,330
这里是。

145
00:09:50,330 --> 00:09:56,210
您说的很好，现在就开始进行下一步，即转换原始数据集

146
00:09:56,330 --> 00:09:59,660
到这个新的提取数据集中。

147
00:09:59,660 --> 00:10:03,090
所以现在事情看起来就像我们对PCa所做的一样。

148
00:10:03,260 --> 00:10:08,030
但是有些事情将会改变，因此我们将逐步进行，我们将看到需要在哪里进行

149
00:10:08,180 --> 00:10:09,240
一些变化。

150
00:10:09,560 --> 00:10:15,370
好吧，首先，当我们很忙时，我们将使用预测功能来转换原始火车

151
00:10:15,620 --> 00:10:17,800
放入新提取的训练集中。

152
00:10:17,840 --> 00:10:24,530
因此，这种新的训练集具有从PCA上校派生的新提取特征，我们称之为训练集

153
00:10:25,190 --> 00:10:26,910
强调PCA。

154
00:10:27,200 --> 00:10:32,650
好吧，然后等于，然后我们使用预测函数来进行转换。

155
00:10:32,690 --> 00:10:38,830
在这个漂亮的函数中，我们首先像对PCa一样输入KPC对象。

156
00:10:39,050 --> 00:10:42,720
然后训练集原来的训练集。

157
00:10:42,730 --> 00:10:44,590
因此，让我们进行训练。

158
00:10:44,690 --> 00:10:46,150
第二个。

159
00:10:46,190 --> 00:10:46,940
好吧。

160
00:10:47,180 --> 00:10:48,980
与PCA相对。

161
00:10:48,980 --> 00:10:57,230
与LDA一样，这将返回一个矩阵，我们需要它作为日期框架以要求LDK使用该矩阵

162
00:10:57,320 --> 00:11:02,340
因为该数据认为大脑功能正常。

163
00:11:02,360 --> 00:11:09,640
所以在这里括号，我们在这里关闭括号以设置此变换训练

164
00:11:09,650 --> 00:11:16,320
PCA与众不同，提醒我们，我们这样做是为了提供下一个功能在下一个功能中的用途

165
00:11:16,320 --> 00:11:18,190
部分期望。

166
00:11:18,200 --> 00:11:19,960
到目前为止一切都很好。

167
00:11:20,020 --> 00:11:25,460
因此，现在让我们选择这个计划并执行它，您将看到将要发生的事情，

168
00:11:25,460 --> 00:11:31,010
您将了解为什么我们称这种新的训练集训练为PCa的名称不同

169
00:11:31,100 --> 00:11:33,840
比起原来的训练说火车。

170
00:11:33,860 --> 00:11:35,060
好吧，让我们执行。

171
00:11:35,060 --> 00:11:36,100
开始了。

172
00:11:36,140 --> 00:11:37,490
执行正确。

173
00:11:37,670 --> 00:11:42,550
现在让我们看一下我们刚刚创建的训练说的PCa。

174
00:11:42,560 --> 00:11:47,990
因此，我将扩大此范围，以便我们可以看到哪一种是PCA的菌株。

175
00:11:48,050 --> 00:11:49,540
因此，让我们来看一下。

176
00:11:49,550 --> 00:11:53,360
我要单击它，这是我们的火车PCA。

177
00:11:53,360 --> 00:11:57,930
因此，我们可以看到它仅由我们想要的两列和V-2组成。

178
00:11:58,370 --> 00:12:00,840
因此，请尝试猜测这两列是什么。

179
00:12:00,860 --> 00:12:06,470
我现在要告诉你，这两列是我们通过获得的主要组成部分

180
00:12:06,470 --> 00:12:07,570
当前的PCA。

181
00:12:07,700 --> 00:12:11,000
那就是这些是两个新提取的功能。

182
00:12:11,030 --> 00:12:16,580
在使用Caltech将所有这些映射到高维之后，然后将PCA应用于数据

183
00:12:16,580 --> 00:12:18,780
集映射到这个高维度。

184
00:12:19,220 --> 00:12:26,390
但是现在的问题是，在PCA的这次培训中，我们不必依赖Roybal，我们需要它

185
00:12:26,390 --> 00:12:32,270
在下一节中，因为在我们的代码模板中，我们需要具有独立变量，并且

186
00:12:32,270 --> 00:12:33,480
因变量。

187
00:12:33,530 --> 00:12:40,450
那么下一步是什么下一步是将因变量添加到此训练集PCa中

188
00:12:40,790 --> 00:12:47,180
这里要了解的是，我们失去了深渊和罗伊巴尔，但我们保留了观察

189
00:12:47,180 --> 00:12:53,830
那就是这里的这个对应于我们在原始训练集中的第一次观察。

190
00:12:53,960 --> 00:12:54,650
这个。

191
00:12:54,650 --> 00:13:01,590
因此，这里的第一个观察结果带有零标签，即该第一个客户未购买SUV。

192
00:13:01,700 --> 00:13:04,190
这是原始的自变量。

193
00:13:04,190 --> 00:13:10,460
然后，如果您接受我们的培训，说PCA，那么这个第一位客户就是这个第一位客户

194
00:13:10,460 --> 00:13:11,290
训练集。

195
00:13:11,300 --> 00:13:14,840
因此它将在购买列中带有零标签。

196
00:13:14,840 --> 00:13:20,690
但是，这些是新提取的功能，我们当然不能获得与独立功能相同的值

197
00:13:20,690 --> 00:13:22,940
我们原始火车的变量。

198
00:13:22,940 --> 00:13:29,510
所以我们现在所能做的就是简单地将因变量称为他购买的原始培训

199
00:13:29,510 --> 00:13:36,890
设置并将其添加到我们的训练集中PCA，因为此处的这些观察与

200
00:13:36,890 --> 00:13:38,500
我们原来的火车。

201
00:13:38,570 --> 00:13:46,070
因此，我们现在需要做的非常简单，我们只需要带上训练套件PCa，然后我们就可以

202
00:13:46,070 --> 00:13:49,450
添加一个新列，该列将调用购买的。

203
00:13:49,670 --> 00:13:55,280
因此，通过执行此操作，您知道我正在创建一个我也称为购买的新列，因为此新列

204
00:13:55,280 --> 00:14:01,430
专栏将要购买依赖无价，然后等于，然后我现在要做的是

205
00:14:01,520 --> 00:14:07,730
是要从Ben Roybal从原始火车上给他打电话来的真正购买，我们可以做到

206
00:14:07,730 --> 00:14:12,920
因为训练说PCA包含与原始训练相同的观察结果

207
00:14:12,920 --> 00:14:13,570
组。

208
00:14:13,670 --> 00:14:19,430
因此，在这里获取原始火车的购买列，我们只需要获取原始火车的

209
00:14:19,550 --> 00:14:22,590
这称为训练集，然后称为美元。

210
00:14:22,820 --> 00:14:26,100
然后就是在这里进行纯洁专栏文章。

211
00:14:26,180 --> 00:14:32,890
因此，通过执行此操作，我将在预添加此新列，并且您可以在预添加后添加

212
00:14:32,890 --> 00:14:36,190
原始训练集的“预追踪”列的值。

213
00:14:36,420 --> 00:14:37,600
让我们来看看。

214
00:14:37,600 --> 00:14:40,480
我将选择此行并执行。

215
00:14:40,660 --> 00:14:47,590
现在，如您所见，我是否要重新训练PCA，其中包含原始PC的pre-Chase列

216
00:14:47,590 --> 00:14:48,480
动车组。

217
00:14:48,670 --> 00:14:49,200
这样很好。

218
00:14:49,200 --> 00:14:50,490
那是下一步。

219
00:14:50,530 --> 00:14:55,960
现在我们需要照顾好沙漠，因此要照顾好我们将要做的工作

220
00:14:55,960 --> 00:14:59,090
PCA说，和我们在这次培训中所做的一样。

221
00:14:59,110 --> 00:15:03,670
因此，让我们复制此副本并将其粘贴到此处。

222
00:15:03,700 --> 00:15:10,760
当然，我们现在要做的是用测试仪PCA代替PCA所说的培训。

223
00:15:11,050 --> 00:15:17,110
同样在这里，我们采用原始测试集进行转换，然后添加

224
00:15:17,110 --> 00:15:22,600
将原始测试集的购买列添加到该新测试集。

225
00:15:22,690 --> 00:15:25,550
这是从代码PCA中提取的经过测试的。

226
00:15:25,650 --> 00:15:28,400
因此，在这里进行测试，应该可以。

227
00:15:28,540 --> 00:15:32,890
因此，我将在此处选择这两行并执行。

228
00:15:33,130 --> 00:15:34,040
完善。

229
00:15:34,190 --> 00:15:36,410
我们通过测试测试的PCA已创建。

230
00:15:36,580 --> 00:15:37,860
让我们快速检查一下。

231
00:15:37,870 --> 00:15:39,130
这就是测试集。

232
00:15:39,460 --> 00:15:45,480
这就是我们的测试，说PCa具有两个新提取的功能和购买列。

233
00:15:45,530 --> 00:15:49,180
现在，这意味着我们正确地应用了PCA上校。

234
00:15:49,180 --> 00:15:51,960
太好了，我们准备继续进行下一部分。

235
00:15:52,140 --> 00:15:59,590
因此，让我们回到当前的PCA文件中，现在将logistic回归修复为训练集。

236
00:15:59,590 --> 00:16:02,200
现在我们需要在此代码部分中进行任何更改。

237
00:16:02,320 --> 00:16:04,540
是的，我们当然会这样做，因为。

238
00:16:04,570 --> 00:16:05,460
小心。

239
00:16:05,470 --> 00:16:09,670
我们称新提取的火车组切尔宁说PCA。

240
00:16:09,790 --> 00:16:14,820
因此，在这里对于数据参数，我们需要指定训练说PCA。

241
00:16:14,830 --> 00:16:16,510
因此，这是我们唯一需要更改的地方。

242
00:16:16,510 --> 00:16:20,830
因此，我们准备选择该部分并执行。

243
00:16:20,830 --> 00:16:22,290
好的CAS非常准备。

244
00:16:22,450 --> 00:16:26,150
现在，让我们继续进行与测试结果有关的下一部分。

245
00:16:26,320 --> 00:16:33,640
当然，这里是相同的，我们不得不用测试集PCA替换测试集，这对艺术家来说意味着一点

246
00:16:33,640 --> 00:16:34,500
一点。

247
00:16:34,690 --> 00:16:35,400
对。

248
00:16:35,950 --> 00:16:36,620
就是这样。

249
00:16:36,660 --> 00:16:40,810
我们准备执行该部分以预测结果。

250
00:16:41,080 --> 00:16:41,920
现在我们开始。

251
00:16:41,920 --> 00:16:46,730
我们得到了预测的载体，为什么今年要参加这个新测试。

252
00:16:47,020 --> 00:16:49,330
现在，让我们创建混淆矩阵。

253
00:16:49,330 --> 00:16:53,570
我们当然需要通过测试PCA来更改测试集。

254
00:16:53,900 --> 00:16:54,620
开始了。

255
00:16:54,640 --> 00:16:55,630
现在准备就绪。

256
00:16:55,720 --> 00:17:01,180
现在，我们可以执行以下代码行以获取混淆矩阵。

257
00:17:01,180 --> 00:17:08,620
这是我们可以快速浏览的内容，议会由个人看到他进入，我们得到57加26

258
00:17:08,710 --> 00:17:10,120
等于83。

259
00:17:10,210 --> 00:17:16,030
并且由于我们在测试集中拥有100个观察值，因此可以使我们达到83％的准确性。

260
00:17:16,070 --> 00:17:17,170
因此，这非常好。

261
00:17:17,350 --> 00:17:19,210
现在让我们进入令人兴奋的部分。

262
00:17:19,240 --> 00:17:23,500
非常迅速地可视化导致的趋势，我们需要改变什么。

263
00:17:23,500 --> 00:17:27,640
请记住，我们需要更改自变量的名称并在此处调用名称。

264
00:17:27,640 --> 00:17:28,820
那是强制性的。

265
00:17:28,960 --> 00:17:34,590
提醒一下，名称是一个，而两个是自变量的名称。

266
00:17:34,600 --> 00:17:41,890
因此，在这里我们需要将年龄替换为1，并将估计工资替换为V-2。

267
00:17:42,280 --> 00:17:44,540
这不是强制性的。

268
00:17:44,540 --> 00:17:46,650
无论如何，我们已经有了两个好名字。

269
00:17:46,650 --> 00:17:48,000
P.s. 也是一个NPC

270
00:17:48,160 --> 00:17:49,580
所以不要忘记这一点。

271
00:17:49,660 --> 00:17:54,700
当然，我们需要更改培训集的名称，因为我们接受了培训集的培训

272
00:17:54,700 --> 00:17:55,900
PCA说。

273
00:17:55,930 --> 00:17:58,440
因此，我在这里添加了Trainset PCA。

274
00:17:58,690 --> 00:17:59,770
那是完美的。

275
00:17:59,770 --> 00:18:01,540
准备执行。

276
00:18:01,540 --> 00:18:03,470
可视化训练集结果。

277
00:18:03,730 --> 00:18:05,970
因此，我们将仅可视化结果的趋势。

278
00:18:05,980 --> 00:18:11,310
但是，让我们对测试集进行相同的更改，以便您自己查看一下。

279
00:18:11,320 --> 00:18:18,160
同样，我们将年龄替换为V-2的一个估算工资。

280
00:18:18,160 --> 00:18:23,040
在这里，我们用强调PCA的测试代替测试。

281
00:18:23,410 --> 00:18:23,860
好吧。

282
00:18:23,870 --> 00:18:25,060
现在让我们看看。

283
00:18:25,060 --> 00:18:27,100
我期待着向您展示会发生什么。

284
00:18:27,130 --> 00:18:32,890
所以我将选择从这里到顶部的所有内容，这是要可视化的整个部分

285
00:18:32,890 --> 00:18:34,150
结果趋势。

286
00:18:34,180 --> 00:18:37,810
然后按命令控制我们输入执行。

287
00:18:37,810 --> 00:18:38,560
开始了。

288
00:18:38,590 --> 00:18:42,870
正在运行计算机患者。

289
00:18:42,960 --> 00:18:43,320
好吧。

290
00:18:43,320 --> 00:18:50,000
因此，这些是上校PCA结合到我们应用于非线性的Logistic回归模型的结果

291
00:18:50,010 --> 00:18:51,800
可分离的数据集。

292
00:18:52,020 --> 00:18:57,540
因此，我们可以体会到所得结果的简单性与复杂性之间的对比

293
00:18:57,540 --> 00:19:02,820
幕后发生的事情，因为在这两个方面，我们确实得到了非常简单的结果

294
00:19:02,820 --> 00:19:08,340
用直线分隔的类与幕后发生的事情是我们的原始数据

295
00:19:08,340 --> 00:19:15,030
使用内核技巧将我们原始特征空间中的集合映射到更高维度，以避免

296
00:19:15,030 --> 00:19:17,120
高度计算密集型计算。

297
00:19:17,310 --> 00:19:22,950
然后通过将原始特征空间中的数据集映射到这个高维，对我们来说，

298
00:19:22,950 --> 00:19:24,550
创建了一些新尺寸。

299
00:19:24,630 --> 00:19:30,520
多数情况下，这创建了一个新的特征空间，在这里我们的数据可以线性分离。

300
00:19:30,780 --> 00:19:34,950
但是这样做可以使尺寸比原始尺寸更多。

301
00:19:34,950 --> 00:19:40,470
因此，我们仍然需要应用PCA降维技术以减少数量

302
00:19:40,470 --> 00:19:41,620
尺寸。

303
00:19:41,640 --> 00:19:46,950
因此，然后将PCa应用于此新特征空间，在该空间中数据是线性可分离的，并且通过

304
00:19:47,430 --> 00:19:52,950
在可用状态中提取的一些内容已被创建，除了主要成分

305
00:19:53,040 --> 00:19:53,860
PCa。

306
00:19:54,000 --> 00:20:00,450
最后，我们获得了由这两个新提取的主成分形成的新特征空间

307
00:20:00,660 --> 00:20:07,110
来自PCa的结果，现在我们的数据是线性可分离的，并且通过线性更好地分离

308
00:20:07,110 --> 00:20:08,140
分类器。

309
00:20:08,550 --> 00:20:10,690
好的，对于内核PCA就是这样。

310
00:20:10,770 --> 00:20:16,420
这也是该零件降维的结束，我将在下一部分中看到您

311
00:20:16,520 --> 00:20:22,650
道德选择和提高本课件的最后一部分将介绍一个非常令人兴奋的算法

312
00:20:22,650 --> 00:20:25,820
在机器学习中称为极限提升。

313
00:20:25,830 --> 00:20:28,260
因此，我期待在下一部分看到您。

314
00:20:28,260 --> 00:20:30,170
在那之前，享受机器学习。

