1
00:00:00,240 --> 00:00:02,850
您好，欢迎来到本Python教程。

2
00:00:02,970 --> 00:00:03,620
所以我们到了。

3
00:00:03,630 --> 00:00:09,300
我们最后的非线性回归模型的最后一轮回归，这对我们来说是随机的

4
00:00:09,750 --> 00:00:13,170
在上一节中，我们看到了决策树回归模型。

5
00:00:13,180 --> 00:00:17,940
因此，现在决策树回归对您没有任何秘密，那么您将完全理解

6
00:00:18,000 --> 00:00:24,090
Ranum对我们来说是回归，因为随机森林只是一组决策树，每个决策树

7
00:00:24,090 --> 00:00:30,210
您的因变量的预测和随机数本身的最终预测很简单

8
00:00:30,210 --> 00:00:34,500
森林中所有不同树木的不同预测的平均值。

9
00:00:34,500 --> 00:00:39,450
在决策树的最后一部分结束时，我问你一个谜

10
00:00:39,450 --> 00:00:43,870
您认为将获得预测和可视化方面的帮助。

11
00:00:43,980 --> 00:00:47,800
您认为我们会像树上一样拥有一些恒星的形状吗？

12
00:00:48,000 --> 00:00:52,950
您认为我们会得到诸如决策树之类的较差的预测还是更准确的预测

13
00:00:52,950 --> 00:00:53,120
。

14
00:00:53,280 --> 00:00:59,610
希望这将接近该未来雇员应有的16万薪水

15
00:00:59,610 --> 00:01:00,940
它以前的公司。

16
00:01:01,350 --> 00:01:07,010
这就是我们将在本教程中找到的内容，并让我们随机构建

17
00:01:07,010 --> 00:01:08,460
对我们来说回归。

18
00:01:08,460 --> 00:01:12,110
因此，让我们进入Flight Explorer，将步枪设置为其余工作目录。

19
00:01:12,270 --> 00:01:15,000
因此，我们开始进行机器学习的第二部分回归。

20
00:01:15,150 --> 00:01:17,370
在这里您可以看到这是最后一轮。

21
00:01:17,370 --> 00:01:21,690
好吧，这实际上是我们正在建立的最后一个回归模型。

22
00:01:21,690 --> 00:01:26,700
因此，我们打开该文件夹，这就是要设置为工作目录的步枪。

23
00:01:26,790 --> 00:01:29,340
确保您拥有要提交的职位。

24
00:01:29,430 --> 00:01:34,520
如果是这种情况，请单击此小按钮，以将该文件夹设置为工作目录。

25
00:01:34,530 --> 00:01:34,980
好吧。

26
00:01:35,010 --> 00:01:41,400
现在让我们建立一个模型，以便我们进入回归模板，这次我们将不做任何事情

27
00:01:41,400 --> 00:01:47,050
到这里为止，对回归结果进行标准的可视化处理，但我们取而代之。

28
00:01:47,070 --> 00:01:49,080
因为我实际上是在给你一个提示。

29
00:01:49,080 --> 00:01:55,230
对我们来说，跑步可能是不连续的回归，因为这仅仅是一些不连续的组合

30
00:01:55,290 --> 00:01:58,080
决策树本身之外的回归模型。

31
00:01:58,230 --> 00:02:03,840
因此，我们不想接受与决策树中相同的问题。

32
00:02:03,900 --> 00:02:07,580
因此，我们将在此处使用此代码段来可视化回归结果。

33
00:02:07,600 --> 00:02:15,420
好的，所以我将复制粘贴在此处的所有副本，然后删除该部分，因为

34
00:02:15,750 --> 00:02:18,250
这不适用于我们的模型。

35
00:02:18,300 --> 00:02:24,360
所以您已经有了一个提示，您可能会猜测我们将在拟合曲线和预测方面获得什么

36
00:02:24,380 --> 00:02:24,850
。

37
00:02:25,300 --> 00:02:35,730
好的，现在让我们更改基础知识，在这里随机替换回归模型以进行回归。

38
00:02:35,730 --> 00:02:36,820
还有这里。

39
00:02:36,930 --> 00:02:37,630
对。

40
00:02:38,020 --> 00:02:40,140
现在让我们做重要的事情。

41
00:02:40,140 --> 00:02:42,190
创建我们的回归器。

42
00:02:42,270 --> 00:02:45,900
因此，与往常一样，我们将分三个步骤进行操作。

43
00:02:45,900 --> 00:02:48,480
第一步是为工作导入合适的类。

44
00:02:48,480 --> 00:02:53,710
第二步是创建此类的对象，这将是我们为美国回归者设计的对象。

45
00:02:53,940 --> 00:02:59,730
然后，第三步是使用fit方法将我们的随机变量拟合到我们的数据集中。

46
00:02:59,730 --> 00:03:01,110
因此，让我们开始吧。

47
00:03:01,110 --> 00:03:05,120
我们将要导入的类是随机森林回归类。

48
00:03:05,130 --> 00:03:13,950
另一个非常直观的名称，我们将从S-K学习点将其导入并进行组装。

49
00:03:13,980 --> 00:03:15,750
这是图书馆。

50
00:03:15,750 --> 00:03:23,960
因此，从比例尺或符号库中，我们导入随机森林回归或类。

51
00:03:23,970 --> 00:03:27,620
因此，这是具有资本，我们的资本和资本R的类别。

52
00:03:27,750 --> 00:03:33,730
现在第二步，我们创建此类的对象，这是我们自己回归的Ranum。

53
00:03:33,780 --> 00:03:40,600
因此，我们将其称为回归，或者像往常一样在这里，现在我们称为针对美国回归者的课程运行

54
00:03:40,610 --> 00:03:40,640
。

55
00:03:40,650 --> 00:03:42,420
我将复制此权利。

56
00:03:42,420 --> 00:03:48,320
复制粘贴到这里，然后现在我们输入参数。

57
00:03:48,320 --> 00:03:48,860
大。

58
00:03:48,900 --> 00:03:53,700
因此，让我们检查此类，以查看需要输入哪些参数，然后命令I。

59
00:03:53,820 --> 00:03:54,590
现在我们开始。

60
00:03:54,600 --> 00:03:56,100
这些是参数。

61
00:03:56,400 --> 00:03:58,950
因此，第一个参数是估计量。

62
00:03:58,950 --> 00:03:59,970
你能猜出这是什么。

63
00:03:59,970 --> 00:04:01,420
好吧，它实际上写在这里。

64
00:04:01,500 --> 00:04:04,470
这是您要在森林中拥有的树木数量。

65
00:04:04,500 --> 00:04:06,580
因此，这是一个非常重要的参数。

66
00:04:06,610 --> 00:04:11,730
此参数的Reiffel值是我们在十棵树的森林中构建的十棵树。

67
00:04:11,790 --> 00:04:15,340
但是请放心，我们将尝试10棵以上的树木。

68
00:04:15,340 --> 00:04:19,890
我们将尝试100棵树，您最多可以种植300棵树。

69
00:04:19,920 --> 00:04:23,100
因此，让我们以这个论点和估计量结尾

70
00:04:25,560 --> 00:04:27,920
等于，让我们从10开始。

71
00:04:28,080 --> 00:04:31,110
我想我们将从那开始，然后再尝试300。

72
00:04:31,320 --> 00:04:31,890
好。

73
00:04:32,100 --> 00:04:35,310
这就是一年级中的第一个参数条目。

74
00:04:35,340 --> 00:04:37,200
那么第二个参数是准则。

75
00:04:37,200 --> 00:04:42,540
并记住，这是我们用于构建决策的决策树类中的参数

76
00:04:42,540 --> 00:04:43,870
树回归模型。

77
00:04:43,890 --> 00:04:49,050
因此，您知道是均方误差占用了预测之间的一些平方差

78
00:04:49,050 --> 00:04:50,870
和真正的结果。

79
00:04:51,170 --> 00:04:52,380
那是除雾剂Tyrian。

80
00:04:52,430 --> 00:04:56,430
我们不会说这是因为这是最常见的标准。

81
00:04:56,510 --> 00:04:59,800
这将与我们的模型完美结合，也许在这里也可以。

82
00:04:59,810 --> 00:05:05,300
Grace然后询问决策树，您有一些可选参数，例如默认的Max功能

83
00:05:05,300 --> 00:05:06,860
值是自动的。

84
00:05:07,010 --> 00:05:13,410
这样很好，因为然后您会看到很多最大参数和最小参数，因此

85
00:05:13,420 --> 00:05:16,780
您可以用来改善模型的一些参数。

86
00:05:16,820 --> 00:05:20,850
但是，让我们关注回归模型的Ranum本质。

87
00:05:20,930 --> 00:05:25,730
因此，我们将根据决策树回归的需要添加此参数随机状态。

88
00:05:25,730 --> 00:05:30,090
因此，我们将在此处输入等于零的随机状态。

89
00:05:30,290 --> 00:05:30,960
对。

90
00:05:31,010 --> 00:05:36,260
所以基本上我们只感兴趣的是估计器的数量，即树的数量

91
00:05:36,380 --> 00:05:37,130
在森林里。

92
00:05:37,130 --> 00:05:39,760
因此，我们将从10开始，然后尝试其他值。

93
00:05:39,880 --> 00:05:40,270
好。

94
00:05:40,370 --> 00:05:46,790
所以这是第二步，现在的最后一步是让我们感到遗憾或适合我们的数据集，

95
00:05:46,790 --> 00:05:53,020
为此，我们使用了fit方法，必须通过点将其与回归或对象分开。

96
00:05:53,300 --> 00:06:00,140
在这里，我们像往常一样使用第五种方法将回归或对象拟合到组成的数据集

97
00:06:00,230 --> 00:06:05,500
X矩阵特征以及因变量矢量的原因。

98
00:06:05,600 --> 00:06:09,910
现在，我们已经准备好为Ranum for us建立回归模型。

99
00:06:09,920 --> 00:06:11,420
因此，我们等待中。

100
00:06:11,510 --> 00:06:13,340
这对整个代码意味着什么。

101
00:06:13,340 --> 00:06:15,740
这意味着它可以执行了。

102
00:06:15,740 --> 00:06:17,700
因此，现在我们没有任何事情要做。

103
00:06:17,720 --> 00:06:22,550
作为选择，我们唯一可以做的就是随便玩几棵树，然后选择

104
00:06:22,550 --> 00:06:26,520
为他的回归模型建立的最好的树木团队。

105
00:06:26,750 --> 00:06:32,600
因此，我们将使用此参数并添加更多树并可视化结果，

106
00:06:32,600 --> 00:06:33,800
得到预测。

107
00:06:33,800 --> 00:06:36,290
直到我们得到连贯的东西。

108
00:06:36,320 --> 00:06:39,350
因此，让我们执行此操作，让我们一步一步地执行这些部分。

109
00:06:39,350 --> 00:06:42,940
因此，首先让我们导入库。

110
00:06:43,100 --> 00:06:43,900
开始了。

111
00:06:43,930 --> 00:06:45,070
库已导入。

112
00:06:45,230 --> 00:06:47,290
现在是导入的数据集。

113
00:06:47,300 --> 00:06:49,180
大卫，我们在这里说进口。

114
00:06:49,370 --> 00:06:52,630
现在让我们围绕它们构建回归模型。

115
00:06:52,700 --> 00:06:54,820
如您所见，我们应该没有任何问题。

116
00:06:54,810 --> 00:06:58,370
因此，首先，我们围绕10棵树的随机森林进行建设。

117
00:06:58,370 --> 00:07:01,190
在这里，我们走了路线，为我们创建了。

118
00:07:01,280 --> 00:07:04,780
因此，您可以看到所有不同的参数都是自动选择的。

119
00:07:04,850 --> 00:07:07,510
所以现在是时候玩得开心了。

120
00:07:07,520 --> 00:07:12,130
您想从预测结果还是可视化回归结果开始。

121
00:07:12,200 --> 00:07:19,580
因此，也许让我们首先执行您知道的步骤，尝试我们模型的一些不同方法，然后进行测试

122
00:07:19,580 --> 00:07:21,230
不同的预测。

123
00:07:21,230 --> 00:07:25,490
但是首先在执行本节之前，您能猜到会发生什么吗？

124
00:07:25,490 --> 00:07:30,890
你知道一棵树我们得到了一些楼梯的形状，你知道这是一个不连续的模型

125
00:07:31,190 --> 00:07:37,160
并在算法中提出将整个级别范围分为两个单独的间隔

126
00:07:37,430 --> 00:07:41,990
并在每个间隔中考虑因变量工资的平均值。

127
00:07:41,990 --> 00:07:47,030
实际上，由于我们在间隔中只有一个值，因为我们平均只有10个级别

128
00:07:47,150 --> 00:07:51,820
只是间隔中包含的级别的薪水。

129
00:07:51,830 --> 00:07:55,760
因此，这一点都不有趣，这就是为什么我们得出的预测很差。

130
00:07:55,820 --> 00:07:57,700
但是这里我们有几棵树。

131
00:07:57,770 --> 00:08:01,400
因此，我们将有几个楼梯，因此您将如何可视化。

132
00:08:01,400 --> 00:08:05,230
您如何想象一维楼梯的组合。

133
00:08:05,300 --> 00:08:09,030
因此，请尝试在这些视频中暂停播放并尝试预测将要发生的情况。

134
00:08:09,100 --> 00:08:13,020
现在，我将选择该部分以了解我们将得到什么。

135
00:08:13,220 --> 00:08:21,080
因此，我们得到的是可视化的由10棵树组成的一些随机回归变量。

136
00:08:21,080 --> 00:08:22,580
这就是我们所获得的。

137
00:08:22,700 --> 00:08:26,810
因此，我们可以按预期对非连续模型进行确认。

138
00:08:26,810 --> 00:08:32,750
因此，让我们通过将步进降低到0.1来增加分辨率，以了解更多信息。

139
00:08:32,750 --> 00:08:36,220
垂直线表示更好的非连续性。

140
00:08:36,350 --> 00:08:37,300
因此，我们再次开始。

141
00:08:37,310 --> 00:08:40,000
让我们来执行它。

142
00:08:40,220 --> 00:08:40,480
好吧。

143
00:08:40,490 --> 00:08:41,390
现在好了。

144
00:08:41,480 --> 00:08:46,250
我们在视觉上看到一些垂直的直线，比不连续的要好。

145
00:08:46,460 --> 00:08:47,880
所以现在让我们看一下。

146
00:08:48,260 --> 00:08:50,350
让我们来解释一下。

147
00:08:50,390 --> 00:08:55,130
因此，我在上一节中问过的谜题的答案是，我又在问你

148
00:08:55,140 --> 00:09:02,630
在本教程中，我们只是通过拥有多个决策树来简单地走上台阶

149
00:09:02,630 --> 00:09:04,600
一棵决策树。

150
00:09:04,640 --> 00:09:09,740
与一棵决策树相比，我们的阶梯要多得多。

151
00:09:10,130 --> 00:09:15,770
因此，我们在整个级别范围内有更多的拆分，因此有更多的间隔

152
00:09:15,950 --> 00:09:16,780
不同级别的。

153
00:09:16,780 --> 00:09:24,320
因此，此处的每条水平直线都被这些垂直线或一个间隔（即一格）分开

154
00:09:24,650 --> 00:09:29,260
而且我们在楼梯上走更多步的事实实际上非常直观，因为您知道我们是否

155
00:09:29,260 --> 00:09:35,180
举例来说，这里的预测是6.5级。

156
00:09:35,180 --> 00:09:43,030
我们有10棵树投票决定6.5级职位的薪水是哪一步，然后是Ranum

157
00:09:43,070 --> 00:09:48,350
对我们来说，取所有人对6.5级薪水的所有不同预测的平均值

158
00:09:48,350 --> 00:09:50,150
森林里的不同树木。

159
00:09:50,150 --> 00:09:55,970
例如，如果我们在第四级别获得10票，则这10票对应

160
00:09:55,970 --> 00:10:00,590
对这十棵树中的每一棵树的薪水水平进行了一项预测。

161
00:10:00,800 --> 00:10:05,960
然后，美国竞选获得了这10个预测的平均值，那么这个平均值就不算什么了

162
00:10:05,960 --> 00:10:11,260
比Ranum森林本身对薪水水平的预测要高。

163
00:10:11,270 --> 00:10:17,120
因此，我们获得了更多的步骤，因为仅将整个级别范围划分为更多的间隔，并且

164
00:10:17,330 --> 00:10:22,280
是因为随机森林正在计算其决策树预测的许多不同平均值

165
00:10:22,340 --> 00:10:24,210
在每个这些间隔中。

166
00:10:24,230 --> 00:10:26,560
因此发生的事情非常直观。

167
00:10:26,570 --> 00:10:32,090
但是，有一点需要指出的是，如果我们在随机数中添加更多树

168
00:10:32,090 --> 00:10:38,720
森林好吧，这并不意味着我们会在楼梯上走很多步，因为您添加的树越多

169
00:10:39,080 --> 00:10:44,360
树木做出的不同预测的平均值收敛到相同平均值的程度越高

170
00:10:44,570 --> 00:10:48,600
您知道这是基于相同的技术熵和信息增益。

171
00:10:48,620 --> 00:10:54,140
因此，您添加树的数量越多，这些投票的平均值就会收敛到相同的最终平均值

172
00:10:54,740 --> 00:10:58,690
因此它将在此处收敛到某种形状的楼梯。

173
00:10:58,790 --> 00:11:01,130
因此，这对于可视化也很重要。

174
00:11:01,160 --> 00:11:06,320
现在，既然我们对美国回归的运行可视化有了直观的认识，那么让我们

175
00:11:06,320 --> 00:11:08,040
看看预测会发生什么。

176
00:11:08,120 --> 00:11:11,580
因此，让我们执行一下。

177
00:11:11,660 --> 00:11:13,490
并且您可以预测会发生什么。

178
00:11:13,490 --> 00:11:20,000
记住，通过我们的决策树，我们得到了十五万美元的预测，但是

179
00:11:20,000 --> 00:11:21,820
只是第六级的薪水。

180
00:11:21,830 --> 00:11:26,390
所以这实际上不是很有趣，但是正如我们在图表上看到的那样，这里还有更多

181
00:11:26,390 --> 00:11:27,790
在楼梯上的台阶。

182
00:11:27,860 --> 00:11:30,910
因此，也许您将获得更准确的预测。

183
00:11:31,160 --> 00:11:32,450
让我们来了解一下。

184
00:11:32,570 --> 00:11:34,200
让我们执行一下。

185
00:11:34,280 --> 00:11:40,010
在这里，我们得到的预测是十六万七千美元。

186
00:11:40,100 --> 00:11:42,460
所以比一棵树好一点。

187
00:11:42,560 --> 00:11:46,480
但是等等，我们周围只有10棵树供我们回归。

188
00:11:46,520 --> 00:11:48,770
我们可以建造的树木多于10棵。

189
00:11:48,770 --> 00:11:53,720
我们可以为他们建造一百棵树，而这正是我们要做的正确的事情

190
00:11:53,720 --> 00:11:56,610
现在来看我们的预测是否会变得更好。

191
00:11:56,840 --> 00:11:58,840
因此，让我们尝试一百棵树。

192
00:11:59,030 --> 00:11:59,790
开始了。

193
00:11:59,810 --> 00:12:04,450
现在有一百棵树，让我们重建执行模型。

194
00:12:04,520 --> 00:12:05,840
现在，模型已重建。

195
00:12:05,840 --> 00:12:09,990
让我们看看它是否可以在楼梯上走更多的台阶。

196
00:12:10,700 --> 00:12:12,860
那就是我告诉你的。

197
00:12:12,890 --> 00:12:18,050
我们将树木的数量乘以10，但是台阶和楼梯的数量似乎没有

198
00:12:18,050 --> 00:12:22,970
乘以10，这就是出于同样的原因，我正在谈论的下一代融合

199
00:12:22,970 --> 00:12:23,890
给你。

200
00:12:24,170 --> 00:12:29,720
所以实际上这是相同的想法，就像是相同的楼梯，但也许台阶更好

201
00:12:29,720 --> 00:12:32,530
选择做出最终的预测。

202
00:12:32,540 --> 00:12:33,810
因此，让我们检查一下。

203
00:12:33,950 --> 00:12:38,430
我们将结束这一点，看看在预测方面会发生什么。

204
00:12:38,450 --> 00:12:44,120
因此，请记住，在一棵树下，我们对10棵树的预测为150 k。

205
00:12:44,120 --> 00:12:47,480
我们的预测值为167K。

206
00:12:47,720 --> 00:12:51,470
现在让我们来看一看100棵树的预测。

207
00:12:51,560 --> 00:12:52,640
因此，让我们检查一下。

208
00:12:52,760 --> 00:12:54,940
我将执行此模型构建。

209
00:12:55,130 --> 00:12:59,510
所以我们开始吧，哇。

210
00:12:59,540 --> 00:13:00,140
真的很好

211
00:13:00,140 --> 00:13:07,800
现在我们得到了158个K的预测，因此它正在收敛到假定的先前

212
00:13:07,800 --> 00:13:09,220
员工的工资。

213
00:13:09,380 --> 00:13:13,220
您知道当我们应用多项式回归时，结论是真实的。

214
00:13:13,220 --> 00:13:18,050
这些员工不会虚张声势，我们很乐意聘请这名员工，因为这不仅好

215
00:13:18,120 --> 00:13:20,370
对于这份工作，他也很诚实。

216
00:13:20,370 --> 00:13:26,060
现在看来，在约有100棵树木的森林中，该模型正在确认这一点，因为现在

217
00:13:26,060 --> 00:13:30,180
得到一个十五万八千美元的预测。

218
00:13:30,230 --> 00:13:33,260
但是，让我们尝试是否可以更进一步地证明这一点。

219
00:13:33,260 --> 00:13:33,730
抱歉。

220
00:13:33,730 --> 00:13:39,960
例如通过增加树木数量到300来增加一百六十万。

221
00:13:39,980 --> 00:13:44,870
也许我们会得到更接近的预测，因为现在我认为多项式回归仍在跳动

222
00:13:45,050 --> 00:13:46,410
Ranum为我们回归。

223
00:13:46,550 --> 00:13:51,500
我不记得确切的镜像回归预测，但我认为实际上甚至

224
00:13:51,500 --> 00:13:55,300
接近160 K甚至接近平均。

225
00:13:55,430 --> 00:14:03,460
好吧，无论如何，让我们看看是否可以为我们的员工以前的薪水获得16万的预测。

226
00:14:03,530 --> 00:14:04,400
因此，让我们检查一下。

227
00:14:04,400 --> 00:14:10,180
我们将尝试使用300棵树，让我们建立这个模型。

228
00:14:10,220 --> 00:14:10,990
开始了。

229
00:14:11,000 --> 00:14:15,020
新模型创建了300棵树木的新森林。

230
00:14:15,260 --> 00:14:21,350
现在，我们将这些结果可视化，但我们将获得相同的结果，而步骤数却不会成倍增加

231
00:14:21,350 --> 00:14:22,300
到30岁。

232
00:14:22,370 --> 00:14:24,890
将具有与以前相同的形状。

233
00:14:24,890 --> 00:14:25,430
这里是。

234
00:14:25,430 --> 00:14:28,130
如您所见，它几乎是相同的。

235
00:14:28,130 --> 00:14:29,980
我们得到相同数量的步骤。

236
00:14:29,980 --> 00:14:36,440
仅仅是针对我们特定人员的薪资获取而实际设置的步骤

237
00:14:36,440 --> 00:14:37,460
问题。

238
00:14:37,460 --> 00:14:40,160
因此，现在让我们了解最终的预测。

239
00:14:40,340 --> 00:14:44,420
而且我认为我们将能够得出结论，这是事实还是虚张声势。

240
00:14:44,420 --> 00:14:46,370
因此，让我们执行这一行。

241
00:14:46,520 --> 00:14:48,410
准备好了吗

242
00:14:48,410 --> 00:14:55,580
我们走了，我们得到了哦，哇，我们现在就当场获得了一百六十万K的预测薪水

243
00:14:55,580 --> 00:14:55,610
。

244
00:14:55,610 --> 00:14:59,420
我们大胆地预测我们未来的雇员以前的薪水。

245
00:14:59,720 --> 00:15:04,160
现在，对于我们来说，Ranum回归正在击败单纯回归模型，因为现在我们

246
00:15:04,160 --> 00:15:07,720
得到与实际值几乎相同的预测。

247
00:15:08,000 --> 00:15:14,870
所以当场恭喜，我们实际上制作了最终模型，现在只想总结一下

248
00:15:14,870 --> 00:15:20,960
通过将本教程过渡到我们的未来观点之一，这实际上是非常重要的

249
00:15:20,960 --> 00:15:20,980
。

250
00:15:20,980 --> 00:15:24,000
我们将建立一些简单的机器学习模型。

251
00:15:24,020 --> 00:15:30,170
有些模型是多种机械模型的组合，您在机械和

252
00:15:30,190 --> 00:15:31,910
这些实际上是最好的模型。

253
00:15:31,970 --> 00:15:36,830
您知道当您拥有多个机器模型的团队时，他们实际上可以做出出色的预测

254
00:15:36,830 --> 00:15:41,660
因为除非我们在论证机械模型时有一个爱因斯坦机器运行模型

255
00:15:41,660 --> 00:15:43,050
唯一正确的模型。

256
00:15:43,280 --> 00:15:47,690
那么，您更有可能使用十台机器或任何模型来预测正确的预测

257
00:15:47,690 --> 00:15:50,330
只是一个模型而已。

258
00:15:50,390 --> 00:15:51,960
这实际上就是我们在这里所做的。

259
00:15:52,070 --> 00:15:57,300
好吧，我们有一个相同的机械模型团队，即决策树回归模型。

260
00:15:57,380 --> 00:16:01,510
但是将来，我们将组成一个由不同机器模型组成的团队。

261
00:16:01,580 --> 00:16:02,850
因此，这将非常有趣。

262
00:16:02,870 --> 00:16:04,860
那也将非常强大。

263
00:16:05,030 --> 00:16:07,330
我期待与您同行。

264
00:16:07,550 --> 00:16:12,770
因此，现在我要告诉您，首先要祝贺您建立此非常强大的回归功能的两件事

265
00:16:12,770 --> 00:16:14,830
明天为我们跑步的模特。

266
00:16:15,020 --> 00:16:22,340
其次，为了建立所有回归模型，我们建立了一些线性回归模型，

267
00:16:22,340 --> 00:16:28,880
线性回归模型一些非线性非连续回归模型以及一些非线性或非连续模型

268
00:16:28,970 --> 00:16:30,840
和简单的回归模型。

269
00:16:30,890 --> 00:16:35,850
因此，恭喜您，您一定会成为机器学习方面的专家。

270
00:16:35,870 --> 00:16:38,020
但是，等待接下来发生的事情。

271
00:16:38,030 --> 00:16:42,830
因此，谈到接下来会发生什么，我期待在下一部分或下一部分中与您见面。

272
00:16:42,830 --> 00:16:44,640
在此之前，请尽情学习

