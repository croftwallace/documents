1
00:00:00,240 --> 00:00:05,190
您好，欢迎您购买Tauriel，我非常兴奋，因为今天我将向您展示一个新的

2
00:00:05,190 --> 00:00:11,360
一种具有新型预测区域和新型预测边界的分类器。

3
00:00:11,550 --> 00:00:15,230
您将看到它与到目前为止所看到的完全不同。

4
00:00:15,360 --> 00:00:18,780
因此，我迫不及待想向您展示最终的图形结果。

5
00:00:18,780 --> 00:00:22,540
让我们转到这里的Flight Explorer，将我们的完整目录设置为工作目录。

6
00:00:22,680 --> 00:00:27,360
因此，现在我要进行Partha重分类和决策树分类。

7
00:00:27,360 --> 00:00:33,390
在这里，我们确保文件夹中的社交网络X Files，并且您可以单击此按钮

8
00:00:33,390 --> 00:00:35,900
将此文件夹设置为工作目录。

9
00:00:35,910 --> 00:00:36,480
好吧。

10
00:00:36,480 --> 00:00:38,910
现在，让我们使用分类模板。

11
00:00:39,300 --> 00:00:46,130
让我们将所有内容从这里带到底部副本。

12
00:00:46,650 --> 00:00:50,660
让我们回到这里并粘贴到这里。

13
00:00:50,670 --> 00:00:53,850
好吧，让我们在这里更改图形标题。

14
00:00:54,030 --> 00:01:01,330
这里还有分类器决策树。

15
00:01:02,050 --> 00:01:05,120
这不是树。

16
00:01:05,220 --> 00:01:06,140
好吧。

17
00:01:06,230 --> 00:01:10,760
现在，我们只需要在这里创建我们的类即可。

18
00:01:10,860 --> 00:01:16,400
您在Kirill上看到的决策树不是基于欧几里得距离的算法。

19
00:01:16,590 --> 00:01:22,140
因此，我们实际上并不需要在这里应用特征缩放，因为正如我所解释的，我们只是在获取它们

20
00:01:22,140 --> 00:01:28,140
当您的算法基于以下内容时，有一天需要在书中进行处理：

21
00:01:28,140 --> 00:01:33,310
欧氏距离，但是既然不是这种情况，我们就不需要这样做了。

22
00:01:33,540 --> 00:01:38,580
此外，当我们建立决策树模型时，我们需要一些解释，因为我们可以绘制

23
00:01:38,670 --> 00:01:44,580
一些树代表决策树和当然条件中的所有不同条件

24
00:01:44,580 --> 00:01:49,560
您不想拥有刻度值，而是想要拥有真正的原始价值，因此您不会暗示

25
00:01:49,560 --> 00:01:50,880
如果使用缩放。

26
00:01:50,910 --> 00:01:57,060
但是，当我们在此处绘制这些预测区域时，我们使用的分辨率为0.2。

27
00:01:57,090 --> 00:02:03,540
这意味着如果缩放比例变量，此处生成绘图的代码将执行得更快。

28
00:02:03,550 --> 00:02:03,750
。

29
00:02:03,930 --> 00:02:05,430
所以这是妥协。

30
00:02:05,460 --> 00:02:11,580
我们将要素保留在此处，因为我们正在以高分辨率绘制该图，但是

31
00:02:11,670 --> 00:02:17,040
知道如果您想绘制一些决策树（如真实树本身），则可以将其删除

32
00:02:17,040 --> 00:02:21,420
功能缩放部分，以解释您的贵重物品。

33
00:02:21,450 --> 00:02:23,530
好吧，这里让我们保持这种方式。

34
00:02:23,610 --> 00:02:25,440
现在，让我们构建分类器。

35
00:02:25,800 --> 00:02:32,780
因此，我们将导入决策树，从树中的标量对类进行分类，然后

36
00:02:32,790 --> 00:02:37,740
将要创建，分类或该类的对象，然后我们将使该主题适合该培训

37
00:02:37,740 --> 00:02:39,150
照常设置。

38
00:02:39,150 --> 00:02:41,340
因此，让我们首先导入该类。

39
00:02:41,430 --> 00:02:52,140
因此，它是从S-K学习该树，然后导入，然后是决策树分类器。

40
00:02:52,380 --> 00:02:53,490
那是你的考验。

41
00:02:53,580 --> 00:02:58,530
那就是我们将用来创建对象并将其适合训练集的类，让我们创建对象

42
00:02:58,530 --> 00:03:10,710
分类器等于树分类器，然后让我们检查此类以查看我们需要哪些参数

43
00:03:10,710 --> 00:03:11,490
输入。

44
00:03:11,670 --> 00:03:13,800
所以命令我检查。

45
00:03:13,800 --> 00:03:15,210
现在我们开始。

46
00:03:15,240 --> 00:03:18,970
因此，参数参数是第一个参数准则。

47
00:03:19,230 --> 00:03:22,610
确定，所以我知道它说的是标准的默认参数。

48
00:03:22,610 --> 00:03:28,310
这是基尼，但我们会说这个国家是基于熵的。

49
00:03:28,350 --> 00:03:32,100
最常见的决策树是基于熵的决策树。

50
00:03:32,220 --> 00:03:37,620
实际上，您有很多基于熵的惊人分类器，例如最大熵。

51
00:03:37,640 --> 00:03:40,520
对于人们来说，最终他们将是白人。

52
00:03:40,770 --> 00:03:45,960
因此，我们将习惯于信息获取的熵标准。

53
00:03:46,140 --> 00:03:52,440
基本上，这里所说的熵就是一个测量分割质量的函数，因此

54
00:03:52,440 --> 00:03:59,580
您希望树的最终节点与每个节点尽可能地同质

55
00:03:59,640 --> 00:04:06,620
您知道相同类别的用户，并且在每次拆分后，一组用户越同质

56
00:04:06,620 --> 00:04:11,040
熵从父节点减少到组子节点。

57
00:04:11,040 --> 00:04:16,260
因此，在拆分之后，如果结果子节点中的熵为零，则意味着

58
00:04:16,260 --> 00:04:23,040
子节点是您认识的完全同质的用户组，只有相同类别的用户，并且

59
00:04:23,040 --> 00:04:29,220
这里提到的信息游戏基本上是显示前后的熵差

60
00:04:29,220 --> 00:04:29,530
。

61
00:04:29,550 --> 00:04:35,910
因此，这意味着拆分后信息越多，那里的组越齐

62
00:04:35,910 --> 00:04:37,560
将在拆分之后。

63
00:04:37,590 --> 00:04:41,330
因此，让我们习惯于这一熵准则。

64
00:04:41,550 --> 00:04:47,610
因此，这里的鞋子标准等于熵。

65
00:04:48,040 --> 00:04:52,240
好吧，接下来的参数不是很重要。

66
00:04:52,260 --> 00:04:57,600
到目前为止，我们只希望专注于构建树并进行一些相当不错的分类。

67
00:04:57,660 --> 00:04:59,730
我们将看到如何改进决策树。

68
00:04:59,730 --> 00:05:02,370
然后，我们将使用其他参数。

69
00:05:02,520 --> 00:05:06,230
但是在这里，我们仅采用随机状态参数只是为了获得相同的结果。

70
00:05:06,240 --> 00:05:08,560
这样您就可以更轻松地跟我学习课程。

71
00:05:08,790 --> 00:05:13,030
随机状态等于零。

72
00:05:13,440 --> 00:05:14,080
好。

73
00:05:14,280 --> 00:05:17,990
现在，我们只需要将此分类器固定到我们的训练集即可。

74
00:05:18,300 --> 00:05:26,300
因此，我们在这里使用拟合方法，并将其应用于X火车以及为什么训练。

75
00:05:26,560 --> 00:05:26,970
好吧。

76
00:05:27,030 --> 00:05:29,150
现在整个代码已经准备就绪。

77
00:05:29,330 --> 00:05:33,550
因此，让我们首先在此处选择预处理数据的部分。

78
00:05:33,560 --> 00:05:34,130
开始了。

79
00:05:34,130 --> 00:05:35,420
所有预处理正确。

80
00:05:35,420 --> 00:05:36,010
完善。

81
00:05:36,050 --> 00:05:37,970
让我们去探索男孩吧。

82
00:05:38,570 --> 00:05:39,510
一切看起来都很好。

83
00:05:39,530 --> 00:05:44,250
现在，通过在此处执行此代码部分来创建分类器。

84
00:05:44,840 --> 00:05:45,300
好吧。

85
00:05:45,320 --> 00:05:45,760
都好。

86
00:05:45,760 --> 00:05:51,770
您可以看到我们在类中看到的所有参数都在这里使参数变形，因为

87
00:05:51,770 --> 00:05:53,260
没有指定它们。

88
00:05:53,270 --> 00:05:59,480
现在让我们做一些预测，让我们应用分类器来预测新的观测值以测试他们的观测值

89
00:05:59,480 --> 00:05:59,960
。

90
00:05:59,960 --> 00:06:01,510
这里我们去白面包。

91
00:06:01,910 --> 00:06:03,590
这是白面包。

92
00:06:03,590 --> 00:06:07,670
让我们快速将其与白色测试进行比较，以了解其外观。

93
00:06:08,420 --> 00:06:08,830
好。

94
00:06:08,840 --> 00:06:09,130
哇。

95
00:06:09,140 --> 00:06:10,130
这看起来很好。

96
00:06:10,130 --> 00:06:12,740
到目前为止，您已经看到这些是正确的预测。

97
00:06:12,740 --> 00:06:14,360
让我们看10

98
00:06:17,750 --> 00:06:18,610
19

99
00:06:18,790 --> 00:06:19,150
好。

100
00:06:19,160 --> 00:06:20,840
所以从1到9太完美了。

101
00:06:20,840 --> 00:06:23,930
但是从10到19，我们在这里看到了一些错误的预测。

102
00:06:23,930 --> 00:06:28,390
13号，15号，16号，仅此而已。

103
00:06:28,850 --> 00:06:29,240
好。

104
00:06:29,240 --> 00:06:35,060
因此，现在让我们看看在混淆矩阵上有多少不正确的预测。

105
00:06:35,060 --> 00:06:39,600
因此，让我们执行此操作，我们准备找出答案。

106
00:06:39,920 --> 00:06:44,260
因此，让我们在此处采用一个计算矩阵，它看起来非常不错。

107
00:06:44,350 --> 00:06:46,300
九个错误的预测。

108
00:06:46,310 --> 00:06:49,780
与我们的其他型号相比，这非常好。

109
00:06:49,790 --> 00:06:53,950
例如，在逻辑回归中，我认为这是11个错误的预测。

110
00:06:54,050 --> 00:06:57,310
然后，我们通过SBA获得了更多的间接保护。

111
00:06:57,500 --> 00:06:59,330
因此，这里做得很好。

112
00:06:59,680 --> 00:07:00,090
好。

113
00:07:00,110 --> 00:07:04,730
现在，让我们进入有关可视化结果的有趣和令人兴奋的部分。

114
00:07:04,910 --> 00:07:09,290
在这里，这会更加令人兴奋，因为我们将要有所不同。

115
00:07:09,350 --> 00:07:14,660
如果您遵循Carroll的直觉课程，您可能会猜到什么样的预测界限

116
00:07:14,660 --> 00:07:15,220
得到。

117
00:07:15,380 --> 00:07:18,050
尝试实际猜测会发生什么。

118
00:07:18,050 --> 00:07:20,190
这是一个很好的锻炼的好习惯。

119
00:07:20,240 --> 00:07:27,140
这将使您将直觉与练习结合起来，以便您可以在可以思考的视频中暂停播放

120
00:07:27,140 --> 00:07:28,110
关于它。

121
00:07:28,130 --> 00:07:33,410
现在，我要选择这个，现在选择Showtime。

122
00:07:33,650 --> 00:07:34,620
执行这个。

123
00:07:34,730 --> 00:07:38,580
这是决策树的图形解析。

124
00:07:38,600 --> 00:07:39,920
那是最大的。

125
00:07:39,950 --> 00:07:42,010
好的，所以很新。

126
00:07:42,080 --> 00:07:43,270
这是很新的。

127
00:07:43,280 --> 00:07:48,980
请记住，当分类器为Leno时我们有一条直线，或者我们有某种曲线

128
00:07:48,980 --> 00:07:52,630
使用甘蔗增压器时有很多不规则之处。

129
00:07:52,640 --> 00:07:59,390
我们也有一些漂亮的平滑曲线，内核为V.M. 和不错的低音，但现在我们有了一些东西

130
00:07:59,390 --> 00:08:00,800
非常不一样。

131
00:08:00,830 --> 00:08:09,290
首先，预测边界仅由水平线和垂直线组成

132
00:08:09,290 --> 00:08:14,590
正是您在Carroll和关于决策树的直觉的直觉教程中所看到的。

133
00:08:14,870 --> 00:08:21,260
您会看到它正在根据自变量的条件进行一些拆分，这些自变量是

134
00:08:21,260 --> 00:08:22,900
这些已发工资的年龄。

135
00:08:23,030 --> 00:08:28,760
所以我们可以在这里清楚地看到您如何知道它在变量的某些间隔上处于某些条件

136
00:08:28,760 --> 00:08:29,470
年龄。

137
00:08:29,480 --> 00:08:38,780
因此，例如，在这种情况下，估计薪水介于此值和该值之间

138
00:08:39,050 --> 00:08:41,660
年龄介于该值和该值之间。

139
00:08:41,660 --> 00:08:47,390
这些都是基于自变量间隔的条件，我们可以清楚地看到

140
00:08:47,390 --> 00:08:53,200
在这里，预测边界仅由水平线和垂直线组成。

141
00:08:53,240 --> 00:08:55,300
是的，这是很新的东西。

142
00:08:55,300 --> 00:08:56,750
哇

143
00:08:56,750 --> 00:08:59,210
所以我也该如何形容。

144
00:08:59,540 --> 00:09:05,540
首先，我们看到它确实是在试图抓住合适的每个用户

145
00:09:05,540 --> 00:09:06,690
在这里分类。

146
00:09:06,830 --> 00:09:12,150
它甚至不希望错过任何人，即使是那些明显位于绿色区域的红色用户。

147
00:09:12,260 --> 00:09:17,990
例如，像一口井一样，这里显然要努力抓住这个红色使用者，

148
00:09:17,990 --> 00:09:19,570
在绿色区域。

149
00:09:19,570 --> 00:09:24,930
但是，有些事情引起了您的注意，并告诉您要小心。

150
00:09:25,550 --> 00:09:32,030
好吧，我们应该小心，因为这种看起来像过拟合，因为您看到它正在尝试

151
00:09:32,030 --> 00:09:35,410
将每个用户都吸引到正确的类别中。

152
00:09:35,450 --> 00:09:42,590
因此，我不确定在某些新观察结果中，这些区域是否会以与

153
00:09:42,590 --> 00:09:45,140
同样，您知道正确的预测。

154
00:09:45,470 --> 00:09:47,530
因此，我们现在将看到测试。

155
00:09:47,570 --> 00:09:56,560
让我们关闭它，然后选择它以查看它执行的测试。

156
00:09:56,660 --> 00:09:58,190
这是测试。

157
00:09:58,220 --> 00:10:00,130
所以实际上那还不错。

158
00:10:00,200 --> 00:10:06,350
但是在这里，例如，火车中的这个红色矩形得到了一些红点，因为

159
00:10:06,350 --> 00:10:11,040
在培训中心进行了重新培训，从中我们可以清楚地看到一些观察结果。

160
00:10:11,070 --> 00:10:14,650
幸运的是，这里的新观察结果恰好是绿色的。

161
00:10:14,720 --> 00:10:19,790
因此，对于这个红色矩形来说，这太糟糕了，它不适用于某些新观察结果，尤其是这些

162
00:10:19,790 --> 00:10:20,590
这里的。

163
00:10:21,110 --> 00:10:26,300
就是说，例如对于这个矩形，这里没有任何捕捉，因为没有

164
00:10:26,600 --> 00:10:32,250
测试的新观察点说，例如在该红色中没有测试集的新用户，因此这是

165
00:10:32,330 --> 00:10:33,950
在这里绝对有用。

166
00:10:33,950 --> 00:10:36,780
这只是为了过度适应训练条件。

167
00:10:36,800 --> 00:10:43,490
因此，我们必须对此谨慎，但是请不要忘记本部分的最后部分是关于评估

168
00:10:43,550 --> 00:10:46,790
并改进分类模型。

169
00:10:46,790 --> 00:10:48,130
所以坚持下去。

170
00:10:48,410 --> 00:10:54,860
顺便说一句，我们仍然可以说这个决定，Drats对我们进行分类，因为它实际上是

171
00:10:54,860 --> 00:11:00,080
做得好，顺便说一句，我们发现了九个错误的预测，您可以对它们进行计数，而您将实际计算出

172
00:11:00,260 --> 00:11:04,910
绿色区域中有九个红色点，红色区域中有绿色点。

173
00:11:05,190 --> 00:11:11,480
好，现在我很想知道您喜欢哪种分类，因为我们即将结束分类

174
00:11:11,480 --> 00:11:12,220
旅程。

175
00:11:12,250 --> 00:11:15,630
实际上，我们被介绍了很多分类器。

176
00:11:15,710 --> 00:11:17,270
我们只剩一个。

177
00:11:17,270 --> 00:11:18,680
这是随机森林。

178
00:11:18,740 --> 00:11:22,580
因此，您了解这是一棵决策树。

179
00:11:22,580 --> 00:11:27,740
因此，想象一下，如果我们对500个决策树进行相同的分类工作。

180
00:11:27,830 --> 00:11:29,880
想象结果如何。

181
00:11:30,080 --> 00:11:35,330
尝试已经在您的脑海中预测出您会通过直觉部分了解到随机森林

182
00:11:35,390 --> 00:11:39,110
基于树木的多数票。

183
00:11:39,110 --> 00:11:45,980
因此，既然这是一棵树，请想象一下一个团队喜欢做什么，通常情况下，决策树将对您的树进行分类

184
00:11:45,980 --> 00:11:49,250
这一点我会让你知道的。

185
00:11:49,730 --> 00:11:49,970
好。

186
00:11:49,970 --> 00:11:51,140
这就是故事的结局。

187
00:11:51,140 --> 00:11:54,330
好吧，我很高兴向您介绍这棵树。

188
00:11:54,350 --> 00:11:55,860
这实际上是一个非常酷的模型。

189
00:11:55,860 --> 00:11:57,420
我非常喜欢这个。

190
00:11:57,560 --> 00:12:03,320
并且不要忘记决策树非常适合于解释，因为您可以构建一些树

191
00:12:03,320 --> 00:12:10,040
为了清楚地看到进行分类的条件，我们实际上将这棵树放在我们的

192
00:12:10,100 --> 00:12:14,910
因为您会看到我们可以非常快地在一行甚至两句话中做到这一点。

193
00:12:14,960 --> 00:12:18,110
所以我们会这样做，或者我们想邀请他来这里。

194
00:12:18,110 --> 00:12:21,560
我只是想向您展示预测区域，但我们将在其上进行。

195
00:12:21,770 --> 00:12:28,010
我迫不及待地向您展示分类的树木和雨林将如何为您做分类

196
00:12:28,020 --> 00:12:28,610
。

197
00:12:29,210 --> 00:12:30,830
感谢您观看本教程。

198
00:12:30,920 --> 00:12:32,950
我期待在下一个见到你。

199
00:12:32,960 --> 00:12:34,880
在此之前，请享受机器学习。

