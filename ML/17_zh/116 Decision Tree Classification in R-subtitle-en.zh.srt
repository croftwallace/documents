1
00:00:00,360 --> 00:00:06,200
您好，欢迎来到Tauriel，我非常兴奋，因为今天我们要实现这一目标

2
00:00:06,540 --> 00:00:08,470
我们的分类模型。

3
00:00:08,640 --> 00:00:14,250
我们只是邀请那些在这里的人而感到兴奋，这是因为我们会发现

4
00:00:14,250 --> 00:00:20,250
一种非常新的分类，您会看到预测区域和预测边界

5
00:00:20,370 --> 00:00:22,190
是您尚未见过的东西。

6
00:00:22,380 --> 00:00:24,420
因此，让我们立即了解一下。

7
00:00:24,750 --> 00:00:27,490
因此，我们将尽快将文件夹设置为目录。

8
00:00:27,690 --> 00:00:31,090
第3部分分类决策树分类。

9
00:00:31,110 --> 00:00:35,470
这是为了确保您具有社交网络并查看文件。

10
00:00:35,550 --> 00:00:39,520
然后单击此处的更多按钮，将文件夹设置为工作目录。

11
00:00:39,540 --> 00:00:50,490
然后，让我们快速使用我们的模板，从此处选择所有内容，直至底部副本，然后将其粘贴到此处

12
00:00:50,500 --> 00:00:51,180
。

13
00:00:51,900 --> 00:00:52,620
好吧。

14
00:00:52,990 --> 00:00:55,950
现在，让我们更改一些内容。

15
00:00:55,950 --> 00:00:59,420
因此，不要忘记这会改变情节中的标题。

16
00:00:59,640 --> 00:01:09,820
因此，我们还将在此处用决策树替换分类器。

17
00:01:12,530 --> 00:01:13,350
好。

18
00:01:13,480 --> 00:01:20,910
现在让我们创建分类器，这样一来，我们将再次使用创建决策树分类器的技巧

19
00:01:21,090 --> 00:01:25,070
最受欢迎的库是我们的零件库。

20
00:01:25,440 --> 00:01:30,070
因此，现在只需检查包裹中是否有机场图书馆即可。

21
00:01:30,090 --> 00:01:32,980
例如，我的就在这里。

22
00:01:33,030 --> 00:01:36,030
如果您是初学者或第一次，那么情况可能并非如此。

23
00:01:36,150 --> 00:01:40,600
因此，我将为需要安装它的那些人编写此行代码。

24
00:01:40,740 --> 00:01:44,940
和往常一样，它是安装软件包。

25
00:01:45,180 --> 00:01:51,910
然后在括号中的引号中输入包装的名称，然后将其作为我们的零件。

26
00:01:51,960 --> 00:01:52,740
好吧。

27
00:01:52,830 --> 00:01:56,350
然后在其中安装软件包以选择此行并执行。

28
00:01:56,550 --> 00:02:02,130
我现在不会这样做，因为我的行李已经安装好了，所以我将其作为注释并

29
00:02:02,490 --> 00:02:10,140
但是，我们将在库中包括此行代码，并在括号中将我们的部分自动

30
00:02:10,140 --> 00:02:15,780
选择该库，因为一旦执行该库，就将其选中。

31
00:02:15,960 --> 00:02:20,610
如您现在所见，它没有被选中，但是一旦执行就将被选中。

32
00:02:21,070 --> 00:02:21,710
好。

33
00:02:21,960 --> 00:02:25,180
现在，我们准备创建分类器。

34
00:02:25,290 --> 00:02:32,760
所以让我们像平常一样做这个分类器，然后我们将实际使用一个

35
00:02:32,760 --> 00:02:34,920
与库或部分相同。

36
00:02:34,980 --> 00:02:42,420
因此，对于我们这一部分和该函数，我们将正确地放置正确的参数，如您现在所见，我们可以

37
00:02:42,420 --> 00:02:44,320
看看这些参数是什么。

38
00:02:44,430 --> 00:02:50,010
但是，如果您需要更多信息，我们可以单击此处并按一个。

39
00:02:50,580 --> 00:02:57,450
在这里，我们只需要单击此处以获取有关我们零件的一些信息，我想您会看到第一个

40
00:02:57,480 --> 00:02:59,490
参数是公式。

41
00:02:59,490 --> 00:03:05,250
因此，这是我们必须输入的参数，并且像往常一样，我们将写公式等于

42
00:03:05,250 --> 00:03:11,700
变量，直到数据中的点是替换自变量之和的快捷方式

43
00:03:11,700 --> 00:03:13,720
是写公式的另一种方式。

44
00:03:13,920 --> 00:03:21,240
所以这和通常一样，然后我们在这里有data参数，当然是

45
00:03:21,240 --> 00:03:22,890
您想训练您的分类器。

46
00:03:22,890 --> 00:03:25,260
因此，这些数据将成为训练集。

47
00:03:25,260 --> 00:03:27,050
好的，让我们以参数结尾。

48
00:03:27,210 --> 00:03:34,580
所以我记得第一个带有公式等于购买的参数。

49
00:03:34,620 --> 00:03:37,340
那是因变量直到。

50
00:03:37,400 --> 00:03:45,900
我只想按一下，然后点一个包含所有变量的点，然后出现，然后将

51
00:03:45,900 --> 00:03:52,820
第二个参数，它记住了它，我们选择了完美的火车。

52
00:03:52,900 --> 00:03:56,340
现在让我们执行整个代码。

53
00:03:56,340 --> 00:04:01,590
因此，首先我们像往常一样在这里执行此预处理部分。

54
00:04:01,590 --> 00:04:02,810
完善。

55
00:04:02,910 --> 00:04:07,800
因此，我们可以看一下find的数据集数据集，其中两个独立变量的年龄为

56
00:04:07,800 --> 00:04:11,810
薪水和我们的因变量追逐训练集。

57
00:04:11,910 --> 00:04:14,410
一切都很好，测试集也都不错。

58
00:04:14,610 --> 00:04:15,240
好。

59
00:04:15,420 --> 00:04:21,990
所以训练集和测试集是尺度，因为我们将用

60
00:04:21,990 --> 00:04:24,090
高分辨率，所以我们需要扩展。

61
00:04:24,090 --> 00:04:29,160
实际上，您可以尝试在此处不缩放自变量，因为对于决策树

62
00:04:29,160 --> 00:04:34,650
您不需要扩展自变量，因为决策树模型不是基于欧几里得

63
00:04:34,650 --> 00:04:39,990
距离，但由于我们要以高分辨率绘制预测区域，因此您会看到

64
00:04:39,990 --> 00:04:44,170
代码将比不扩展时更快地执行大量时间。

65
00:04:44,190 --> 00:04:51,030
实际上，我认为如果您不扩展代码可能会破坏代码，则可以尝试但要小心。

66
00:04:51,030 --> 00:04:57,150
因此，我们将执行此操作，然后再次执行代码，而无需进行缩放以插入树。

67
00:04:57,360 --> 00:04:58,670
因此，我们将清除所有内容。

68
00:04:58,830 --> 00:05:03,020
然后，预处理部件会选择除特征缩放以外的所有内容。

69
00:05:03,120 --> 00:05:06,120
然后，我们将以非常简单的方式绘制树。

70
00:05:06,360 --> 00:05:08,530
但是现在我们要在预测区域中进行绘制。

71
00:05:08,550 --> 00:05:10,300
因此，我们缩放自变量。

72
00:05:10,510 --> 00:05:20,070
好了，现在Castro已经准备好了，所以让我们执行好吧，现在我们可以执行

73
00:05:20,070 --> 00:05:26,800
行来预测测试结果，而实际上有趣的是，为什么pred与什么不一样

74
00:05:26,800 --> 00:05:27,820
我们习惯了。

75
00:05:27,810 --> 00:05:29,930
首先例如我们可以了解为什么在这里面包。

76
00:05:29,940 --> 00:05:34,480
记住，在这里没有白面包之前，我们必须在控制台中键入它才能查看

77
00:05:34,480 --> 00:05:34,980
在它。

78
00:05:35,250 --> 00:05:36,190
这里是这里。

79
00:05:36,190 --> 00:05:39,440
因此，让我们单击它以了解它是什么。

80
00:05:39,450 --> 00:05:45,560
好的，这就是为什么pred，它实际上是两列一百行的矩阵。

81
00:05:45,810 --> 00:05:47,670
那么，为什么有什么新的祷告。

82
00:05:47,880 --> 00:05:48,720
究竟是什么。

83
00:05:48,730 --> 00:05:56,350
就像您看到的那样，每一行中的两个单元格中的一些等于一个。

84
00:05:56,350 --> 00:05:58,610
所以你能猜出它是什么。

85
00:05:59,110 --> 00:06:00,740
这些都是概率。

86
00:06:00,850 --> 00:06:05,770
第一列给出用户所属观察值不能为零的概率。

87
00:06:05,820 --> 00:06:08,080
那不是买SUV。

88
00:06:08,280 --> 00:06:15,490
第二列中的此概率是用户购买SUV属于类别的概率

89
00:06:15,490 --> 00:06:17,820
一种是由SUV。

90
00:06:17,820 --> 00:06:22,120
因此，在这里，如果您看一下第一个观察结果，您会发现很有可能

91
00:06:22,120 --> 00:06:27,390
用户购买SUV 0.26的可能性很小。

92
00:06:27,390 --> 00:06:32,430
当然，因为总和等于1，所以在这里意味着这里的预测是

93
00:06:32,440 --> 00:06:34,810
用户不购买SUV。

94
00:06:34,810 --> 00:06:41,080
如果您查看此处的测试并查看索引零，则实际上用户可以看到

95
00:06:41,070 --> 00:06:48,300
没有购买SUV，因此预测是正确的，但这是因为我们有白面包

96
00:06:48,310 --> 00:06:54,360
模板中的方式，如果您不希望这种格式的白面包，那么您只需要添加一个简单的

97
00:06:54,370 --> 00:06:59,340
参数是类型等于类型的头发。

98
00:06:59,500 --> 00:07:01,920
您只需要输入课程。

99
00:07:02,380 --> 00:07:07,940
好的，让我们尝试再次执行此行，这就是原因。

100
00:07:07,950 --> 00:07:12,690
让我们看一下为什么Prit现在广为流传。

101
00:07:12,690 --> 00:07:13,290
妳去

102
00:07:13,300 --> 00:07:14,710
现在为什么要使用Pretis载体。

103
00:07:14,750 --> 00:07:21,100
因此，您可以看到针对测试的每个观察结果，这些观察结果适用于我们之前喜欢的测试集的每个用户

104
00:07:21,490 --> 00:07:25,430
每个用户0的预测为零或一。

105
00:07:25,440 --> 00:07:32,470
如果预测用户不购买SUV，则预测用户不购买SUV

106
00:07:32,710 --> 00:07:35,070
到我们的决策树分类器。

107
00:07:35,650 --> 00:07:35,910
好。

108
00:07:35,930 --> 00:07:38,400
因此，这里需要更改一点。

109
00:07:38,400 --> 00:07:44,830
确保您知道您的妻子面包是您得出的零或一结果的从属向量

110
00:07:44,820 --> 00:07:50,820
过去是因为如您所见，我们使用了带有两个参数classifier和new的相同乘积函数

111
00:07:50,820 --> 00:07:52,710
数据等于网格集。

112
00:07:52,890 --> 00:07:59,270
因此，这意味着它将不起作用，因为它应该是预测结果的向量。

113
00:07:59,440 --> 00:08:05,330
只是这一次，您才知道所有像素点在网格中的虚构像素点用户

114
00:08:05,390 --> 00:08:10,650
Predict函数与分类器（决策树气化器）相关联。

115
00:08:10,730 --> 00:08:15,400
如果我们仅在此处保留这两个参数，那么这将是有意义的，因为它将返回

116
00:08:15,660 --> 00:08:22,330
WEIGERT作为两个概率的矩阵，因此这里将存在一些问题，因为它将

117
00:08:22,320 --> 00:08:26,640
是一个矩阵的一个矩阵，而这里应该是一个向量。

118
00:08:26,640 --> 00:08:33,500
因此，我们希望他们做的事情现在就要做，这样我们就不会忘记添加此类型参数

119
00:08:33,900 --> 00:08:39,120
然后将其设置为与class相等，然后它将完美运行。

120
00:08:39,390 --> 00:08:45,870
因此，我将复制并在此处添加。

121
00:08:45,880 --> 00:08:46,290
完善。

122
00:08:46,310 --> 00:08:47,690
现在它已经准备好了。

123
00:08:47,790 --> 00:08:51,010
它将插入图表而没有任何错误。

124
00:08:51,000 --> 00:08:57,210
所以我知道我给您提供了一个模板，该模板应该可以正常工作，而无需对分类进行任何更改

125
00:08:57,220 --> 00:08:57,610
。

126
00:08:57,610 --> 00:09:02,620
抱歉，有时我们需要更改一些内容，因此我们需要您执行

127
00:09:02,710 --> 00:09:06,810
每一行都一个接一个地查看是否正确。

128
00:09:07,000 --> 00:09:12,180
在这里，我们发现由于决策树分类器的这种稍有不同的结构

129
00:09:12,180 --> 00:09:12,350
。

130
00:09:12,660 --> 00:09:18,790
好吧，如果我们扩展了先前的预测，我们需要进行调整以获得正确的向量

131
00:09:18,780 --> 00:09:19,490
分类器。

132
00:09:19,530 --> 00:09:25,530
因此，我们通过在此处添加此类型参数并将其设置为相等来对决策树进行调整

133
00:09:25,530 --> 00:09:26,900
上课。

134
00:09:26,920 --> 00:09:29,210
现在，我们有了所需的白色骄傲系数。

135
00:09:29,560 --> 00:09:34,890
而且是的，如果您知道计算混淆矩阵，我们将遇到一些问题

136
00:09:34,890 --> 00:09:38,280
用这种方式为什么要提前概率矩阵。

137
00:09:38,330 --> 00:09:42,270
但是现在就可以了，因为白面包的设置正确。

138
00:09:42,610 --> 00:09:46,230
因此，我们将执行此操作，并查看不正确的制作数量。

139
00:09:46,240 --> 00:09:47,000
好吧。

140
00:09:47,130 --> 00:09:54,840
现在让我们输入C.M. 在这里，我们有6加11 UKL 17个错误的预测。

141
00:09:54,850 --> 00:10:01,200
现在，让我们看看我们是否正确地以这种方式更改代码，以便可以绘制图形。

142
00:10:01,200 --> 00:10:02,680
让我们看看它是否会起作用。

143
00:10:02,700 --> 00:10:08,430
我希望它能起作用，因为我想向您展示决策树的预测区域和预测边界

144
00:10:08,430 --> 00:10:08,460
。

145
00:10:08,460 --> 00:10:09,990
我真的很想给你看。

146
00:10:10,060 --> 00:10:13,370
对于那些不喜欢自行车和推特的人。

147
00:10:13,540 --> 00:10:19,470
因此，让我们选择它，看看我们是否做得很好。

148
00:10:19,480 --> 00:10:20,420
好吧，看起来不错。

149
00:10:20,430 --> 00:10:22,830
到目前为止看起来不错，没有错误。

150
00:10:22,840 --> 00:10:24,190
让我们看看发生了什么。

151
00:10:26,140 --> 00:10:31,750
我们是对的，这很好地运行了，这是对决策树的测试。

152
00:10:31,930 --> 00:10:33,880
那就是预测边界。

153
00:10:34,070 --> 00:10:37,940
因此，您可以看到只有水平和垂直线。

154
00:10:37,960 --> 00:10:44,470
这是因为正如Karylle解释的那样，密度算法是基于您的独立条件

155
00:10:44,470 --> 00:10:44,920
变量。

156
00:10:44,920 --> 00:10:51,000
通过发现您知道每个时间间隔，这些时间间隔将使条件分类为某些矩形

157
00:10:51,370 --> 00:10:57,610
您的观察结果，实际上有趣的是，我们明显比Python拥有更少的过拟合

158
00:10:57,620 --> 00:10:57,750
。

159
00:10:57,880 --> 00:11:04,690
实际上，这就是为什么我们会有更多不正确的预测的原因，因为在python中，您知道红色矩形

160
00:11:04,690 --> 00:11:06,700
这里的红色矩形。

161
00:11:06,700 --> 00:11:10,720
这里和这里也有红色矩形。

162
00:11:10,720 --> 00:11:16,420
我们实际上没有指定更多参数，但是这些了不起的Archerd库，这就是为什么它非常

163
00:11:16,420 --> 00:11:22,870
流行选择了正确的参数径向参数知道事前过度拟合，因为这里

164
00:11:23,080 --> 00:11:24,750
我们显然没有过度拟合。

165
00:11:24,850 --> 00:11:29,500
我们非常适合Python，因为这里我们拼命试图绘制所有红色矩形

166
00:11:29,500 --> 00:11:31,910
抓住正确类别中的每个用户。

167
00:11:32,030 --> 00:11:34,490
但是这里不是这种情况。

168
00:11:34,600 --> 00:11:39,430
您知道这里所做的工作非常出色，可以正确地对大多数正确的点进行分类。

169
00:11:39,430 --> 00:11:44,870
此处的大多数绿点都位于正确的区域，而此处的这些绿用户无法

170
00:11:44,900 --> 00:11:52,680
争取线性分类器，例如将回归或线性核作为VM。

171
00:11:52,870 --> 00:11:55,900
因此，这是一项很好的工作，但我们仍然有一些不正确的保护措施。

172
00:11:55,900 --> 00:11:58,240
那是因为很难分类。

173
00:11:58,240 --> 00:12:01,710
好吧，如果您想防止数据过度拟合。

174
00:12:01,990 --> 00:12:07,210
因此，即使我们有17个正确的加法，这也是一个很好的分类。

175
00:12:07,300 --> 00:12:08,300
我们在这里。

176
00:12:08,820 --> 00:12:14,170
好的，但是现在让我们看一下测试结果，实际上我并不太担心，因为

177
00:12:14,170 --> 00:12:19,420
我们这里没有过度拟合，那么这意味着我们也很可能会获得一些良好的结果

178
00:12:19,480 --> 00:12:20,180
在测试中。

179
00:12:20,230 --> 00:12:21,880
让我们来看看。

180
00:12:22,000 --> 00:12:24,710
测试并执行。

181
00:12:24,740 --> 00:12:26,770
这就对了。

182
00:12:28,480 --> 00:12:29,470
这是测试集。

183
00:12:29,530 --> 00:12:29,990
好。

184
00:12:30,100 --> 00:12:32,440
因此，正如我告诉您的那样，这看起来非常好。

185
00:12:32,770 --> 00:12:37,660
这是我们拥有的这17条法规的集合，您可以根据需要对其进行计数。

186
00:12:37,660 --> 00:12:40,140
您会找到17并将其分类。

187
00:12:40,150 --> 00:12:44,980
红色区域中的大多数红色用户，以及红色区域中的大多数红色用户。

188
00:12:44,980 --> 00:12:50,070
顺便说一句，我们可以看到大多数不正确的预测都在这里。

189
00:12:50,080 --> 00:12:54,920
我们可以看到在绿色区域中有许多红点，这很不幸。

190
00:12:54,950 --> 00:13:01,870
但是正如我告诉您的那样，我们宁愿防止过度拟合，也不愿尝试将过剩的数量最小化为零。

191
00:13:01,870 --> 00:13:03,450
错误的预测。

192
00:13:03,460 --> 00:13:10,960
因此，存在另一种新的分类器，因此还有另一种新的预测区域和预测

193
00:13:10,960 --> 00:13:11,560
边界。

194
00:13:11,590 --> 00:13:17,620
现在在下一节中，您将了解实际上是决策团队的随机森林

195
00:13:17,620 --> 00:13:23,260
您为我们所知的许多决策树的树可以包含500棵树，因此每个决策

196
00:13:23,260 --> 00:13:29,740
团队树将为每个数据点投票此数据点是否属于类别0 0类​​别1

197
00:13:29,740 --> 00:13:29,920
。

198
00:13:29,920 --> 00:13:36,940
也就是说，如果该用户购买了SUV或未购买SUV，并且根据

199
00:13:36,940 --> 00:13:42,880
多数表决将决定每个人属于红色区域还是绿色区域。

200
00:13:42,910 --> 00:13:48,160
因此，我告诉您这是因为您知道，从这里开始，我们的工作只有一个决策树。

201
00:13:48,160 --> 00:13:53,050
想象一下决策树（如500,000棵树）的工作。

202
00:13:53,050 --> 00:13:55,860
想象一下他可以如何对结果进行分类。

203
00:13:56,080 --> 00:13:58,720
但是，我们不要对此太热心。

204
00:13:58,720 --> 00:14:01,300
请记住，我们要防止过度捕捞。

205
00:14:01,450 --> 00:14:04,430
因此，让我们找出答案。

206
00:14:04,450 --> 00:14:04,980
好吧。

207
00:14:05,030 --> 00:14:07,250
现在该花一点额外的奖金了。

208
00:14:07,360 --> 00:14:09,640
让我们插入决策树。

209
00:14:09,640 --> 00:14:14,970
因此，为了获得最佳解释，我们将删除特征缩放并执行此操作

210
00:14:14,980 --> 00:14:19,430
我们将清除所有内容并重新执行代码。

211
00:14:19,620 --> 00:14:21,230
但是没有鱼就可以。

212
00:14:21,400 --> 00:14:22,240
因此，让我们这样做。

213
00:14:22,240 --> 00:14:26,680
单击此处的此按钮，我们将清除此处的数据。

214
00:14:26,690 --> 00:14:30,250
是的，那么插头在这里。

215
00:14:30,520 --> 00:14:31,760
是。

216
00:14:31,780 --> 00:14:34,690
然后通过键入control杀死控制台。

217
00:14:34,760 --> 00:14:36,570
好了，我们去。

218
00:14:36,590 --> 00:14:38,140
现在一切都清楚了。

219
00:14:38,230 --> 00:14:41,070
我们可以执行整个脚本。

220
00:14:41,080 --> 00:14:42,790
因此，让我们逐步进行。

221
00:14:42,790 --> 00:14:49,270
我们首先要执行所有预处理步骤，但是如果没有这些功能，您就可以

222
00:14:49,270 --> 00:14:51,060
看到我没有选择这个。

223
00:14:51,130 --> 00:14:55,020
好的，所以现在命令控制请百分比很可爱。

224
00:14:55,180 --> 00:14:56,200
都好。

225
00:14:56,200 --> 00:15:03,700
现在您可以看到，如果我们转到数据集训练集和测试集，则可以看到这些功能

226
00:15:03,790 --> 00:15:05,390
不再缩放。

227
00:15:05,710 --> 00:15:05,980
好吧。

228
00:15:05,980 --> 00:15:08,820
因此，例如，在训练集中，我们具有实际年龄。

229
00:15:08,830 --> 00:15:14,630
我的意思是说，用他们的实际价值和实际估计的工资，实际价值还可以。

230
00:15:14,710 --> 00:15:15,550
没有规模。

231
00:15:15,580 --> 00:15:21,550
现在，让我们选择它以适合决策树，将其分类为训练集。

232
00:15:21,580 --> 00:15:24,970
因此命令控制控制器并执行它。

233
00:15:25,180 --> 00:15:37,220
现在通过我将要用“决策树”分类代替它的方式来适应它。

234
00:15:37,260 --> 00:15:38,720
好吧。

235
00:15:38,740 --> 00:15:42,810
我们现在真正感兴趣的是绘制决策树。

236
00:15:42,820 --> 00:15:50,710
因此，我们将在下面添加一个新部分，我将其称为“血腥的决定”

237
00:15:50,710 --> 00:15:52,280
树。

238
00:15:53,710 --> 00:15:58,960
正如我在Python教程中告诉您的那样，这将需要两行。

239
00:15:58,960 --> 00:16:00,100
这很简单。

240
00:16:00,100 --> 00:16:01,260
因此，让我们开始吧。

241
00:16:01,270 --> 00:16:03,600
这将很快。

242
00:16:03,610 --> 00:16:08,920
再说一遍，除了我们需要编写的两行代码之外，因为我们确实需要

243
00:16:08,920 --> 00:16:11,310
在这里键入的是情节。

244
00:16:11,500 --> 00:16:19,240
然后在括号分类器中，因为您知道分类器这是我们创建的分类器

245
00:16:19,780 --> 00:16:26,800
在这一部分中，我们使用Arbre库的part函数，仅需输入

246
00:16:26,800 --> 00:16:29,400
使用plup分类器，将绘制树。

247
00:16:29,530 --> 00:16:33,880
但是没有条件的标签没有明确的理由。

248
00:16:33,880 --> 00:16:42,810
因此，为了添加这些明确编写的条件，我们只需要在文本分类器下方添加即可。

249
00:16:42,820 --> 00:16:43,340
开始了。

250
00:16:43,360 --> 00:16:44,560
现在准备就绪。

251
00:16:44,650 --> 00:16:46,020
没那么简单。

252
00:16:46,040 --> 00:16:50,000
只有两行，我们将绘制一个可解释的决策树。

253
00:16:50,200 --> 00:16:51,350
让我们来看看。

254
00:16:51,670 --> 00:16:58,850
我将选择所有这一切，然后按Command Control按Enter执行。

255
00:16:59,440 --> 00:17:00,820
这是一棵树。

256
00:17:00,940 --> 00:17:06,620
因此，正如您所看到的，我们在每个拆分中都有生成拆分的条件。

257
00:17:06,820 --> 00:17:14,770
因此，例如，根据年龄低于四十四分之五的条件获得第一名

258
00:17:14,920 --> 00:17:15,710
岁。

259
00:17:16,000 --> 00:17:23,890
因此，这意味着，如果用户年龄在40岁以下，4.5岁以下，则他将在

260
00:17:23,890 --> 00:17:30,500
拆分，如果用户年龄超过四十四岁五岁，他将最终属于此类别

261
00:17:30,510 --> 00:17:37,240
这个地方的位置，然后我们在这里有一些新条件，另一个自变量有新条件

262
00:17:37,240 --> 00:17:37,400
。

263
00:17:37,420 --> 00:17:44,170
在这种情况下的估计薪金为估计薪金低于$ 90000。

264
00:17:44,380 --> 00:17:50,800
因此，这意味着如果用户年龄不到四十四岁五岁，并且估计

265
00:17:50,800 --> 00:17:58,900
如果工资低于$ 90000，则根据我们的决策树分类器，该用户将不会购买SUV，因为

266
00:17:59,200 --> 00:18:00,830
这里的结果为零。

267
00:18:01,180 --> 00:18:08,650
并且如果该用户还不到四十四岁和五岁，并且估计工资超过$ 90000

268
00:18:09,070 --> 00:18:14,770
然后根据我们的决策树分类器，该用户将购买SUV，因为这里的结果是

269
00:18:14,770 --> 00:18:15,810
一。

270
00:18:15,850 --> 00:18:21,880
同样，如果您很好地转到树的另一侧，则树的另一侧首先包含所有

271
00:18:21,880 --> 00:18:25,390
年龄大于四十四岁的用户。

272
00:18:25,630 --> 00:18:29,210
然后我们有了一些新条件，产生了新的分裂。

273
00:18:29,230 --> 00:18:34,900
所以这里的年龄的另一个条件，然后是对不起的另一个条件，然后又是另一个

274
00:18:34,900 --> 00:18:40,980
军事条件，并遵循是或否这些条件，我们最终归结为一些最终类别

275
00:18:40,990 --> 00:18:48,130
决策树的最终节点，预计用户不会为此节点购买SUV，并且

276
00:18:48,130 --> 00:18:52,670
预计要购买SUV的“此节点”和“此节点”以及此注释。

277
00:18:52,940 --> 00:18:53,310
好。

278
00:18:53,320 --> 00:18:55,650
因此，值得一看。

279
00:18:55,840 --> 00:19:00,280
我们有点探索了可在幕后分类的决策树。

280
00:19:00,400 --> 00:19:05,320
现在，在使用决策树时要了解的重要一点是，

281
00:19:05,320 --> 00:19:11,860
这些分类器，我们可以得到这些非常可解释的结果，因为在此决策树上

282
00:19:11,860 --> 00:19:14,440
情节，我们可以看到正在发生的一切。

283
00:19:14,530 --> 00:19:21,810
我们可以看到决策树如何决定将预测用户是不购买SUV还是购买

284
00:19:21,810 --> 00:19:22,380
SUV。

285
00:19:22,450 --> 00:19:29,080
我们清楚地看到了决策树的整个思维过程，并且以某种方式看到了它是如何学习的

286
00:19:29,230 --> 00:19:35,950
从数据中如何对我们每个社交网络用户进行分类。

287
00:19:35,950 --> 00:19:39,930
好的，我期待与您在下一节中为我们跑步。

288
00:19:39,970 --> 00:19:45,400
我们将为我们提供有关Python和我们的培训，我迫不及待地向您展示最终的图形结果

289
00:19:45,970 --> 00:19:46,860
直到下一次。

290
00:19:46,900 --> 00:19:47,900
享受机器学习

