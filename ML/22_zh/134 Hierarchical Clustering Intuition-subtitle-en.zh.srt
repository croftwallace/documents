1
00:00:00,730 --> 00:00:03,330
您好，欢迎回到机器学习课程。

2
00:00:03,420 --> 00:00:07,650
在本节中，我们将启动分层集群的教程。

3
00:00:07,650 --> 00:00:13,380
好的，所以我们面前还有一个非常有趣的话题，一如既往，让我们将复杂的事物简化

4
00:00:13,410 --> 00:00:16,350
并揭开一切的神秘面纱。

5
00:00:16,350 --> 00:00:18,640
好的，那么什么是层次集群。

6
00:00:18,810 --> 00:00:20,000
信不信由你。

7
00:00:20,130 --> 00:00:27,180
但是，如果您在散点图中有数据点或像我们之前看过的数据点一样，这是两个

8
00:00:27,180 --> 00:00:28,200
维空间。

9
00:00:28,380 --> 00:00:32,790
如果您应用分层聚类，或者只是说H.C. 简而言之。

10
00:00:32,800 --> 00:00:40,620
将会发生的事情是，您将再次获得与Kamins非常相似的集群。实际上有时结果

11
00:00:40,950 --> 00:00:45,510
没有结果可以与k均值聚类完全相同。

12
00:00:45,720 --> 00:00:48,260
但是整个过程有些不同。

13
00:00:48,270 --> 00:00:50,510
因此，让我们更详细地讨论它。

14
00:00:50,760 --> 00:00:55,530
因此，我们需要注意的第一件事是存在两种类型的层次聚类

15
00:00:55,950 --> 00:00:57,070
和分裂。

16
00:00:57,080 --> 00:01:03,000
自下而上的方法就是这样，您现在将更详细地看到这意味着什么

17
00:01:03,000 --> 00:01:07,680
我们将从最底部开始，然后从相反的位置开始建立与他相反的设备

18
00:01:07,680 --> 00:01:14,580
在此课程中，我们将重点放在方法的glomerata上

19
00:01:15,000 --> 00:01:19,890
分歧，或者我们现在将在谈论毛刺的地方提到埃比兹，但基本上

20
00:01:19,890 --> 00:01:20,570
一样的东西。

21
00:01:20,580 --> 00:01:23,380
但是绕过它的另一种方式是倒钩之旅。

22
00:01:23,550 --> 00:01:27,220
而且，如果您愿意，您绝对可以对设备方法进行更多研究。

23
00:01:27,330 --> 00:01:30,880
但是目前，我们将专注于其中之一。

24
00:01:30,900 --> 00:01:31,190
好吧。

25
00:01:31,200 --> 00:01:35,850
因此，层次聚类的光荣开始如何运作良好，我们将要打破

26
00:01:35,850 --> 00:01:40,440
逐步介绍它，然后我们看一个示例并手动执行教室

27
00:01:40,450 --> 00:01:40,670
。

28
00:01:40,890 --> 00:01:47,260
好的，所以在H.C.的第一步 是使每个数据点成为单个点簇，以便形成和或

29
00:01:47,520 --> 00:01:48,620
和数据点。

30
00:01:48,630 --> 00:01:53,560
第一步是将它们中的每一个视为一个单独的集群。

31
00:01:53,640 --> 00:01:59,910
然后，第二步是获取两个最接近的数据点，并使其成为一个簇以将它们合并为

32
00:01:59,910 --> 00:02:05,910
形成一个集群并减去一个集群，然后饮以将两个最接近的类从中剔除

33
00:02:05,910 --> 00:02:11,800
现在您拥有的，并使它们成为一个集群，然后减去两个集群，那就是

34
00:02:11,820 --> 00:02:18,060
重复第3步，直到只有一个班级，基本上只是重复第3步并合并

35
00:02:18,300 --> 00:02:21,310
指向越来越大的集群。

36
00:02:21,360 --> 00:02:24,180
因此，只剩下一个庞大的集群。

37
00:02:24,180 --> 00:02:26,110
因此，您只需重复第三步。

38
00:02:26,190 --> 00:02:33,810
最后，您已经完成，您将剩下一个庞大的集群，以及如何从一变为二或三

39
00:02:33,810 --> 00:02:34,290
类。

40
00:02:34,290 --> 00:02:36,390
您如何获得最终想要的最终结果。

41
00:02:36,390 --> 00:02:40,660
我们还将在本节中讨论这一点，因此这就是当然的目标。

42
00:02:40,800 --> 00:02:47,490
但是这里突出的一件事是最接近的簇。

43
00:02:47,700 --> 00:02:52,580
现在我们已经讨论了距离，并且我们清楚地提到了您可以使用欧几里得距离

44
00:02:52,580 --> 00:02:56,160
或其他距离，当您使用通常的点进行工作时完全可以。

45
00:02:56,160 --> 00:03:01,500
但是在这里，我们实际上更进一步，我们不仅在讨论点的接近

46
00:03:01,530 --> 00:03:03,730
但实际上是群集附近。

47
00:03:03,810 --> 00:03:09,270
这是值得注意的事情，所以我想在这里稍作停留，或者我有点迈向

48
00:03:09,270 --> 00:03:14,970
一边讨论簇的紧密度以及如何测量簇之间的距离，因为

49
00:03:14,970 --> 00:03:19,020
如果使用分层聚类，那确实会影响您的算法。

50
00:03:19,020 --> 00:03:21,530
因此，让我们讨论几分钟。

51
00:03:21,540 --> 00:03:27,180
所以首先是欧几里德距离只是为了一劳永逸地解决这个问题

52
00:03:27,180 --> 00:03:30,120
在二维空间中，其计算方式如下。

53
00:03:30,120 --> 00:03:37,110
因此，如果得到两个点，且坐标为x 1和Y 1 p来将x坐标为

54
00:03:37,260 --> 00:03:44,040
y到核，距离或这条线的长度计算为X到负x 1，这样

55
00:03:44,040 --> 00:03:49,980
x的平方之间的距离加上Y的平方之间的距离。

56
00:03:49,980 --> 00:03:53,160
所以基本上，然后将它们加起来然后取平方。

57
00:03:53,160 --> 00:03:58,960
所以基本上就是这样，你在这里有一个直角三角形，你已经把牛赶出去了

58
00:03:59,370 --> 00:04:01,800
在这里，您在这里还有另外一根肠。

59
00:04:01,830 --> 00:04:06,510
我希望我的发音正确，然后这就是假设的权利。

60
00:04:06,510 --> 00:04:10,860
这样便可以计算出两点之间的距离。

61
00:04:10,860 --> 00:04:14,680
因此，这是中学时代的基本几何图形。

62
00:04:14,850 --> 00:04:20,220
这就是欧几里得距离的计算方式，这就是我们将要工作的

63
00:04:20,220 --> 00:04:20,340
用。

64
00:04:20,340 --> 00:04:26,220
但是同样，您可能会在算法中调用其他类型的距离，这实际上是

65
00:04:26,220 --> 00:04:33,720
取决于场景以及算法的确切结构，但在我们的示例中

66
00:04:33,720 --> 00:04:39,630
我们将使用欧几里得距离，因为它们是更自然的距离类型。

67
00:04:39,630 --> 00:04:44,970
现在让我们谈谈两个聚类之间的距离，假设您有两个聚类

68
00:04:44,970 --> 00:04:45,330
和蓝色。

69
00:04:45,330 --> 00:04:47,780
您如何测量它们之间的距离。

70
00:04:47,840 --> 00:04:50,970
两个簇之间的距离的定义是什么。

71
00:04:50,970 --> 00:04:56,500
它不像开始时听起来那么明显，因为例如可以有几个选择

72
00:04:56,520 --> 00:04:57,050
您可以。

73
00:04:57,180 --> 00:05:02,250
方法一是取两个最接近的点，然后测量该点并称其为

74
00:05:02,250 --> 00:05:03,480
两个集群。

75
00:05:03,480 --> 00:05:08,310
选项2实际上是采用另外两个点，并称两个类之间的距离也是

76
00:05:08,310 --> 00:05:09,390
有效的方法。

77
00:05:09,390 --> 00:05:14,280
选项3取平均值取平均值，取所有点之间所有距离的平均值

78
00:05:14,320 --> 00:05:18,740
对不同点的所有组合进行分类，并取该距离的平均值。

79
00:05:18,750 --> 00:05:23,520
选项4是取质心之间的距离，然后找到“中心”，然后找到

80
00:05:23,910 --> 00:05:27,330
质心和称两个簇之间的距离。

81
00:05:27,330 --> 00:05:34,170
因此，这是她称为聚类的一个非常重要的部分，您将其定义为两个类之间的距离

82
00:05:34,170 --> 00:05:39,060
因为这会严重影响算法的输出。

83
00:05:39,060 --> 00:05:42,070
现在，我们将不再对此进行深入研究。

84
00:05:42,090 --> 00:05:44,420
这只是要记住的事情。

85
00:05:44,550 --> 00:05:51,870
根据您遇到业务问题或其他类型的数据科学的特定情况

86
00:05:51,870 --> 00:05:56,970
根据您认为到底是什么最好的方法来定义问题

87
00:05:57,030 --> 00:06:02,790
在算法中，因此请记住，对于层次聚类算法，距离

88
00:06:02,790 --> 00:06:08,880
群集之间的连接是至关重要的元素，您需要记住您所设置的确切位置是

89
00:06:08,880 --> 00:06:12,630
将其设置为您在方法中如何定义它。

90
00:06:12,630 --> 00:06:13,080
好吧。

91
00:06:13,110 --> 00:06:14,580
因此，我们谈到了这一点。

92
00:06:14,640 --> 00:06:21,060
现在让我们回到示例中，我们已经逐步研究了规则，我非常

93
00:06:21,060 --> 00:06:23,210
循序渐进的粉丝。

94
00:06:23,340 --> 00:06:28,530
现在我们有了这种逐步的方法，它似乎一如既往地不堪重负，因为

95
00:06:28,530 --> 00:06:29,340
我们没有一个例子。

96
00:06:29,340 --> 00:06:34,300
但现在我们将有一个示例，我们将研究如何构建这些层次结构

97
00:06:34,380 --> 00:06:34,960
类。

98
00:06:35,100 --> 00:06:40,740
因此，第一步使每个数据点成为一个单一的点簇，从而形成六个簇。

99
00:06:40,740 --> 00:06:41,060
在那个。

100
00:06:41,070 --> 00:06:44,070
您可以看到每个点都是一个单独的群集。

101
00:06:44,070 --> 00:06:51,270
接下来，获取两个Close的数据点，并使其成为一个簇，在那里我们可以看到这两个点

102
00:06:51,300 --> 00:06:52,190
最接近。

103
00:06:52,200 --> 00:06:56,370
因此，我们将它们放在一堂课中，现在我们有五堂课一二三四，这些

104
00:06:56,370 --> 00:06:57,770
二是一类。

105
00:06:57,780 --> 00:06:58,190
好吧。

106
00:06:58,200 --> 00:07:03,820
第三步将两个最接近的集群作为我们拥有的集群，然后将它们变成一个集群。

107
00:07:03,870 --> 00:07:07,690
因此，在课堂之外，因为记住每一点都是这四个。

108
00:07:07,710 --> 00:07:11,160
所以，如果我们回到这里，这里的每一点都是一堂单独的课，这是一间教室。

109
00:07:11,190 --> 00:07:15,750
因此，现在只需测量聚类之间的距离即可，在我们的示例中，我们将要讨论

110
00:07:15,750 --> 00:07:20,820
关于群集之间的距离作为最小距离，因此将是这些群集之间的距离

111
00:07:20,820 --> 00:07:25,500
两个班级将是距离，这将是桑吉测量所有差异

112
00:07:25,500 --> 00:07:30,120
在群集之间，您发现它们实际上是最接近的群集，并将它们组合在一起

113
00:07:30,120 --> 00:07:31,410
一堂课

114
00:07:31,490 --> 00:07:36,810
接下来，您重复第3步，因此，接下来的这些中，有一，二，三，四集群中的课程

115
00:07:36,810 --> 00:07:37,950
可以看到我们有五个。

116
00:07:37,950 --> 00:07:42,310
现在，每次我们将簇数减少这四个类之一时，

117
00:07:42,310 --> 00:07:43,020
类。

118
00:07:43,020 --> 00:07:46,700
这似乎是最接近的群集，因此我们将对其进行合并。

119
00:07:46,890 --> 00:07:50,190
现在，在这三个集群中，当然看起来像这两个集群。

120
00:07:50,400 --> 00:07:54,990
所以合并它们，现在我们只剩下两个类，所以最后一步是将它们合并

121
00:07:54,990 --> 00:07:58,460
因为默认情况下它们是最接近的。

122
00:07:58,510 --> 00:07:59,040
所以我们走了。

123
00:07:59,040 --> 00:08:01,620
这就是我们算法的结尾。

124
00:08:01,620 --> 00:08:02,970
这就是它的收敛方式。

125
00:08:03,060 --> 00:08:11,470
您已经完成了CRE的过程，从各个角度来看待各个点

126
00:08:11,470 --> 00:08:16,920
因此，每个点都是一个单独的群集，现在我们只有一个群集，它将所有点组合在一起

127
00:08:16,940 --> 00:08:17,220
。

128
00:08:17,400 --> 00:08:19,020
那么所有这些的目的是什么。

129
00:08:19,020 --> 00:08:20,550
此练习的目的是什么。

130
00:08:20,670 --> 00:08:27,810
分层聚类算法的工作方式是保持对我们经历的方式的记忆

131
00:08:27,810 --> 00:08:32,910
这个过程和记忆存储在一个危险克中，这正是我们要谈论的

132
00:08:32,910 --> 00:08:38,820
在下一个教程中，一旦我们涵盖了Dendron克，就完全可以理解为什么层次结构

133
00:08:38,820 --> 00:08:42,540
聚类算法执行其功能以及其工作方式。

134
00:08:42,540 --> 00:08:45,430
因此，我们今天喜欢部门迫不及待地想看到下一个。

135
00:08:45,450 --> 00:08:47,860
在此之前，请享受机器学习。

