1
00:00:00,360 --> 00:00:06,480
您好，欢迎回到深度学习课程，这是关于

2
00:00:06,480 --> 00:00:08,670
软和交叉熵函数。

3
00:00:08,670 --> 00:00:15,320
不需要100％地使您遍历我们经过的所有部分

4
00:00:15,330 --> 00:00:21,510
在本节的主要部分中，我们讨论卷积神经网络，但是在

5
00:00:21,510 --> 00:00:26,580
同时我认为这对您的知识和技能组合将是一个很好的补充。

6
00:00:26,580 --> 00:00:30,840
因此，让我们继续深入研究这些功能。

7
00:00:30,840 --> 00:00:37,530
因此，首先我们要建立的是神经网络的结论

8
00:00:37,530 --> 00:00:44,210
该部分的一部分，然后在最后它弹出一些概率为零分九十五

9
00:00:44,220 --> 00:00:48,000
狗为猫的0.05％或5％。

10
00:00:48,060 --> 00:00:53,250
考虑到左边的照片作为输入这是在火车开行之后，实际上是

11
00:00:53,260 --> 00:00:57,210
它正在运行，并且正在对某个图像进行分类。

12
00:00:57,360 --> 00:01:00,850
因此，这里的问题是这两个值如何加起来为一。

13
00:01:00,900 --> 00:01:06,750
因为据我所了解的关于人工神经网络的所有知识

14
00:01:06,750 --> 00:01:11,600
可以说这两个最终神经元之间相互连接。

15
00:01:11,730 --> 00:01:16,590
那么他们怎么知道持有的价值，他们每个人都知道另一个的价值

16
00:01:16,590 --> 00:01:17,310
一个是。

17
00:01:17,400 --> 00:01:20,140
他们怎么知道将自己的价值加到一。

18
00:01:20,340 --> 00:01:22,060
答案是他们不会。

19
00:01:22,260 --> 00:01:28,500
在我们的人工神经网络的经典版本中，这样做的唯一方法是因为我们

20
00:01:28,710 --> 00:01:33,960
引入一个称为soft max函数的特殊函数，以帮助我们摆脱困境。

21
00:01:33,960 --> 00:01:40,890
因此通常情况下，狗和猫的神经元会发生任何实际值

22
00:01:41,490 --> 00:01:44,940
他们不必是，他们不必加一。

23
00:01:45,180 --> 00:01:51,900
但是然后我们将应用soft max函数，该函数写在顶部的那边，

24
00:01:51,900 --> 00:01:58,430
将这些值设置为0到1之间，这会使它们的PPTA总计为1到3。

25
00:01:59,250 --> 00:02:04,320
soft max函数或归一化指数函数是logistic函数的一般化

26
00:02:04,350 --> 00:02:11,640
引用非引号的壁球的k维矢量具有任意实数值到k维矢量

27
00:02:11,640 --> 00:02:15,320
在0到1范围内的实数值加起来等于1。

28
00:02:15,330 --> 00:02:17,620
因此，基本上它可以满足我们的要求。

29
00:02:17,670 --> 00:02:22,700
它使这些值介于0和1之间，并确保它们加起来为1。

30
00:02:22,960 --> 00:02:27,780
它的工作方式是可能的方法是，因为在底部，我们在这里

31
00:02:27,780 --> 00:02:29,970
您可以看到有一个总和。

32
00:02:29,970 --> 00:02:38,100
因此，它采用了指数并将其置于Zed的力量中，并将其相加，从而使所有人之间的比例为一

33
00:02:38,100 --> 00:02:38,830
您的课程。

34
00:02:38,850 --> 00:02:39,990
所有这些值。

35
00:02:39,990 --> 00:02:44,400
这样一来，您的规范化就在那里发生。

36
00:02:44,400 --> 00:02:51,300
因此，这就是Saucebox函数的工作方式，将soft next函数引入其中很有意义

37
00:02:51,600 --> 00:02:59,490
卷积神经网络，因为如果您有可能的狗类，它将是多么奇怪

38
00:02:59,490 --> 00:03:05,140
而猫和狗类则有80％的可能性。

39
00:03:05,160 --> 00:03:08,660
对于猫爪，您有45％的权利。

40
00:03:08,670 --> 00:03:14,430
只是没有意义，因此当您介绍下一个软件时会更好

41
00:03:14,430 --> 00:03:19,760
函数，这就是您在卷积神经网络中大部分时间都会发现的情况。

42
00:03:19,770 --> 00:03:26,010
现在，另一件事是soft max函数与称为Cross的东西并驾齐驱

43
00:03:26,100 --> 00:03:29,040
熵函数，这对我们来说非常方便。

44
00:03:29,050 --> 00:03:30,610
因此，让我们首先看一下公式。

45
00:03:30,660 --> 00:03:33,090
这就是交叉输入功能的样子。

46
00:03:33,090 --> 00:03:38,910
实际上，我们将使用另一种计算方法，将使用

47
00:03:39,060 --> 00:03:40,670
世纪，但结果基本相同。

48
00:03:40,670 --> 00:03:42,300
这只是更容易计算。

49
00:03:42,570 --> 00:03:49,220
我所知道的这听起来可能与屏幕上的任何公式都无关，但是

50
00:03:49,850 --> 00:03:54,300
在本节末尾将有一些其他的推荐读物，所以如果您正在

51
00:03:54,600 --> 00:03:56,380
不懂数学。

52
00:03:56,380 --> 00:03:58,350
就像即使我们现在还没有解释数学一样。

53
00:03:58,350 --> 00:04:03,630
但是，这里的要点是跨熵函数的跨熵很好。

54
00:04:03,630 --> 00:04:11,870
记住我们以前在人工神经网络中曾经有一个称为均方根箭头的函数

55
00:04:11,880 --> 00:04:17,760
我们用作评估自然绩效的成本函数的函数。

56
00:04:17,760 --> 00:04:23,750
我们的目标是最小化MSE，以优化我们的网络性能。

57
00:04:23,940 --> 00:04:31,830
那就是我们的成本函数，然后在卷积神经网络中我们仍然可以使用MSE

58
00:04:31,830 --> 00:04:38,070
但是在应用soft max函数之后，在卷积神经网络中有一个更好的选择

59
00:04:38,070 --> 00:04:39,840
是交叉熵函数。

60
00:04:39,840 --> 00:04:46,080
在卷积神经网络中，当您应用交叉输入函数而不是成本时，称为成本

61
00:04:46,080 --> 00:04:49,450
函数不再被称为最后一个函数，它们非常相似。

62
00:04:49,470 --> 00:04:55,520
它们只是术语上的一些差异，并且在含义上也有所不同。

63
00:04:55,530 --> 00:04:58,430
但是出于所有目的，它几乎是同一回事。

64
00:04:58,450 --> 00:05:07,530
然后发生的是最后一个函数再次是我们要最小化以便最大化的东西

65
00:05:07,530 --> 00:05:09,670
网络的性能。

66
00:05:09,690 --> 00:05:15,260
因此，让我们看一下如何应用此功能的快速示例。

67
00:05:15,260 --> 00:05:19,260
因此，可以说我们将狗的图像放入了我们的网络。

68
00:05:19,650 --> 00:05:26,160
狗的预测值为0.9，这是在进行训练，因此我们知道我们知道标签

69
00:05:26,160 --> 00:05:27,330
那是一条狗。

70
00:05:27,330 --> 00:05:34,140
因此，cat的预测值0.9的预测值为0.1，那么这里有标签，所以我们知道

71
00:05:34,140 --> 00:05:37,810
它的狗，因为这是为狗或猫训练0 1。

72
00:05:37,980 --> 00:05:47,600
因此，在这种情况下，您需要使用这些数字，并将它们插入交叉熵的公式中。

73
00:05:47,810 --> 00:05:53,340
因此，如何操作是左侧提示口的值。

74
00:05:53,420 --> 00:05:58,940
右边对数下的那个和右边的对数值

75
00:05:58,940 --> 00:06:04,340
进入P，因此重要的是要记住哪一个去了，因为如果您弄错了它们，您就不会

76
00:06:04,340 --> 00:06:09,620
想要从零开始或从1开始为我全取对数。

77
00:06:09,620 --> 00:06:11,660
所以您只想插入它们。

78
00:06:11,720 --> 00:06:14,520
确保将它们插入正确的位置。

79
00:06:14,840 --> 00:06:17,030
然后您基本上将其加起来。

80
00:06:17,030 --> 00:06:22,370
这就是交叉输入的工作方式，我们现在来看一个实际的

81
00:06:22,370 --> 00:06:28,130
在现实生活中应用此功能的具体分步示例以及不适的产品

82
00:06:28,130 --> 00:06:32,360
感觉到交叉熵是什么，它会变得更少。

83
00:06:32,360 --> 00:06:39,290
我在此工作中的目标是使您在跨世纪时更加舒适，因为它听起来很复杂

84
00:06:39,320 --> 00:06:43,840
而且没有双关语就可以做到。

85
00:06:43,850 --> 00:06:50,870
就像卷积神经网络一样，它听起来可能非常复杂和令人恐惧，但事实并非如此。

86
00:06:50,870 --> 00:06:51,650
这就是重点。

87
00:06:51,650 --> 00:06:54,090
因此，让我们继续应用它，以便我们知道它并不可怕。

88
00:06:54,080 --> 00:06:56,350
这就是您的全部。

89
00:06:56,360 --> 00:07:01,790
这也将解释为什么我们要这样做，为什么要研究不同的原因函数。

90
00:07:01,790 --> 00:07:06,650
因此，神经网络一个神经网络，假设我们有两个神经网络，然后传递图像

91
00:07:06,650 --> 00:07:11,960
的狗，我们知道这是狗而不是猫。

92
00:07:12,200 --> 00:07:18,620
然后我们有另一幅图像，这次我们的猫是动物，它是猫而不是狗，这里有

93
00:07:19,040 --> 00:07:22,490
我们正在看一个实际上是狗而不是猫的洞。

94
00:07:22,490 --> 00:07:24,280
如果您仔细观察。

95
00:07:24,320 --> 00:07:28,440
因此，我们想看看我们的神经网络在第一种情况下将如何预测。

96
00:07:28,460 --> 00:07:36,110
神经网络1 90％的狗10％的猫正确的无网络号到60％的狗40％

97
00:07:36,110 --> 00:07:38,230
猫仍然纠正更糟。

98
00:07:38,270 --> 00:07:40,030
但是正确。

99
00:07:40,280 --> 00:07:46,040
第二种选择首先是神经网络10％猫狗90％猫。

100
00:07:46,040 --> 00:07:47,300
正确。

101
00:07:47,300 --> 00:07:53,560
您知道该数字比30％的狗，70％的猫更糟，但仍然正确。

102
00:07:53,570 --> 00:08:01,460
然后最后在图像岁的网络中的神经网络赢得了40％的狗60％的猫不正确

103
00:08:01,870 --> 00:08:08,270
神经网络的数量不正确，甚至更糟的是10％的狗和90％的猫。

104
00:08:08,270 --> 00:08:15,380
因此，这里的关键是，即使网民在所有三个图像中的最后一个图像中都弄错了

105
00:08:15,620 --> 00:08:18,870
神经网络之一是优于神经网络。

106
00:08:18,890 --> 00:08:27,010
因此，即使在最后一种情况下，它也给狗带来了40％的机会，而不是神经

107
00:08:27,030 --> 00:08:32,330
网络只能给狗带来10％的机会，或者神经网络的整体表现要好

108
00:08:33,200 --> 00:08:35,310
与神经网络相比2。

109
00:08:35,520 --> 00:08:41,780
因此，现在我们将研究可以衡量性能的功能，

110
00:08:41,780 --> 00:08:42,800
谈到了评级。

111
00:08:43,040 --> 00:08:48,090
因此，让我们将它们放入表格中，以便在神经网络1中输入错误的数字。

112
00:08:48,350 --> 00:08:49,430
这就是图像编号。

113
00:08:49,550 --> 00:08:51,140
然后对于图像一

114
00:08:51,140 --> 00:08:54,010
据预测，百分之九十的狗黑猩猩和猫。

115
00:08:54,110 --> 00:09:00,550
因此，这里有Marable的帽子，那么您具有实际值，因此狗正确的猫不正确。

116
00:09:00,560 --> 00:09:07,460
对于第二个图像，相同的事物；对于最小的三个图像，相同的事物；对于神经网络数量，相同的事物

117
00:09:07,460 --> 00:09:07,720
二。

118
00:09:07,750 --> 00:09:11,060
因此，Dog 60％的人保留了第一张照片中的40％。

119
00:09:11,060 --> 00:09:13,800
那就是它所预测的巴豆是狗而不是猫。

120
00:09:13,820 --> 00:09:14,820
等等。

121
00:09:15,200 --> 00:09:18,050
现在，让我们看看我们实际上会遇到什么错误。

122
00:09:18,050 --> 00:09:24,940
因此，我们可以计算出哪些错误来估计性能并监视网络的性能。

123
00:09:24,950 --> 00:09:28,480
因此，一种类型的错误称为分类错误。

124
00:09:28,640 --> 00:09:33,990
基本上，这只是问您是否正确。

125
00:09:34,010 --> 00:09:36,940
不论概率大小，您都正确无误。

126
00:09:36,950 --> 00:09:37,970
还是您做对了。

127
00:09:37,970 --> 00:09:44,790
因此，在两种情况下，对于两个神经网络，它们每个都有一个。

128
00:09:44,810 --> 00:09:46,330
所以这就是你他们出错的方式。

129
00:09:46,340 --> 00:09:48,460
因此他们错了三分之一。

130
00:09:48,470 --> 00:09:54,960
因此，您的网络1的错误率为33％，而神经网络的错误率为30％。

131
00:09:55,100 --> 00:09:59,750
从这个角度出发，两个神经网络的性能相同，但是我们知道

132
00:09:59,750 --> 00:10:00,250
不对。

133
00:10:00,260 --> 00:10:04,400
我们知道，神经网络Ikhwan的性能优于神经网络。

134
00:10:05,120 --> 00:10:10,850
这就是为什么分类误差不是很好的衡量标准的原因，特别是对于反向传播而言

135
00:10:11,810 --> 00:10:17,960
均方误差不同，顺便说一下，我不想在Excel中进行这些计算

136
00:10:17,960 --> 00:10:22,010
和他们在一起，但您可以Tony坐下来在纸上或Excel中进行处理。

137
00:10:22,010 --> 00:10:28,760
这些是非常简单的计算，基本上只求平方误差的总和，然后

138
00:10:28,760 --> 00:10:35,010
取您的观察结果的平均值，仅此而已。

139
00:10:35,060 --> 00:10:43,320
因此，对于神经网络1来说，神经网络2的错误率是25％，那么错误率是71％。

140
00:10:43,330 --> 00:10:45,930
因此您可以看到这一点更加准确。

141
00:10:45,940 --> 00:10:50,380
告诉我们，几乎有一个错误率比您自己的网络低得多。

142
00:10:51,150 --> 00:10:52,970
然后再次交叉熵。

143
00:10:52,990 --> 00:10:57,250
我们已经看到您也可以计算出的公式实际上比平均值更容易计算

144
00:10:57,250 --> 00:11:04,780
平方误差跨熵的横截面积为神经网络1提供了38％，为神经网络提供了1.0 6

145
00:11:04,780 --> 00:11:05,350
网络2。

146
00:11:05,500 --> 00:11:08,180
因此，您可以看到结果有些不同。

147
00:11:08,350 --> 00:11:16,510
当您看着它们时，您会知道超短裙区域和交叉熵以及

148
00:11:16,510 --> 00:11:26,350
为什么为什么要使用交叉熵而不是平方误差的问题

149
00:11:26,350 --> 00:11:32,030
他们说的数字，但所有这些计算只是向您表明，这就是全部

150
00:11:32,050 --> 00:11:34,680
可行，您可以只在纸上做，不是。

151
00:11:34,780 --> 00:11:37,890
它不是很激烈的数学。

152
00:11:37,890 --> 00:11:41,130
这些都是非常简单明了的事情。

153
00:11:41,200 --> 00:11:47,680
但是为什么您要使用均值的问题导致熵超过均值，这是一个非常非常好的问题

154
00:11:47,680 --> 00:11:48,250
问。

155
00:11:48,250 --> 00:11:58,530
我很高兴您问到答案，就像交叉熵比

156
00:11:58,540 --> 00:12:01,430
均方误差不明显。

157
00:12:01,450 --> 00:12:07,160
因此，我将提及一对夫妇，但除此之外，我将告诉您在哪里可以找到更多信息。

158
00:12:07,160 --> 00:12:18,550
因此，其中之一是，例如，如果您在反向传播的一开始就将输出

159
00:12:18,550 --> 00:12:22,260
价值非常非常非常非常非常微小。

160
00:12:22,360 --> 00:12:25,680
因此，它比所需的实际值小得多。

161
00:12:25,750 --> 00:12:32,920
然后，从一开始，您伟大而体面的世界中的梯度将非常低，您将不会

162
00:12:32,920 --> 00:12:33,840
足够。

163
00:12:33,850 --> 00:12:40,630
对于神经网络来说，实际上很难开始做某事并开始四处走动，

164
00:12:40,630 --> 00:12:45,010
开始调整这些权重，并开始Movistar朝正确的方向移动。

165
00:12:45,130 --> 00:12:50,920
而当您使用诸如交叉熵之类的东西时，因为它实际上具有对数

166
00:12:51,400 --> 00:12:57,310
帮助网络评估甚至一个很小的区域并对此进行处理。

167
00:12:57,310 --> 00:12:58,520
这是思考的方法。

168
00:12:58,520 --> 00:13:03,260
因此，让我们再说一次，这是一种非常直观的方法。

169
00:13:03,410 --> 00:13:08,830
这将与数学建立联系，您可以通过数学来推导这些东西

170
00:13:08,830 --> 00:13:11,260
更详细但又非常直观的方法。

171
00:13:11,260 --> 00:13:16,030
假设您喜欢自己想要的结果。

172
00:13:16,030 --> 00:13:22,810
现在是1，而现在您是1的百万分之一。

173
00:13:22,870 --> 00:13:23,140
对。

174
00:13:23,170 --> 00:13:30,790
$ 0.00或有一个，然后您改善，下一次您将结果提高一百万分之一

175
00:13:30,790 --> 00:13:32,680
到千分之一

176
00:13:32,860 --> 00:13:39,330
就计算平方误差而言，您只需从另一个中减去一个即可。

177
00:13:39,610 --> 00:13:44,980
或基本上在每种情况下，您都是用正方形的Kalka进行比较时，您会发现平方误差

178
00:13:44,980 --> 00:13:48,210
一个案例与另一个案例并没有太大变化。

179
00:13:48,220 --> 00:13:51,940
当您查看那里的均方时，您的网络并没有得到太大改善。

180
00:13:52,120 --> 00:13:58,750
但是，如果您正在查看交叉熵，因为您正在取对数，那么您正在比较

181
00:13:58,750 --> 00:14:01,090
就是将一个人分开。

182
00:14:01,390 --> 00:14:09,390
您会看到您实际上已经大大改善了您的网络，从而使这一数字从一百万

183
00:14:09,460 --> 00:14:12,810
均方误差项中的1000会非常低。

184
00:14:12,820 --> 00:14:15,710
这将是微不足道的，并且不会。

185
00:14:15,790 --> 00:14:22,270
它不会指导您的梯度增强过程或正确方向的反向传播。

186
00:14:22,340 --> 00:14:28,180
一切都会朝着正确的方向进行指导，但这就像一个非常缓慢的指导，它将没有足够的指导

187
00:14:28,540 --> 00:14:34,960
力量，但是如果您确实跨越熵，那么熵将理解，即使这些非常

188
00:14:34,960 --> 00:14:42,220
小调整，就是您知道，相对值绝对值的微小变化是

189
00:14:42,220 --> 00:14:43,770
巨大的进步。

190
00:14:43,870 --> 00:14:46,110
我们肯定会朝正确的方向前进。

191
00:14:46,110 --> 00:14:54,820
让我们继续前进，以便交叉熵将帮助您的神经网络正确到达

192
00:14:54,820 --> 00:15:01,090
最佳状态是神经网络使其达到最佳状态的更好方法。

193
00:15:01,090 --> 00:15:08,260
但请记住，这仅在跨越熵仅是分类的首选方法时才有效。

194
00:15:08,260 --> 00:15:14,200
因此，如果您谈论的是诸如人工神经网络中的回归之类的事情，

195
00:15:14,230 --> 00:15:20,770
您宁愿和我一起求平方误差，而交叉熵更适合分类和

196
00:15:20,770 --> 00:15:26,200
同样，这与我们正在使用下一个软功能有关，这是一种直观的解释

197
00:15:26,200 --> 00:15:31,690
如果您真的对此感兴趣，那么这是一个学习更多的好地方，知道为什么

198
00:15:31,690 --> 00:15:34,740
我们使用交叉与均方误差。

199
00:15:35,200 --> 00:15:43,160
Google播放了Geoffrey Hinton的一段视频，该视频称为soft max输出函数，他解释得很好，并且

200
00:15:43,160 --> 00:15:48,760
您知道作为深度学习的教父，无论如何它都能更好地解释它。

201
00:15:48,890 --> 00:15:51,680
顺便说一句，杰弗里·欣顿（Geoffrey Hinton）拍摄的任何视频都是金色的。

202
00:15:51,680 --> 00:15:55,590
无论如何，他只是具有解释事物的巨大才能。

203
00:15:55,610 --> 00:16:01,310
这就是柔和的对位，希望对您有种直觉的理解

204
00:16:01,310 --> 00:16:02,110
这里发生了什么。

205
00:16:02,120 --> 00:16:08,030
但更重要的是，您不要被交叉熵这个术语所困扰，因为标题会提到

206
00:16:08,030 --> 00:16:11,280
在实际故事中，我想确保您为此做好了准备。

207
00:16:11,280 --> 00:16:15,740
这只是计算上一个函数的另一种方法。

208
00:16:15,740 --> 00:16:21,830
优化网络的另一种方法专门针对分类问题

209
00:16:21,860 --> 00:16:28,180
因此，卷积神经网络与soft max函数紧密相关。

210
00:16:28,280 --> 00:16:35,480
因此，如果您希望对交叉熵有一个简要的介绍，可以阅读更多

211
00:16:35,480 --> 00:16:37,170
当然更集中一点。

212
00:16:37,250 --> 00:16:43,370
值得一读的好文章被Rob DePietro称为对交叉熵损失的友好介绍

213
00:16:44,180 --> 00:16:45,280
2016。

214
00:16:45,350 --> 00:16:46,860
这是下面的链接。

215
00:16:47,150 --> 00:16:54,350
非常非常好非常软，没有什么没有超级复杂的数学。

216
00:16:54,440 --> 00:16:59,660
很好的类比，使用汽车的类比的很好的例子，您看汽车并谈论信息

217
00:16:59,660 --> 00:17:04,910
以及位和限制，您会知道如何解码整个Unico，所以这是一个很好的选择

218
00:17:04,910 --> 00:17:10,730
文章，我们将为您提供交叉条目的良好概述，例如入门

219
00:17:10,820 --> 00:17:11,680
立场。

220
00:17:11,900 --> 00:17:18,590
如果您想像这里看到的那样深入研究繁琐的数学运算，请查看作者或博客

221
00:17:18,680 --> 00:17:25,180
通过如何实现神经网络，Intertermezzo也是如此，就像在使用中间一样

222
00:17:25,220 --> 00:17:27,410
像一个。

223
00:17:27,550 --> 00:17:28,910
间歇性

224
00:17:28,990 --> 00:17:35,690
你知道当你去剧院的时候，你就像在第一部分和第二部分之间休息一样

225
00:17:35,690 --> 00:17:36,290
部分。

226
00:17:36,350 --> 00:17:40,820
所以，因为他喜欢经历所有这些步骤，然后又喜欢，然后他说我必须解释

227
00:17:40,820 --> 00:17:42,210
首先。

228
00:17:42,470 --> 00:17:44,080
是的，所以这就是为什么它被称为intermezzo。

229
00:17:44,090 --> 00:17:51,620
据我了解彼得·罗兰兹（Peter Rolands）2016年的文章，没有其他原因，所以两者都是

230
00:17:51,620 --> 00:17:52,470
最近。

231
00:17:52,580 --> 00:18:00,150
如果您想深入研究Kross熵背后的数学原理，就可以查看一下

232
00:18:00,150 --> 00:18:02,600
实际上，本文中的soft Max和交叉熵。

233
00:18:02,930 --> 00:18:03,790
所以我们去了。

234
00:18:03,860 --> 00:18:07,360
这就是这两个的全部。

235
00:18:07,370 --> 00:18:12,780
希望我能够为此添加更多的清晰度和好运。

236
00:18:12,830 --> 00:18:16,970
这将很有趣，并喜欢实用的教程。

237
00:18:16,970 --> 00:18:18,070
下次见。

238
00:18:18,080 --> 00:18:19,700
在此之前，请尽情学习。

