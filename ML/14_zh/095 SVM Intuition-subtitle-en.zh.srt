1
00:00:00,810 --> 00:00:03,440
您好，欢迎回到机器学习课程。

2
00:00:03,540 --> 00:00:07,570
今天，我们谈论的是支持向量机或SVM的简称。

3
00:00:07,670 --> 00:00:16,800
因此SVM最初是在1960年代开发的，然后在1990年代再次进行了完善，直到现在

4
00:00:16,800 --> 00:00:22,170
它们在机器学习中变得非常流行，因为它们表明它们可以非常

5
00:00:22,170 --> 00:00:26,830
非常强大，因为它们与其他机器学习算法有些不同。

6
00:00:26,910 --> 00:00:28,860
我们将找出它们的特殊之处。

7
00:00:28,860 --> 00:00:29,970
故事快要结束了。

8
00:00:30,090 --> 00:00:34,560
现在，让我们了解一下支持向量机的实际工作原理。

9
00:00:34,560 --> 00:00:39,620
好的，为了简单起见，在这里我们在二维空间上具有平常的点。

10
00:00:39,660 --> 00:00:41,850
我们只有两列x1和x2。

11
00:00:42,060 --> 00:00:48,060
我们已经有了一些观察，有些已经同意，所以我们已经对它们进行了分类，但是现在如何

12
00:00:48,060 --> 00:00:51,530
我们是否得出将它们分开的线？

13
00:00:51,530 --> 00:00:53,820
那么，我们如何实际上将这些点分开。

14
00:00:53,820 --> 00:00:55,710
因为那是分离。

15
00:00:55,710 --> 00:01:00,960
换句话说，当我们前进时，决策边界对于我们前进非常重要

16
00:01:00,960 --> 00:01:04,410
开始添加新点，这就是我们分类的重点。

17
00:01:04,410 --> 00:01:08,880
这就是开放性的目的，因为我们希望在这两者之间创建一个边界，以便当我们

18
00:01:08,880 --> 00:01:13,740
将来添加我们要分类的尚未分类的新点，我们将知道

19
00:01:13,740 --> 00:01:17,240
它们将落在较大的绿色区域或红色区域中。

20
00:01:17,370 --> 00:01:20,390
那么我们如何分开这些要点呢？

21
00:01:20,610 --> 00:01:26,840
一种方法是在我们的二维空间中画一条这样的线，然后在右边说些什么

22
00:01:26,840 --> 00:01:32,640
将变成绿色，左边的任何东西将变成红色，如果新点落在该空间的某处，我们将

23
00:01:32,640 --> 00:01:35,550
立即知道它是红色还是绿色，因为我们会知道它在哪里。

24
00:01:35,550 --> 00:01:40,860
但是，还有另一种方法，我们可以像这样绘制一条水平线，或者我们可以绘制一条对角线，例如

25
00:01:40,860 --> 00:01:41,640
那。

26
00:01:41,640 --> 00:01:44,960
我们实际上可以绘制另一条对角线，也可以绘制另一条对角线。

27
00:01:44,970 --> 00:01:49,920
因此，我们可以创建许多不同的行，它们将实现相同的结果，它们将分开

28
00:01:49,950 --> 00:01:51,790
我们的观点分为两类。

29
00:01:51,810 --> 00:01:58,560
但是与此同时，它们将来都将产生不同的后果，因此当我们添加新点时

30
00:01:58,830 --> 00:02:03,090
根据下降的位置，它将被归类为绿色区域的一部分，

31
00:02:03,090 --> 00:02:07,290
红色还是我们想找到最佳的线，那就是所有字段的含义。

32
00:02:07,320 --> 00:02:14,490
他们的目的是找到最佳路线或最佳决策边界，这将帮助我们分隔空间

33
00:02:14,490 --> 00:02:15,420
上课。

34
00:02:15,720 --> 00:02:20,920
因此，让我们找出SVM如何实际搜索此指示灯。

35
00:02:21,090 --> 00:02:28,620
好吧，这条线是通过最大边距搜索的，因此在这里您可以看到一条线，这就是该线

36
00:02:28,620 --> 00:02:28,640
。

37
00:02:28,650 --> 00:02:30,200
然后SVM会吸引。

38
00:02:30,330 --> 00:02:35,630
因此，基本上就是将这两个克拉斯点分开的线。

39
00:02:35,730 --> 00:02:42,570
同时，它具有最大的余量，这意味着该距离，所以等距绘制该线

40
00:02:42,570 --> 00:02:47,430
从这一点到这一点，我们将在一秒钟之内确切地找出为什么要指出这些点。

41
00:02:47,430 --> 00:02:52,620
然后，线与这些点中的每一个点之间的距离是等距的。

42
00:02:52,860 --> 00:02:59,940
这就是余量，因此必须使这两个距离的总和最大才能使这条线

43
00:02:59,940 --> 00:03:01,510
是SVM的结果。

44
00:03:01,740 --> 00:03:06,850
这两个点实际上称为支持向量，为什么将它们称为向量

45
00:03:06,860 --> 00:03:07,540
一个小时又一秒钟。

46
00:03:07,540 --> 00:03:13,140
但是基本上，这两点都支持整个算法。

47
00:03:13,140 --> 00:03:18,480
因此，即使您摆脱了所有其他问题，事情也会改变，算法也将完全正确

48
00:03:18,480 --> 00:03:18,810
相同。

49
00:03:18,810 --> 00:03:25,620
因此，这些其他点对算法结果没有贡献，只有这两个点是

50
00:03:25,890 --> 00:03:31,200
做出贡献，因此它们称为支持向量，您可以称其为支持点，但是

51
00:03:31,200 --> 00:03:32,490
实际上，它们是向量。

52
00:03:32,490 --> 00:03:38,590
这就是为什么，因为在多维空间中，当您拥有两个以上变量时，

53
00:03:38,590 --> 00:03:41,400
可以有三个五个10或100个变量。

54
00:03:41,430 --> 00:03:46,620
实际上，每个点不再是一个点，因为您无法在二维平面或

55
00:03:46,620 --> 00:03:53,400
即使是三维空间，因此我们在此处看到的每个点都被认为是

56
00:03:53,400 --> 00:03:59,730
实际上是多维空间中的向量，所以我们在这里看到的点的更笼统的术语

57
00:03:59,820 --> 00:04:06,090
是向量，这是在大学或高中数学中进行的数学研究

58
00:04:06,120 --> 00:04:07,020
基本上。

59
00:04:07,020 --> 00:04:12,810
因此，一般来说，在这个特定示例中，它们都是向量，我们有两个维度

60
00:04:12,820 --> 00:04:16,230
然后我们可以称它们为点，但实际上有图片，这就是为什么它们被称为支持

61
00:04:16,230 --> 00:04:16,950
向量。

62
00:04:17,040 --> 00:04:23,010
因此，这两个特定向量是支持这种决策边界的一种支持向量

63
00:04:23,010 --> 00:04:26,940
或通过这种方式，我们正在构建此算法，这就是为什么它们很重要，这就是整个算法的原因

64
00:04:27,180 --> 00:04:29,350
被称为支持向量机。

65
00:04:29,550 --> 00:04:31,080
所以现在我们还有什么呢？

66
00:04:31,080 --> 00:04:37,140
好吧，我们在中间的那条线称为最大余量超平面或最大余量

67
00:04:37,140 --> 00:04:37,910
分类器。

68
00:04:37,920 --> 00:04:43,190
因此，在二维空间中，就像分类器就是直线一样。

69
00:04:43,290 --> 00:04:47,000
但实际上在多维空间中，它是一个超平面。

70
00:04:47,250 --> 00:04:52,800
而且我知道这是一个非常令人困惑的术语，但这就是所谓的最大余量双曲线。

71
00:04:52,800 --> 00:04:57,440
所以我们看到的所有这些都是超平面的，但是没有最大余量混合动力

72
00:04:57,450 --> 00:05:02,400
叶片，您可以检查自己，以便在此处绘制其他超平面，然后签出

73
00:05:02,400 --> 00:05:03,060
边缘的。

74
00:05:03,060 --> 00:05:06,530
它总是会更少，因为这是具有最大边距的那个。

75
00:05:06,660 --> 00:05:09,320
然后，您将获得绿色和红色虚线。

76
00:05:09,330 --> 00:05:14,130
因此，绿色的一个称为正超平面，红色的称为负超平面。

77
00:05:14,130 --> 00:05:18,660
命名的顺序并不重要，只不过其中之一是肯定的

78
00:05:18,900 --> 00:05:26,160
负数或正数右边的任何东西都归为绿色类别

79
00:05:26,160 --> 00:05:31,940
或肯定类别，将左侧的任何内容分类为否定类别或红色类别

80
00:05:31,950 --> 00:05:32,570
就我们而言。

81
00:05:32,670 --> 00:05:39,540
因此，监督机算法的工作方式当然是有些复杂的数学

82
00:05:39,540 --> 00:05:46,240
它的背后，但其直观部分的本质恰恰是我们正在线性处理

83
00:05:46,290 --> 00:05:52,410
默认情况下，我们实际上可以在其中存在的可分离数据集可以通过

84
00:05:52,410 --> 00:05:58,470
一张图表，它将两个类别分开，然后我们只在搜索最大类别的图表

85
00:05:58,680 --> 00:05:59,820
余量。

86
00:05:59,820 --> 00:06:05,010
因此，从概念上讲，当您考虑它时，它实际上是一个非常简单的算法

87
00:06:05,010 --> 00:06:05,770
这样。

88
00:06:05,840 --> 00:06:11,460
如果我要学习数学，问题是SVM的特别之处是为什么它们如此

89
00:06:11,460 --> 00:06:17,520
如此受欢迎，为什么它们不同于其他机器学习算法，这正是我们

90
00:06:17,520 --> 00:06:18,740
现在要谈论。

91
00:06:18,930 --> 00:06:26,820
因此，想象一下您正在尝试教一台机器如何区分苹果和橙子如何分类

92
00:06:27,500 --> 00:06:29,390
水果变成苹果或橘子。

93
00:06:29,390 --> 00:06:31,400
所以您是在告诉机器。

94
00:06:31,440 --> 00:06:36,620
好吧，我将为您提供一些测试数据，因此请看一下所有这些苹果。

95
00:06:36,630 --> 00:06:38,220
这些是苹果桔子。

96
00:06:38,510 --> 00:06:39,180
分析它们。

97
00:06:39,180 --> 00:06:43,830
看看他们，看看他们有什么参数，然后下次他们会给你。

98
00:06:43,830 --> 00:06:48,270
我要给你一个水果，要么是苹果，要么是橙子，你需要分类

99
00:06:48,270 --> 00:06:50,520
告诉我这是苹果还是橙子。

100
00:06:50,520 --> 00:06:50,820
对。

101
00:06:50,820 --> 00:06:55,370
因此，这是一种标准的机器学习问题。

102
00:06:55,470 --> 00:07:01,590
现在在我们这里的例子中，您可以看到在右侧说我们有橙子在左边有苹果。

103
00:07:01,620 --> 00:07:08,130
因此，主要由机器Algren来做的是，他们会看着最多的苹果，苹果和

104
00:07:08,130 --> 00:07:13,320
最橙色的橙色，以便他们查看最库存标准的普通类型的苹果和

105
00:07:13,320 --> 00:07:19,410
多数标准的普通橙子类型，现在的情况是苹果在其中

106
00:07:19,440 --> 00:07:24,700
苹果克劳斯的心很远离橘子。

107
00:07:24,810 --> 00:07:26,670
因为橘子就在那边。

108
00:07:26,670 --> 00:07:31,070
同样，在远离苹果的橙色克劳斯的心脏地带，他们也受到了审判。

109
00:07:31,110 --> 00:07:37,050
机器会尝试从非常像苹果的苹果中学习，因此它将知道什么是苹果

110
00:07:37,050 --> 00:07:37,480
是。

111
00:07:37,530 --> 00:07:43,110
而且它还尝试向橙子学习，这样就可以知道橙子到底是什么，而这就是大多数

112
00:07:43,110 --> 00:07:47,910
机器学习算法的工作原理，然后基于它可以提出一些建议

113
00:07:47,910 --> 00:07:55,050
预测并分类四个新的数据元素和变量，以防万一

114
00:07:55,050 --> 00:07:56,730
支持向量机。

115
00:07:56,730 --> 00:07:57,850
有点不同。

116
00:07:57,870 --> 00:08:03,910
与其关注大多数股票标准的苹果，股票和橙子，不如说这支持增长

117
00:08:03,910 --> 00:08:10,050
机器所做的是实际上他们看着非常像橙色的苹果，所以在这里您可以看到

118
00:08:10,050 --> 00:08:13,590
一个不是您的标准苹果的苹果是橙色和正确的颜色。

119
00:08:13,590 --> 00:08:19,110
因此，很容易为这个苹果注入一个橙子，他们会看那些没有存货的橙子

120
00:08:19,110 --> 00:08:23,400
标准橙子比其他任何东西都更像苹果，因此您可以在这里订购柠檬。

121
00:08:23,400 --> 00:08:29,670
因此，我们中的那些人只是在SVM中从橙色中脱颖而出，

122
00:08:29,670 --> 00:08:31,200
在这种情况下最像苹果。

123
00:08:31,210 --> 00:08:32,550
我们有一个绿色的橙色。

124
00:08:32,610 --> 00:08:37,490
当您想到橙色或橙色时，拥有绿色橙色是不正常的。

125
00:08:37,560 --> 00:08:39,420
就是这样。

126
00:08:39,600 --> 00:08:44,070
这些是支持向量的支持，因此您可以看到这些支持向量实际上非常

127
00:08:44,070 --> 00:08:49,260
靠近边界，因此它们离苹果非常近，而红色离苹果非常近。

128
00:08:49,260 --> 00:08:54,300
和橙色或绿色标记将非常接近红色，因此支持

129
00:08:54,300 --> 00:09:00,240
从这个意义上讲，向量机可以认为它就像是一种更极端的算法，非常反叛

130
00:09:00,240 --> 00:09:06,390
算法的类型非常危险，因为它看起来非常极端，

131
00:09:06,390 --> 00:09:13,030
靠近边界，并以此来构建分析。

132
00:09:13,140 --> 00:09:21,090
这本身使得支持向量机算法与大多数其他算法非常不同

133
00:09:21,090 --> 00:09:22,860
其他机器学习算法。

134
00:09:22,860 --> 00:09:29,880
这就是为什么有时它们的性能要比不受支持的矢量机算法好得多的原因。

135
00:09:30,090 --> 00:09:35,760
所以，我希望您能对支持向量机的这种解释和直觉有用，并且现在

136
00:09:35,790 --> 00:09:41,700
您不仅知道它们是如何工作的，而且还知道它们为何与其他现有算法不同

137
00:09:41,700 --> 00:09:43,050
用于机器学习。

138
00:09:43,050 --> 00:09:45,180
关于这一点，我们将输入此材料。

139
00:09:45,260 --> 00:09:46,940
期待下次见到您。

140
00:09:46,940 --> 00:09:48,630
在那之前在德国学习

