1
00:00:00,210 --> 00:00:06,120
您好，欢迎今天访问此Python教程，我们将实现支持向量机或大多数

2
00:00:06,120 --> 00:00:08,450
通常在Python上称为SVM。

3
00:00:08,520 --> 00:00:10,230
现在开始吧。

4
00:00:10,230 --> 00:00:16,440
我们将转到文件资源管理器以设置文件夹的工作目录，以便进行分类

5
00:00:16,950 --> 00:00:18,950
然后支持向量机。

6
00:00:18,960 --> 00:00:24,630
另外，请确保在我们提交文件后尽快拥有社交网络，然后单击

7
00:00:24,630 --> 00:00:27,270
此按钮用于设置完整的工作目录。

8
00:00:27,270 --> 00:00:28,380
现在开始吧。

9
00:00:28,410 --> 00:00:36,300
我们将使用分类模板来提高效率，并准备好所有代码，因为

10
00:00:36,300 --> 00:00:39,220
我们只有几件事需要更改。

11
00:00:39,240 --> 00:00:42,120
所以在这里我将所有模板作为基础。

12
00:00:42,120 --> 00:00:43,860
现在，我们需要更改一些内容。

13
00:00:43,860 --> 00:00:49,040
我们需要创建的用于分类的很少的东西就是VM分类器。

14
00:00:49,350 --> 00:00:54,170
然后在下面，我们将只需要更改图的标题。

15
00:00:54,180 --> 00:01:02,430
因此，这里我们将替换为V M，这里也将其分类为M。

16
00:01:02,640 --> 00:01:06,850
现在我们只需要做的就是创建我们的分类器。

17
00:01:07,320 --> 00:01:09,960
因此，在这里我也将替换SBM。

18
00:01:09,960 --> 00:01:10,610
对。

19
00:01:11,040 --> 00:01:17,760
好的，和往常一样，我们将从库中导入一个类，然后创建一个

20
00:01:17,760 --> 00:01:24,840
该类的对象，然后我们将使用该类的fit方法将分类器拟合到我们的火车上

21
00:01:24,850 --> 00:01:25,110
。

22
00:01:25,350 --> 00:01:26,580
因此，让我们这样做。

23
00:01:26,580 --> 00:01:33,840
如我们所见，我们必须导入该类，我们将从sikat导入该类，以将其作为VM学习。

24
00:01:33,930 --> 00:01:37,150
所以在这里我们键入OK。

25
00:01:37,710 --> 00:01:40,390
学点为。

26
00:01:40,450 --> 00:01:45,710
在这里，它是可见的。

27
00:01:45,720 --> 00:01:46,830
好吧。

28
00:01:46,830 --> 00:01:50,060
这就是该类，现在让我们创建该类的对象。

29
00:01:50,430 --> 00:01:53,840
因此，我们将通常将此对象分类器称为“对象分类器”。

30
00:01:53,940 --> 00:01:59,920
因此，让我们做一个等于的分类器，然后我们将调用该类。

31
00:02:00,480 --> 00:02:03,460
现在我们要输入参数。

32
00:02:03,630 --> 00:02:08,570
因此，让我们检查VC类，看看参数是什么。

33
00:02:08,640 --> 00:02:12,340
因此命令或控制加II进行检查。

34
00:02:12,540 --> 00:02:16,020
这是我们所看到的about和参数。

35
00:02:16,290 --> 00:02:18,290
确定，所以第一个参数是。

36
00:02:18,300 --> 00:02:24,390
这就是我们现在需要的Benanti参数，因为您知道这将改善模型

37
00:02:24,390 --> 00:02:28,120
如果我们需要它，那将在本部分的结尾处看到。

38
00:02:28,260 --> 00:02:32,360
但是，这里有一个重要的参数是内核。

39
00:02:32,580 --> 00:02:38,730
由于我们是从基本的VM开始的，因此您将了解线性r内核，因此我们将指定

40
00:02:38,730 --> 00:02:44,940
我们需要线性核，因为您可以看到核的默认值是B-F

41
00:02:44,940 --> 00:02:46,760
是高斯核。

42
00:02:46,980 --> 00:02:53,610
您会看到有很多内核，例如线性内核，Poley Gnomeo内核，RBA内核

43
00:02:53,610 --> 00:02:56,490
这是内核S型内核。

44
00:02:56,490 --> 00:03:00,920
好了，有很多内核可以邀请您对其进行全部测试。

45
00:03:01,020 --> 00:03:03,820
您将获得一些有趣的结果。

46
00:03:04,020 --> 00:03:05,760
但是，让我们从基础开始。

47
00:03:05,760 --> 00:03:09,080
让我们从最简单的内核开始，即线性内核。

48
00:03:09,270 --> 00:03:15,720
如您所料，这将使我们的分类器成为线性的分类器，看起来像逻辑

49
00:03:15,720 --> 00:03:16,600
分类器。

50
00:03:16,980 --> 00:03:21,670
所以在这里，让我们输入第一个参数，然后是Kerno。

51
00:03:22,920 --> 00:03:26,830
然后将其设置为线性。

52
00:03:27,510 --> 00:03:28,480
好吧。

53
00:03:29,130 --> 00:03:30,540
那是第二个参数。

54
00:03:30,540 --> 00:03:37,110
好，所以这里的度数是万一您选择了多项式核，那么您可以选择

55
00:03:37,110 --> 00:03:38,600
球，您也会听到。

56
00:03:38,640 --> 00:03:44,880
如果我们选择Noria Poley为S形核，那么我们不必担心gamma不是参数

57
00:03:44,890 --> 00:03:44,960
。

58
00:03:45,000 --> 00:03:47,180
因此，我们也不在乎这一点。

59
00:03:47,190 --> 00:03:51,580
然后，我们还有一些其他参数要包含在分类器中。

60
00:03:51,660 --> 00:03:57,390
实际上，我们将仅包括此随机状态参数，这是最后一个参数，以便所有人

61
00:03:57,390 --> 00:03:58,360
相同的结果。

62
00:03:58,560 --> 00:04:03,750
好吧，欢迎您选择其他随机状态，也可以不包括随机状态，但这只是

63
00:04:04,050 --> 00:04:09,210
如果您希望获得与我相同的结果，那么您可以更轻松地按照我的描述进行操作

64
00:04:09,480 --> 00:04:12,510
在图表上以及我们将要绘制的以下图表上。

65
00:04:12,540 --> 00:04:19,850
因此，这里我们将具有随机状态，并将其设置为零。

66
00:04:20,070 --> 00:04:22,300
至此，分类器已准备就绪。

67
00:04:22,310 --> 00:04:26,450
现在让我们进入训练集。

68
00:04:26,460 --> 00:04:30,770
因此，像往常一样，非常简单的分类器不适合。

69
00:04:30,890 --> 00:04:36,900
因此，在这里我采用第五种方法，然后将我们的课程固定为训练集。

70
00:04:36,990 --> 00:04:38,660
那是。

71
00:04:39,750 --> 00:04:45,250
以及为什么要训练，它应该准备好了。

72
00:04:45,510 --> 00:04:47,660
现在，让我们执行代码。

73
00:04:47,730 --> 00:04:54,480
因此，我们将首先执行此部分，它是执行此操作的数据处理部分。

74
00:04:54,480 --> 00:04:55,390
开始了。

75
00:04:55,440 --> 00:04:58,220
现在，所有数据都经过了很好的预处理。

76
00:04:58,470 --> 00:05:04,930
现在我们可以执行这部分工作，这是将SB纳入我们的训练集中的过程。

77
00:05:04,980 --> 00:05:06,750
因此，让我们这样做。

78
00:05:06,750 --> 00:05:08,810
准备执行。

79
00:05:08,970 --> 00:05:09,540
完善。

80
00:05:09,540 --> 00:05:10,790
一切看起来很棒。

81
00:05:10,800 --> 00:05:15,370
因此，这里我们选择了所有不同的参数，因为我们没有指定它们。

82
00:05:15,480 --> 00:05:20,120
这样很好，我们将在变量资源管理器中查看变量。

83
00:05:20,140 --> 00:05:24,090
在这里，我们有一个数据中心培训课程，一切看起来都很棒。

84
00:05:24,510 --> 00:05:29,850
好吧，现在分类器已经准备好并适合训练集，我们可以预测一些新的

85
00:05:29,850 --> 00:05:34,520
观察和您的观察与往常一样是测试和观察。

86
00:05:34,530 --> 00:05:39,430
因此，在这里我们可以执行此操作以进行矢量预测。

87
00:05:39,480 --> 00:05:40,380
这里是。

88
00:05:40,410 --> 00:05:42,050
现在让我们看看。

89
00:05:42,050 --> 00:05:42,850
为什么弗雷德。

90
00:05:42,900 --> 00:05:45,700
这也是测试的原因。

91
00:05:45,720 --> 00:05:48,060
好的，这就是为什么这里要测试的是白面包。

92
00:05:48,180 --> 00:05:49,860
因此，这些都是真实的结果。

93
00:05:49,870 --> 00:05:51,780
这些是真正发生的结果。

94
00:05:51,880 --> 00:05:53,850
用户是否真的购买了SUV？

95
00:05:53,850 --> 00:05:54,560
是还是不是。

96
00:05:54,720 --> 00:06:00,150
在这里，我们的模型预测所预测的结果是，是或否。

97
00:06:00,210 --> 00:06:00,850
是。

98
00:06:01,230 --> 00:06:01,850
让我们来看看。

99
00:06:01,850 --> 00:06:05,300
由此看来看起来不错，似乎只有很好的预测。

100
00:06:05,310 --> 00:06:06,240
我们来看一下。

101
00:06:06,250 --> 00:06:09,100
因此，前六个用户没有购买SUV。

102
00:06:09,300 --> 00:06:12,630
而且我们的模型可以通过SBC预测天气数据，这非常棒。

103
00:06:13,080 --> 00:06:14,080
是的，这里。

104
00:06:14,130 --> 00:06:15,360
好完美。

105
00:06:15,370 --> 00:06:17,160
现在让我们来看第一个错误。

106
00:06:17,160 --> 00:06:17,860
让我们来看看。

107
00:06:18,080 --> 00:06:18,390
好。

108
00:06:18,410 --> 00:06:20,590
一些人在那个钱包里跳了起来。

109
00:06:20,670 --> 00:06:24,700
一切都很棒，这是不寻常的。

110
00:06:25,120 --> 00:06:25,780
让我们来看看。

111
00:06:25,920 --> 00:06:26,640
好。

112
00:06:27,000 --> 00:06:27,380
哇。

113
00:06:27,390 --> 00:06:28,230
什么问题。

114
00:06:28,230 --> 00:06:29,100
这是怎么回事

115
00:06:32,960 --> 00:06:34,620
。

116
00:06:35,220 --> 00:06:37,640
这是第一个不正确的预测。

117
00:06:37,640 --> 00:06:42,930
我觉得有点奇怪，因为我认为这是第一个100％的模型

118
00:06:42,930 --> 00:06:45,280
对，这是非常罕见的。

119
00:06:45,270 --> 00:06:48,720
我的意思是您永远不会拥有100％正确的模型。

120
00:06:48,720 --> 00:06:53,480
顺便说一句，说所有模型都是错误的，但有些模型是有用的。

121
00:06:53,500 --> 00:06:57,620
因此，这是我们第一个不正确的预测来证明这一点。

122
00:06:57,630 --> 00:07:00,360
因此，是的，Ingrid上瘾存在此错误。

123
00:07:00,530 --> 00:07:02,240
授权首先使用测试集。

124
00:07:02,280 --> 00:07:09,150
因此，这30位第一位用户实际上是购买了SUV，而我们的模型预测，这个人不会购买

125
00:07:09,150 --> 00:07:09,600
SUV。

126
00:07:09,660 --> 00:07:11,570
因此，这是第一个错误的预测。

127
00:07:11,870 --> 00:07:14,010
OK，所以我们按OK OK。

128
00:07:14,280 --> 00:07:17,750
现在让我们更好地看一下错误的预测。

129
00:07:17,910 --> 00:07:21,380
通过查看混乱矩阵来一枪。

130
00:07:21,390 --> 00:07:26,980
因此，从我们比较白色印刷品和白色测试的结果来看，这确实很棒。

131
00:07:26,970 --> 00:07:30,560
因此，我很想知道我们有多少不正确的预测。

132
00:07:30,720 --> 00:07:36,400
因此，让我们选择关于计算指标并执行的这一部分。

133
00:07:36,490 --> 00:07:39,700
现在是这里的计算矩阵。

134
00:07:40,010 --> 00:07:40,850
哦好的。

135
00:07:40,870 --> 00:07:46,360
因此，我想所有最初的观察都具有良好的预测只是一个巧合，因为我们

136
00:07:46,350 --> 00:07:49,190
仍然有10个错误的预测2:08。

137
00:07:49,190 --> 00:07:52,240
这是不正确的预测，但还算不错。

138
00:07:52,250 --> 00:07:57,910
对100个观测值进行10个错误的预测还不错。

139
00:07:57,960 --> 00:08:00,430
所以是的，就像百分之十一样。

140
00:08:00,450 --> 00:08:02,050
除此之外，这是一个新发现。

141
00:08:02,050 --> 00:08:05,470
模型不在训练中心上。

142
00:08:05,830 --> 00:08:09,560
好的，现在是时候度过最美好的时光。

143
00:08:09,570 --> 00:08:11,790
我们将可视化该趋势并获得结果。

144
00:08:12,000 --> 00:08:18,070
因此，现在我们希望您考虑并尝试预测图中将发生的情况。

145
00:08:18,120 --> 00:08:25,620
我们已经知道我们选择了线性核，这表明我们的VM转移是线性分类器

146
00:08:26,010 --> 00:08:31,450
因此请尝试绘制图片或尝试预先猜测预测区域将是什么。

147
00:08:31,590 --> 00:08:34,080
而且有了预测边界。

148
00:08:34,260 --> 00:08:36,100
然后执行代码。

149
00:08:36,510 --> 00:08:41,550
好的，所以您可以发布视频，我现在要执行代码。

150
00:08:42,610 --> 00:08:44,860
因此，首先我要开始进行结果培训。

151
00:08:44,880 --> 00:08:46,430
标题已替换。

152
00:08:46,440 --> 00:08:47,440
一切都很好。

153
00:08:47,430 --> 00:08:49,590
我们准备绘制结果。

154
00:08:49,600 --> 00:08:52,640
因此，让我们来看一下该命令控制器并执行。

155
00:08:52,890 --> 00:08:58,350
在这里，我们将要启动它。

156
00:08:58,360 --> 00:09:02,790
好的，这就是我们对SVM的训练结果。

157
00:09:03,150 --> 00:09:08,980
您是否事先猜测过我们会在此处获得一条直线。

158
00:09:09,210 --> 00:09:15,670
当然，这是因为我们选择了月球内核，因此VM不是线性分类器。

159
00:09:15,820 --> 00:09:22,800
如逻辑对数部分所述，二维空间中的线性分类器为

160
00:09:22,890 --> 00:09:24,130
一条直线。

161
00:09:24,370 --> 00:09:31,380
由于数据不是线性可分离的，因此在这里做出了一些错误的预测，因为您知道

162
00:09:31,380 --> 00:09:37,920
这些绿点是年龄稍高于平均水平且薪水略高的用户

163
00:09:37,920 --> 00:09:44,430
低于平均水平，但与那些不购买SUV的人属于同一类别

164
00:09:44,430 --> 00:09:45,880
买了SUV。

165
00:09:45,990 --> 00:09:51,150
但是由于SBM是线性类，因此它是一条直线，因此无法绘制曲线

166
00:09:51,470 --> 00:09:59,310
您知道以这种方式勾勒出所有红色数据点的轮廓，以便这些绿色点变成绿色

167
00:09:59,320 --> 00:10:00,140
地区。

168
00:10:00,270 --> 00:10:03,880
所以这是不正确的预测，这里我们有一些间接的预测。

169
00:10:03,990 --> 00:10:05,520
如您所见，这些是绿点。

170
00:10:05,520 --> 00:10:11,830
因此，预计真正购买了SUV的用户不会购买SUV，因为他们处于亏损状态

171
00:10:11,830 --> 00:10:13,460
区域权利。

172
00:10:13,550 --> 00:10:14,710
很有趣。

173
00:10:14,700 --> 00:10:19,140
但是实际上，它与逻辑回归是一回事。

174
00:10:19,140 --> 00:10:25,490
因此，您知道我们从逻辑回归开始，我们用KNM K最近邻对模型进行了改进，

175
00:10:25,490 --> 00:10:30,170
现在我们有点回到较低的水平，但是不用担心。

176
00:10:30,170 --> 00:10:36,090
下次我们将构建更复杂的SBM，因为我们将选择其他内核，

177
00:10:36,100 --> 00:10:43,920
将是Galchen内核，所以这将使我们的SVM非线性分类器，您将看到它如何

178
00:10:43,920 --> 00:10:45,580
会做得很棒。

179
00:10:45,880 --> 00:10:48,300
因此，实际上您可以尝试自己做。

180
00:10:48,300 --> 00:10:54,260
非常简单，您只需要再次获取VM代码并尝试在其中使用新内核即可。

181
00:10:54,280 --> 00:11:00,590
您实际上可以尝试使用高斯内核RDF或轮询内核，甚至可以选择S型内核。

182
00:11:00,820 --> 00:11:04,720
但是，如果为SVM选择线性内核，则会得到此结果。

183
00:11:05,080 --> 00:11:09,360
因此，如果我们查看测试结果，结果将是相同的。

184
00:11:09,460 --> 00:11:14,920
我们还将获得一条直线，因为实际上这不会改变，因为这是

185
00:11:14,910 --> 00:11:20,710
基于趋势的预测边界不会改变，但测试结果实际上是

186
00:11:20,700 --> 00:11:21,200
相同。

187
00:11:21,210 --> 00:11:22,550
好艺术家。

188
00:11:23,040 --> 00:11:23,350
好吧。

189
00:11:23,350 --> 00:11:29,610
如您所见，它会将大多数未购买SUV的红色用户定位在右侧

190
00:11:29,830 --> 00:11:33,970
预测区域和右侧绿色预测区域中的绿色区域。

191
00:11:34,200 --> 00:11:40,230
但这仍然在做出一些不正确的预测，因为我们有些家伙

192
00:11:40,230 --> 00:11:46,530
年龄较大，薪水低于仍然购买该SUV的平均水平，因为该SUV非常酷且便宜

193
00:11:46,530 --> 00:11:47,000
。

194
00:11:47,040 --> 00:11:49,770
是的，这与逻辑回归是相同的结果。

195
00:11:49,770 --> 00:11:56,280
因此，我期待向您展示SBM上校可以做的事情，因为这将做得非常好，因为

196
00:11:56,380 --> 00:12:02,650
它甚至会比K更好，然后我们关注主题，因为您记得K

197
00:12:02,670 --> 00:12:08,220
然后有很多不规则之处我们将绘制一条漂亮的曲线，将其分类

198
00:12:08,220 --> 00:12:13,290
好吧，所有的数据点都不会是所有的数据点，而是大多数，尤其是这里的这些人

199
00:12:13,470 --> 00:12:16,230
谁不会是错误的预测。

200
00:12:16,380 --> 00:12:22,380
因此，在我们完成对SVM的支持后，我期待在下一节中与上校SVM见面。

201
00:12:22,440 --> 00:12:24,390
直到那时，机器学习

