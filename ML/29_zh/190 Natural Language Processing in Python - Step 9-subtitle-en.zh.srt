1
00:00:00,360 --> 00:00:02,650
您好，欢迎来到本生活和教程。

2
00:00:02,820 --> 00:00:05,450
这样我们就完成了道德造词的工作。

3
00:00:05,460 --> 00:00:07,670
现在让我们分析一下它是什么。

4
00:00:07,710 --> 00:00:10,140
让我们看看是否可以改善它。

5
00:00:10,440 --> 00:00:12,690
因此，代码执行得很好。

6
00:00:12,720 --> 00:00:17,460
因此，现在让我们进入void Explorer，以更好地了解这个单词模型模型。

7
00:00:17,760 --> 00:00:22,290
如您所见，这是我们美丽的稀疏矩阵。

8
00:00:22,290 --> 00:00:23,150
很大

9
00:00:23,160 --> 00:00:32,430
它有1000行和1565个单词，那就是您知道我们何时使用了所使用的1000个单词中的所有单词

10
00:00:32,430 --> 00:00:37,620
在语料库中，我们总共得到了1,655个单词。

11
00:00:37,620 --> 00:00:42,140
现在，假设您有一百万条评论需要分析以训练您的模型。

12
00:00:42,300 --> 00:00:47,740
好吧，在这种情况下，您将获得超过一百万个单词，这将使您变得极为稀疏。

13
00:00:47,940 --> 00:00:50,210
因此，有两件事要说。

14
00:00:50,220 --> 00:00:59,770
首先，我们可以在此处使用另一个参数，即如果我们回到对象Inspektor，请使用此Max

15
00:00:59,770 --> 00:01:05,860
features参数，该参数将在您的评论中保留最常用的单词，因此您知道该参数将被删除

16
00:01:06,190 --> 00:01:10,630
仅在一种或两种用法中出现的不相关词。

17
00:01:10,660 --> 00:01:12,070
您知道这些与您无关。

18
00:01:12,070 --> 00:01:16,150
如果回到语料库，我们可以很容易地找到一些东西。

19
00:01:16,390 --> 00:01:21,250
好吧，通常里克·史蒂夫（Rick Steve）在这里根本不相关。

20
00:01:21,250 --> 00:01:27,070
我敢打赌，如果我们看一看，它不仅是一个不相关的词，而且是其他评论之一

21
00:01:27,070 --> 00:01:30,190
在1000条评论中，它仅出现一次。

22
00:01:30,260 --> 00:01:32,230
因此，这不是一个相关的词。

23
00:01:32,230 --> 00:01:40,380
通过在此处使用此Max features参数，我们可以通过删除它们来过滤这类单词。

24
00:01:40,380 --> 00:01:49,880
因此，让我们实际执行此操作，让我们实际添加此Max功能参数以过滤不相关的单词。

25
00:01:50,040 --> 00:01:56,190
因此，让我们看看我们可以输入哪些最大特征值来决定我们需要具备的最大特征

26
00:01:56,190 --> 00:02:02,100
看看我们从1000条评论中获得的单词数，每个评论

27
00:02:02,100 --> 00:02:03,600
将其归为一列。

28
00:02:03,780 --> 00:02:07,340
在这里，我们看到有1000 565列。

29
00:02:07,380 --> 00:02:11,850
这意味着我们从评论中提取了165个单词。

30
00:02:12,000 --> 00:02:18,620
因此，例如，我们可以仅采用1000个最常见的单词来过滤不相关的单词

31
00:02:18,900 --> 00:02:22,680
或者，如果我们要小心1500字。

32
00:02:22,680 --> 00:02:25,990
因此，让我们实际选择这个数字1000和500。

33
00:02:26,190 --> 00:02:31,090
因此，在此处输入“最大要素”等于1500。

34
00:02:31,350 --> 00:02:37,590
好吧，我们将只保留这里的评论语料库中最常见的1500个单词。

35
00:02:37,590 --> 00:02:43,710
因此，让瑞萨（Reesa）喜欢这三行文字来为S-matrix功能创建新闻。

36
00:02:43,860 --> 00:02:47,220
因此，在此输入命令和控制即可执行。

37
00:02:47,220 --> 00:02:48,080
开始了。

38
00:02:48,300 --> 00:02:57,750
现在，您可以看到特征的稀疏矩阵包含1500列（即1500个单词）

39
00:02:57,750 --> 00:03:04,230
不仅可以减少稀疏性，还可以提供最相关的词来训练我们的算法，

40
00:03:04,380 --> 00:03:10,230
因此有更多的机会在单词的存在和评论之间建立更好的关联

41
00:03:10,590 --> 00:03:14,680
以及审查结果是正面还是负面。

42
00:03:15,000 --> 00:03:20,910
现在我想说的关于稀疏性的第二件事是，我们还可以减少二维事物

43
00:03:20,910 --> 00:03:27,300
缩减技术，我们将在本课程的后面部分第9部分降维中看到。

44
00:03:27,420 --> 00:03:33,330
所以在这里按“确定”，我们就完成了单词袋的制作。恭喜，您刚刚创建了单词袋

45
00:03:33,330 --> 00:03:36,220
这个特定问题的语言模型。

46
00:03:36,240 --> 00:03:40,550
恭喜，您刚刚创建了为我们此处所有视图建模的单词包。

47
00:03:40,570 --> 00:03:46,920
因此，现在进入下一步，即训练我们的机械模型（即分类模型）

48
00:03:47,130 --> 00:03:48,800
明天我们回来。

49
00:03:48,900 --> 00:03:53,640
好吧，我们当然需要因变量，因为现在我们只有特征的稀疏矩阵

50
00:03:53,640 --> 00:03:56,450
那是自变量的矩阵。

51
00:03:56,640 --> 00:04:00,490
因此，我们还需要包括该因变量。

52
00:04:00,630 --> 00:04:06,600
否则，我们的机器学习模型将受到训练，因为它们根本无法理解结果

53
00:04:06,600 --> 00:04:07,730
相关性。

54
00:04:07,740 --> 00:04:15,450
因此，让我们快速创建这个依赖的竞争对手，以便我们将其称为第3部分中的原因，因为您知道

55
00:04:15,450 --> 00:04:22,410
要做的是简单地复制我们在第3部分中创建的粘贴或模型并非常有效地应用它们

56
00:04:22,500 --> 00:04:23,990
没有任何改变。

57
00:04:24,120 --> 00:04:29,030
这就是为什么我们在此处保持相同的名称，因此y等于的原因。

58
00:04:29,460 --> 00:04:31,410
什么是我们的因变量。

59
00:04:31,500 --> 00:04:39,270
因此，就像在第3部分中一样，我们获取数据集，因为它是Penders现在导入的数据朋友

60
00:04:39,270 --> 00:04:41,230
我们需要添加这个点。

61
00:04:41,340 --> 00:04:46,740
我希望您知道因变量列的索引。

62
00:04:47,010 --> 00:04:48,720
因此，我们在此处添加一些括号。

63
00:04:48,930 --> 00:04:53,250
然后，您知道我们要获取数据框的所有行，因为我们希望获取所有评论

64
00:04:53,520 --> 00:05:00,150
因此，我们在此处添加一列，该列将所有行的所有评论都包含在内，然后进行下一步

65
00:05:00,150 --> 00:05:07,110
选择列的索引，也就是说，如果我们看一下索引为1的数据集

66
00:05:07,230 --> 00:05:11,700
因为非常简单，我们的数据帧具有两列索引和字节，并从0开始。

67
00:05:11,700 --> 00:05:13,190
因此，此列的净值为零。

68
00:05:13,200 --> 00:05:14,720
本专栏还有下一篇。

69
00:05:14,970 --> 00:05:19,770
这就是我们需要的颜色，因为那是包含所有结果的列，无论评论是否

70
00:05:19,770 --> 00:05:20,970
正面或负面。

71
00:05:21,000 --> 00:05:29,200
所以这就是我们的因变量，因此我们需要在此处输入一个索引

72
00:05:30,000 --> 00:05:31,220
差不多完成了。

73
00:05:31,230 --> 00:05:36,930
记住，我们需要在此处添加一个带有值的点，就是这样。

74
00:05:37,020 --> 00:05:42,150
我们准备选择此行以创建我们的因变量矢量。

75
00:05:42,150 --> 00:05:44,660
因此，让我们按命令控制我们并执行。

76
00:05:44,760 --> 00:05:45,670
这里是。

77
00:05:45,700 --> 00:05:52,470
为什么要做好准备，因此现在我们拥有训练机器学习分类所需的一切

78
00:05:52,470 --> 00:05:55,310
道德倒退模式。

79
00:05:55,500 --> 00:05:57,280
因此，我们将在下一个教程中做到这一点。

80
00:05:57,320 --> 00:05:59,100
在那之前享受机器学习

