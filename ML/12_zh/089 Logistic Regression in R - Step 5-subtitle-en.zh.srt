1
00:00:00,330 --> 00:00:04,730
您好，欢迎来到逻辑回归的最后一轮。

2
00:00:04,830 --> 00:00:08,400
因此，让我们回顾一下先前教程中的操作。

3
00:00:08,400 --> 00:00:14,430
我们首先使用数据处理模板对数据进行预处理，然后将其作为数据集

4
00:00:14,430 --> 00:00:19,370
准备使逻辑回归适合我们的训练集，这就是我们在此步骤中所做的。

5
00:00:19,530 --> 00:00:26,190
然后，我们使用此逻辑回归分类器来预测测试结果，以便我们可以评估

6
00:00:26,250 --> 00:00:32,760
逻辑回归模型的预测能力，然后我们确实做了这个混淆矩阵

7
00:00:32,970 --> 00:00:37,170
实际计算所有正确的预测和不正确的预测。

8
00:00:37,320 --> 00:00:42,960
现在我们处于最后一步，即可视化火车组结果和测试结果

9
00:00:42,970 --> 00:00:43,240
。

10
00:00:43,470 --> 00:00:49,350
现在，我们终于进入了逻辑回归模型的最后一轮，这很有趣，因为

11
00:00:49,350 --> 00:00:53,500
我们将可视化训练集结果以及结果测试。

12
00:00:53,670 --> 00:00:59,010
基本上，我们将要绘制一个图，以表示我们物流所发生的一切

13
00:00:59,010 --> 00:01:00,050
回归模型。

14
00:01:00,360 --> 00:01:03,470
因此，让我们制作此图，就像在Python中一样。

15
00:01:03,570 --> 00:01:07,820
我将使用已经准备好的代码并将其粘贴到此处。

16
00:01:07,980 --> 00:01:10,770
我将选择并执行代码。

17
00:01:10,770 --> 00:01:15,480
然后对于那些对代码背后的想法感兴趣的人

18
00:01:15,480 --> 00:01:19,550
该图将在本教程的最后解释代码的工作原理。

19
00:01:19,870 --> 00:01:20,140
好。

20
00:01:20,160 --> 00:01:21,140
因此，让我们开始吧。

21
00:01:21,150 --> 00:01:23,850
让我们再次开始获得乐趣。

22
00:01:23,860 --> 00:01:26,090
所以现在我要粘贴代码。

23
00:01:26,250 --> 00:01:27,140
开始了。

24
00:01:27,490 --> 00:01:33,810
好，所以这里要做的第一件事就是注意到您有这个Elam的学习包

25
00:01:33,810 --> 00:01:34,730
需要安装。

26
00:01:34,740 --> 00:01:37,410
如果它不在您的包装中。

27
00:01:37,560 --> 00:01:41,590
如您所见，它在我的软件包列表中，因为我之前安装过它。

28
00:01:41,730 --> 00:01:47,520
如果不在此处，则需要安装它，因此要安装它，您需要删除此注释，然后您

29
00:01:47,520 --> 00:01:54,390
键入install dot软件包，然后在括号ElemStatLearn中用引号将该软件包的名称

30
00:01:55,050 --> 00:02:00,120
然后选择此行并执行。

31
00:02:00,150 --> 00:02:04,370
现在，您可以看到它正在安装软件包，并且现在已经安装了软件包。

32
00:02:04,380 --> 00:02:08,020
我不必这样做，因为它已经安装了，但这只是为了向您展示。

33
00:02:08,160 --> 00:02:11,510
现在我们准备选择代码并执行它。

34
00:02:11,550 --> 00:02:17,790
即使在这里没有选择它，因为我将库Elam放在库中，以了解执行了哪些库

35
00:02:18,060 --> 00:02:20,750
选中后将激活此软件包。

36
00:02:21,030 --> 00:02:23,900
确定，然后此代码包含15行代码。

37
00:02:24,000 --> 00:02:29,580
Python中也存在这种情况，这是因为它是基于相同思想的完全相同的代码

38
00:02:29,580 --> 00:02:29,800
。

39
00:02:29,970 --> 00:02:33,540
我将解释这个想法，这意味着我将解释该代码如何工作。

40
00:02:33,630 --> 00:02:35,000
在本教程的最后。

41
00:02:35,400 --> 00:02:39,600
现在，让我们选择此代码，然后看看会发生什么。

42
00:02:39,600 --> 00:02:44,350
因此，命令和控制再加上Enter键即可执行，让我们等待。

43
00:02:44,380 --> 00:02:49,580
因此，花一点时间就可以了，但是这将会绘制一些东西。

44
00:02:50,280 --> 00:02:51,290
在那里。

45
00:02:51,480 --> 00:02:51,990
在那里。

46
00:02:51,990 --> 00:02:53,280
那是情节。

47
00:02:53,640 --> 00:02:54,070
好。

48
00:02:54,090 --> 00:02:59,240
因此，对于那些遵循Python教程的人，我们可以获得完全相同的图。

49
00:02:59,540 --> 00:03:00,670
这样就描述了这个情节。

50
00:03:00,680 --> 00:03:03,430
我将扩大这个。

51
00:03:04,190 --> 00:03:04,520
好。

52
00:03:04,530 --> 00:03:08,110
因此，让我们逐步分析该图。

53
00:03:08,190 --> 00:03:13,980
首先让我们专注于所有要点，我们可以看到我们有一些红点和一些绿点

54
00:03:13,990 --> 00:03:14,300
。

55
00:03:14,610 --> 00:03:20,470
因此，我们在该图上看到的所有这些点都是我们训练的观察点。

56
00:03:20,790 --> 00:03:26,800
这就是所有被选择去培训站点的社交网络用户

57
00:03:26,820 --> 00:03:27,100
。

58
00:03:27,540 --> 00:03:35,520
这些用户的每个特征都在于其在x轴上的年龄，并且是估算工资

59
00:03:35,520 --> 00:03:37,260
在y轴上

60
00:03:37,260 --> 00:03:45,380
现在我们可以看到这里有一些红色的点，这里有一些绿色的点是

61
00:03:45,390 --> 00:03:52,710
购买因变量等于零和绿点的列车观测值

62
00:03:52,800 --> 00:03:59,050
是因变量pre-Chase等于1的训练集观测值。

63
00:03:59,070 --> 00:04:06,210
这意味着此处的红点是未购买SUV的用户，此处的绿点是

64
00:04:06,210 --> 00:04:09,130
实际购买SUV的购买者。

65
00:04:09,330 --> 00:04:16,230
现在，作为分析的第一步，我们来解释一下我们在此用户身上观察到的内容

66
00:04:16,240 --> 00:04:16,500
。

67
00:04:16,730 --> 00:04:16,960
好。

68
00:04:16,980 --> 00:04:25,260
因此，首先我们可以看到这些用户的年薪很低，因此这些用户实际上

69
00:04:25,260 --> 00:04:26,940
没有买SUV。

70
00:04:27,150 --> 00:04:33,870
然后，如果我们查看年龄较大且估算工资较高的用户，我们可以看到大多数

71
00:04:33,870 --> 00:04:39,000
这些用户中，实际上是购买了SUV，这实际上是有道理的，因为他们的SUV更像

72
00:04:39,150 --> 00:04:42,600
家用汽车，因此对于这些年龄较大的用户来说更有趣。

73
00:04:42,600 --> 00:04:49,260
在这里，除了较高的估计薪水，我们还可以看到有些老年人即使估计的薪水也很低

74
00:04:49,260 --> 00:04:52,090
工资实际上买了SUV。

75
00:04:52,140 --> 00:04:58,170
我们可以看到这里有一些绿点，对应于高于平均年龄的年龄

76
00:04:58,170 --> 00:04:59,070
在这儿。

77
00:04:59,580 --> 00:05:03,230
但是估计薪水低于平均水平，因为这里有平均水平。

78
00:05:03,690 --> 00:05:04,360
好。

79
00:05:04,620 --> 00:05:09,290
因此，这些人这些年纪较大的人，尽管他们对购买SUV的薪水估计较低，

80
00:05:09,570 --> 00:05:13,680
可能是因为他们已经存了一些钱，或者也许他们已经还清了抵押贷款

81
00:05:13,680 --> 00:05:13,950
。

82
00:05:13,950 --> 00:05:20,900
我不知道可以肯定的是，他们无法抗拒购买这款非常酷的豪华SUV，

83
00:05:20,910 --> 00:05:22,580
低得离谱的价格。

84
00:05:23,680 --> 00:05:29,890
另一方面，我们也可以看到有些年轻人的薪水很高

85
00:05:30,160 --> 00:05:32,460
谁真正买了SUV。

86
00:05:32,630 --> 00:05:36,880
您可能知道这是因为这是一款非常酷的SUV，他们想打动自己的朋友，并带他们进入

87
00:05:36,880 --> 00:05:39,520
公路旅行，或者他们已经有家人了。

88
00:05:39,520 --> 00:05:40,490
我不知道。

89
00:05:40,640 --> 00:05:42,800
无论如何，他们买了SUV。

90
00:05:43,090 --> 00:05:47,790
实际上，有很多购买者，因此这一定是一款非常酷且便宜的SUV。

91
00:05:48,060 --> 00:05:48,450
好。

92
00:05:48,480 --> 00:05:51,670
现在分类的目标是什么。

93
00:05:51,730 --> 00:05:58,540
现在我们在谈论机器学习，为什么我们要创建一些分类器，以及分类器在

94
00:05:58,540 --> 00:06:02,510
至少我们要设法使他们解决这个特定的业务问题。

95
00:06:02,830 --> 00:06:08,250
好吧，这里的目标是将合适的用户分类为合适的类别。

96
00:06:08,440 --> 00:06:15,280
也就是说，我们正在尝试创建一个分类器，将正确的用户吸引到正确的类别中，从而

97
00:06:15,280 --> 00:06:22,000
是的，他们购买了SUV，没有，他们不购买SUV，我们代表了分类器捕捉的方式

98
00:06:22,000 --> 00:06:26,290
通过绘制我所谓的预测区域来吸引用户。

99
00:06:26,320 --> 00:06:30,910
因此，预测区域是我们在该图中看到的两个区域。

100
00:06:30,910 --> 00:06:37,690
这个红色的这里和这个绿色的这里和红色的预测区域是我们的分类器所在的区域

101
00:06:37,720 --> 00:06:43,720
捕获所有不购买SUV的用户，绿色预测区域是该区域

102
00:06:43,720 --> 00:06:46,950
grassfire吸引了所有购买SUV的用户。

103
00:06:47,080 --> 00:06:48,190
不过要小心。

104
00:06:48,250 --> 00:06:54,130
根据针对该红色预测区域的每个用户的分类器。

105
00:06:54,130 --> 00:07:01,210
这是我们的逻辑回归分类器，可以预测用户不会购买SUV，对于每种

106
00:07:01,210 --> 00:07:08,140
这个绿色预测区域的用户在这里可以分类，预测购买SUV的使用甚至

107
00:07:08,140 --> 00:07:10,600
如果现实生活中并非如此，那只是一个预测。

108
00:07:10,780 --> 00:07:14,740
但这就是我们的分类器相信会发生的事情。

109
00:07:14,740 --> 00:07:20,760
关键是分类器预测与事实的比较。

110
00:07:20,920 --> 00:07:24,600
重点是事实，这里的原因是预测。

111
00:07:24,820 --> 00:07:31,930
这是一个了不起的工具，因为对于社交网络的每个新用户，我们的分类器a

112
00:07:31,930 --> 00:07:38,920
logistic回归食火鸡将根据其年龄和估计薪水判断该用法是否属于

113
00:07:38,920 --> 00:07:41,080
到这里的红色预测区域。

114
00:07:41,260 --> 00:07:46,630
因此，它不会购买SUV，或者该用户是否属于此绿色预测区域，并且

115
00:07:46,630 --> 00:07:53,620
因此购买了SUV，这家汽车公司可以通过该业务大幅优化其营销

116
00:07:53,620 --> 00:08:00,130
通过将社交网络广告定位到绿色区域的用户来进行广告系列，因为这些是

117
00:08:00,130 --> 00:08:05,150
根据我们的分类，预计昨天会购买昨天的用户。

118
00:08:05,380 --> 00:08:11,740
现在要了解的另一个非常重要的事情是，这是两个分开的预测区域

119
00:08:11,980 --> 00:08:18,100
一条直线，即此处的直线，该直线称为预测边界

120
00:08:18,610 --> 00:08:22,030
因为它是两个预测​​区域之间的边界

121
00:08:22,270 --> 00:08:26,020
而且它的直线不是随机的。

122
00:08:26,020 --> 00:08:31,860
这是出于特定原因，这就是理解的重要性，因为这就是意义

123
00:08:31,870 --> 00:08:36,350
如果预测边界在此处是一条直线，则为逻辑回归。

124
00:08:36,490 --> 00:08:41,710
那是因为我们的逻辑回归分类器是线性的我们的分类器。

125
00:08:41,950 --> 00:08:46,090
这意味着在这里，因为我们处于二维，所以您知道，因为我们有两个自变量

126
00:08:46,090 --> 00:08:51,610
自从逻辑回归分类器以来，我们处于二维的年龄和估计的对不起

127
00:08:51,670 --> 00:08:58,510
是线性分类器，则预测边界分隔符此处只能是一条直线。

128
00:08:58,570 --> 00:09:04,810
如果我们在三个维度上，那么它将是一个将两个空间分隔开的直行星，但是在这里

129
00:09:04,810 --> 00:09:10,060
二维它是一条直线，如果您的分类器是线性的，它将始终是一条直线

130
00:09:10,060 --> 00:09:12,310
分类器，但您稍后会看到。

131
00:09:12,400 --> 00:09:19,150
那就是当我们建立非线性分类器时，预测边界分隔符将不会是直线

132
00:09:19,150 --> 00:09:19,990
行了。

133
00:09:20,170 --> 00:09:23,600
我现在不会告诉您更多信息，我会让您等待惊喜。

134
00:09:23,920 --> 00:09:30,490
因此，在这里我们可以清楚地看到，我们的逻辑回归分类器可以捕获大多数用户

135
00:09:30,820 --> 00:09:33,460
没有在红色区域购买SUV的人

136
00:09:33,700 --> 00:09:37,990
和大多数在绿色区域购买SUV的用户一样。

137
00:09:37,990 --> 00:09:40,220
因此，它实际上做得很好。

138
00:09:40,240 --> 00:09:47,230
但是，似乎很难抓住一些绿色用户，尽管他们的薪水很低

139
00:09:47,320 --> 00:09:54,220
豪华SUV以及这里也购买了豪华SUV的其他绿色用户，因为

140
00:09:54,220 --> 00:10:00,980
可以在这里看到这些绿点，而这些在绿色区域，这是我们的分类器

141
00:10:01,000 --> 00:10:08,580
预测用户不会购买SUV，那些错误的预测是犹太人专门针对的事实

142
00:10:08,590 --> 00:10:16,000
我将其归类为线性分类器，因为如果我们的用户

143
00:10:16,000 --> 00:10:21,550
呈线性分布，那么我们将在此处拥有所有绿色点和所有红色

144
00:10:21,550 --> 00:10:26,650
在空间中指向此处，然后带有直线的线性分类器可以完全分离

145
00:10:26,760 --> 00:10:29,040
这里所有的红色点和这里所有的绿色点。

146
00:10:29,250 --> 00:10:35,130
但是这里我们有一些叛逆点，他们不需要线性区域，因为我们的分类器

147
00:10:35,130 --> 00:10:37,650
具有线性直线分隔符。

148
00:10:37,650 --> 00:10:43,420
这就是为什么它很难抓住这里的那些用户，而您即使在这里也可以清楚地看到

149
00:10:43,410 --> 00:10:50,160
您尝试在此处很好地旋转此直线时，在错误的类别中总会有一些绿点

150
00:10:50,160 --> 00:10:50,500
。

151
00:10:50,560 --> 00:10:55,000
例如，如果我们尝试像放下一样以这种方式旋转此处。

152
00:10:55,240 --> 00:10:59,290
好吧，我们将在这里捕捉这些绿点以及在这里正确的绿色区域。

153
00:10:59,470 --> 00:11:07,390
但是，由于我们调低了旋转速度，我们将在这里吸引更多绿色用户，因为这会增加并且更加绿色

154
00:11:07,380 --> 00:11:10,030
用户将位于红色区域。

155
00:11:10,020 --> 00:11:12,460
因此，这是最好的分隔符。

156
00:11:12,460 --> 00:11:17,860
Logistic回归测试非常好，它做得更好，因为它只能是直线

157
00:11:17,860 --> 00:11:23,130
线分隔这两个区域，因为要抓住这些用户，这里的绿色用户和绿色用户

158
00:11:23,120 --> 00:11:27,550
这里的用户属于绿色区域，属于正确的类别，我们需要对其进行分类

159
00:11:27,550 --> 00:11:34,390
您所知道的曲线可以正确地对此处和此处的绿色用户进行分类，并将其放置在绿色中

160
00:11:34,380 --> 00:11:39,910
区域，这将阻止我们的班级在这里做出这种错误的预测，因为

161
00:11:39,900 --> 00:11:45,870
是一条带有曲线的直线，其中捕获了所有可能在红色区域以及所有

162
00:11:45,880 --> 00:11:47,960
绿色区域中的绿色用户。

163
00:11:47,980 --> 00:11:53,610
这样就构成了一个很棒的分类器，您将看到我们的非线性分类器将如何构成一个

164
00:11:53,620 --> 00:11:55,270
做到这一点很棒。

165
00:11:55,260 --> 00:11:56,950
我等不及要向您展示。

166
00:11:57,440 --> 00:11:57,870
好。

167
00:11:57,880 --> 00:12:03,570
现在，最终要理解的最后一件事是火车。

168
00:12:03,730 --> 00:12:09,660
这是一个培训中心，这意味着我们的分类人员学习了如何根据这些信息进行分类

169
00:12:09,660 --> 00:12:10,270
这里。

170
00:12:10,260 --> 00:12:15,400
所以我会屏住呼吸几秒钟，直到我发现我们的逻辑回归分类器

171
00:12:15,420 --> 00:12:18,940
可以对新观察结果做出良好的预测。

172
00:12:18,970 --> 00:12:24,720
那就是将新用户分类为正确的区域，这些区域在这里是固定区域，因为

173
00:12:25,060 --> 00:12:30,430
这些是我们的逻辑回归分类器的学习经验所产生的区域，

174
00:12:30,420 --> 00:12:36,990
因此，如果我们查看一些新的观察结果（即新的社交网络用户）

175
00:12:37,000 --> 00:12:39,550
我们将在测试集上找到什么。

176
00:12:39,550 --> 00:12:40,550
等一下

177
00:12:40,890 --> 00:12:47,090
所以这很简单，我们将在这里复制所有这些代码部分。

178
00:12:48,150 --> 00:12:54,830
将其粘贴到此处，我将通过测试集更改此处的训练集。

179
00:12:54,850 --> 00:12:55,720
同样在这里。

180
00:12:55,720 --> 00:12:59,280
我通过测试集更改训练集，仅此而已。

181
00:12:59,590 --> 00:13:04,230
这就是因为我以如下方式构造代码，我们只需要更改趋势设置

182
00:13:04,230 --> 00:13:08,740
到此处的测试集以将此图绘制在特定集上。

183
00:13:08,740 --> 00:13:15,850
但是，在这里更改标题是因为我们要指定它是测试集并且已经准备好

184
00:13:15,860 --> 00:13:16,200
。

185
00:13:16,650 --> 00:13:21,070
因此，让我们选择它并执行。

186
00:13:22,170 --> 00:13:24,240
让我们看看发生了什么。

187
00:13:25,360 --> 00:13:28,700
这是测试集的结果。

188
00:13:28,750 --> 00:13:29,830
因此，这还不错。

189
00:13:29,860 --> 00:13:35,530
这还不错，因为正如我们所看到的那样，大多数红点都在正确的区域内

190
00:13:35,530 --> 00:13:35,680
。

191
00:13:35,760 --> 00:13:37,800
这意味着该区域预计为零。

192
00:13:38,080 --> 00:13:41,970
大多数绿点都在正确的区域。

193
00:13:42,580 --> 00:13:46,960
至于培训中心，有一些观察结果被错误地预测。

194
00:13:46,990 --> 00:13:52,260
这是正常现象，因为它是线性分类器，无法在此处绘制曲线以捕获所有

195
00:13:52,250 --> 00:13:54,490
合适的人。

196
00:13:54,940 --> 00:14:01,820
并记住，在混淆矩阵中，我们计算出十个加上七个错误的预测。

197
00:14:02,200 --> 00:14:08,300
如果您要在此处计算错误的预测，则会发现17个错误的预测。

198
00:14:08,320 --> 00:14:11,610
好的，这就是对图形的解释。

199
00:14:11,620 --> 00:14:15,050
我迫不及待地向您展示了如何制作更强大的分类器。

200
00:14:15,190 --> 00:14:16,930
因此将在下一部分中。

201
00:14:16,960 --> 00:14:20,660
当然，这些将是非线性的或分类器。

202
00:14:21,220 --> 00:14:25,520
因此，对于那些对了解代码的工作方式感兴趣的人。

203
00:14:25,570 --> 00:14:27,350
和我在一起，不然我们会解释的。

204
00:14:27,520 --> 00:14:35,120
因此，我将减少此确定，让我们对此进行解释。

205
00:14:35,590 --> 00:14:36,120
好。

206
00:14:36,490 --> 00:14:43,210
因此，我们的想法是，我们将每个像素观察点视为用户和社交网络

207
00:14:43,210 --> 00:14:45,250
像一个富有想象力的用户。

208
00:14:45,580 --> 00:14:51,700
因此，例如，您知道这里的像素点不是数据集中的用户，但我们可以想象

209
00:14:51,700 --> 00:14:59,640
像素点是指拥有此薪水和年龄的用户，因此我们将分类器应用于该像素

210
00:14:59,640 --> 00:15:05,500
这个用户的观察点，以便分类器预测用户是否要购买

211
00:15:05,500 --> 00:15:06,140
或者没有。

212
00:15:06,250 --> 00:15:10,940
然后，SUV要对您的预测进行分类。

213
00:15:11,020 --> 00:15:15,420
根据预测称为像素上升观察点。

214
00:15:15,580 --> 00:15:22,020
因此，如果预测为否，那么该像素将由SUV使用，则它将变为红色。

215
00:15:22,170 --> 00:15:29,040
如果预测为是，则该像素用户将购买SUV，然后它将点变为绿色

216
00:15:29,050 --> 00:15:29,690
。

217
00:15:29,880 --> 00:15:36,870
因此，我们将该想法应用于该帧中的所有像素，以便最终使分类器着色

218
00:15:37,030 --> 00:15:43,870
他预测为零的所有点均以红色表示，而他预测为1的所有点均以绿色表示

219
00:15:43,880 --> 00:15:44,260
。

220
00:15:44,620 --> 00:15:49,780
因此，现在您有了主意，让我们在这里查看我们的代码，并逐步完成此步骤

221
00:15:49,780 --> 00:15:50,730
理念。

222
00:15:51,310 --> 00:15:51,830
好。

223
00:15:51,900 --> 00:15:55,730
因此，首先我声明了一套交叉训练集。

224
00:15:55,890 --> 00:16:00,050
那是因为您知道我要绘制Trenin集合和集合的图表。

225
00:16:00,280 --> 00:16:05,340
而且由于我们在代码中多次使用Trenin set，因此我在这里用set替换了它。

226
00:16:05,430 --> 00:16:09,650
这就是我复制粘贴测试集相同的代码的时候。

227
00:16:09,700 --> 00:16:14,030
我只需要在这里而不是在整个代码中用trainset替换集合。

228
00:16:14,430 --> 00:16:16,750
所以基本上这只是捷径。

229
00:16:16,840 --> 00:16:20,550
因此，首先我们用x 1和x 2构建一个网格。

230
00:16:20,830 --> 00:16:25,540
因此，我们将训练集的最小值减去1，因为我们不希望这些点成为

231
00:16:25,540 --> 00:16:27,180
挤在图中。

232
00:16:27,180 --> 00:16:29,390
最大值加1相同。

233
00:16:29,740 --> 00:16:36,040
因此，通过这样做，我们得到的是训练集观察点的范围减去1加1，以便

234
00:16:36,230 --> 00:16:38,440
这样我们的点就不会在图中被挤压。

235
00:16:38,740 --> 00:16:44,800
因此，我们正在对训练集的“年龄”列和训练集的“薪水”列进行此操作

236
00:16:46,120 --> 00:16:48,730
然后像我们正在构建网格那样编写。

237
00:16:48,750 --> 00:16:53,260
所以基本上用这三行，我们在这里用所有像素观察点制作网格

238
00:16:53,530 --> 00:16:56,790
那是我们想象中的社交网络用户。

239
00:16:56,800 --> 00:17:01,510
然后，由于此网格集实际上是年龄和薪水两列的矩阵。

240
00:17:01,600 --> 00:17:05,720
但是对于所有虚构的用户来说，它们都是最大的SO观察点。

241
00:17:05,830 --> 00:17:07,000
这实际上是一个矩阵。

242
00:17:06,990 --> 00:17:12,400
因此，在这一行中，我们只给该矩阵的列命名，即年龄和估计薪水

243
00:17:13,720 --> 00:17:16,500
然后就是所有魔术发生的地方。

244
00:17:16,740 --> 00:17:25,120
因为在这里我们使用分类器来预测每个像素观察的结果

245
00:17:25,120 --> 00:17:28,080
虚数像素用户的点。

246
00:17:28,090 --> 00:17:34,660
因此，我们对此进行了预测，然后您就知道了，因为打印函数返回了我们转换的概率

247
00:17:34,650 --> 00:17:38,060
结果为一或零。

248
00:17:38,080 --> 00:17:43,830
这就是为什么我称其为WEIGERT的原因，因为它是网格中所有点的预测，因此返回

249
00:17:43,850 --> 00:17:46,850
网格中所有点的预测向量。

250
00:17:47,110 --> 00:17:50,270
最后，我们绘制整个图。

251
00:17:50,290 --> 00:17:57,330
因此，在此图中，我们包括了所有实际用户及其读取的实际操作。

252
00:17:57,370 --> 00:18:03,170
如果他们不买车，绿色则买了车，我们将所有

253
00:18:03,170 --> 00:18:08,480
创建网格时创建的像素观察点。

254
00:18:08,990 --> 00:18:10,020
这就是这里。

255
00:18:10,380 --> 00:18:16,000
因此，您知道我正在使用春天绿色的颜色，这就是春天绿色和番茄

256
00:18:15,990 --> 00:18:17,550
是番茄的颜色。

257
00:18:18,200 --> 00:18:22,650
对于真实点，我仅使用绿色作为其颜色，颜色和颜色。

258
00:18:22,780 --> 00:18:26,210
所以这个颜色是绿色，因为这个颜色是红色。

259
00:18:26,200 --> 00:18:27,990
3就是这样。

260
00:18:27,990 --> 00:18:29,970
这就是该代码背后的想法。

261
00:18:29,970 --> 00:18:33,170
如果您不了解此代码中的某些内容，那完全没问题。

262
00:18:33,390 --> 00:18:38,030
因为无论如何，我们将使用此代码作为下一个分类器的模板。

263
00:18:38,160 --> 00:18:41,350
因此，我们只需要复制粘贴此代码即可，仅此而已。

264
00:18:41,560 --> 00:18:46,500
因此，这仅适用于那些对编码感兴趣并且对我们的使用方式感兴趣的人

265
00:18:46,510 --> 00:18:48,180
代码来绘制这样的情节。

266
00:18:48,370 --> 00:18:54,730
但是这里要了解的重要一点是，逻辑回归是线性分类器

267
00:18:54,780 --> 00:19:00,690
二维表示它是线性r分隔符，因此可能会错过一些预测，例如

268
00:19:00,700 --> 00:19:03,060
这里的预测是不正确的。

269
00:19:03,340 --> 00:19:07,540
好的，感谢您观看Arta的辛劳和祝贺。

270
00:19:07,530 --> 00:19:11,880
现在您知道了如何使用我们的方法来实现逻辑回归模型。

271
00:19:11,880 --> 00:19:15,600
这是我们进入分类器世界的起点。

272
00:19:15,610 --> 00:19:20,230
您将看到，在以后的旅程中，我们将遇到一些更强大的分类器。

273
00:19:20,280 --> 00:19:22,160
因此，我期待继续这一旅程。

274
00:19:22,300 --> 00:19:24,220
在那之前享受机器学习

