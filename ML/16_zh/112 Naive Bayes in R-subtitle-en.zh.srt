1
00:00:00,270 --> 00:00:06,750
您好，欢迎来到今天或Tauriel，我们将在我们的基础上实现朴素的基础算法

2
00:00:06,770 --> 00:00:06,960
。

3
00:00:07,140 --> 00:00:07,930
现在开始吧。

4
00:00:07,950 --> 00:00:10,970
首先，将文件夹设置为工作目录。

5
00:00:11,100 --> 00:00:16,090
因此，我将进行第3部分的分类，即文件夹。

6
00:00:16,320 --> 00:00:20,760
因此，请确保您拥有社交网络并查看文件，如果是这种情况，请单击更多

7
00:00:20,760 --> 00:00:23,890
关于我在这里将此文件夹设置为工作目录。

8
00:00:24,200 --> 00:00:28,240
现在让我们开始基于我们的名称实现。

9
00:00:28,440 --> 00:00:30,880
因此，我们将采用分类模板。

10
00:00:30,930 --> 00:00:41,820
我们将把所有内容从这里移到这里，然后粘贴到这里，现在我们只需要进行更改

11
00:00:41,910 --> 00:00:43,220
一些东西。

12
00:00:43,230 --> 00:00:47,480
因此，首先让我们更改标题或常规名称，以便我们不会忘记。

13
00:00:47,490 --> 00:00:51,440
因此，这里是海军基础分类器。

14
00:00:51,660 --> 00:00:54,370
训练集也一样。

15
00:00:54,510 --> 00:00:57,250
这样就可以在图形上绘制标题。

16
00:00:57,360 --> 00:01:04,150
现在，我们所有人所需要做的就是建立我们的海军基地。

17
00:01:04,530 --> 00:01:06,180
因此，让我们这样做。

18
00:01:06,180 --> 00:01:09,850
要删除此内容，现在我们要做什么。

19
00:01:10,020 --> 00:01:16,440
好吧，这实际上很有趣，因为我们将使用之前已经使用过的库，但是

20
00:01:16,460 --> 00:01:20,370
分类它将是10 71图书馆。

21
00:01:20,760 --> 00:01:26,100
因此，毫不奇怪，为什么该库非常受欢迎，因为它包含

22
00:01:26,190 --> 00:01:27,390
很多工具。

23
00:01:27,390 --> 00:01:31,320
用于SVM的工具和用于海军基地的工具。

24
00:01:31,560 --> 00:01:33,840
所以在这里我们再次使用这个库。

25
00:01:33,840 --> 00:01:37,460
因此，对于那些不遵循我们上的SVM教程的人。

26
00:01:37,530 --> 00:01:44,530
而对于那些刚开始或第一次的人，也许您没有1871软件包

27
00:01:44,540 --> 00:01:45,990
您的包裹清单。

28
00:01:46,140 --> 00:01:49,820
因此，如果是这种情况，您只需要在此处键入命令即可。

29
00:01:49,820 --> 00:01:57,750
安装该软件包，然后在括号中的引号中添加库的名称

30
00:01:57,750 --> 00:02:04,710
这是10 71，然后您只需要选择它并执行安装软件包即可。

31
00:02:04,710 --> 00:02:10,180
我不会在这里做这件事，因为我已经安装了我的，但是我保证你会正常工作。

32
00:02:10,530 --> 00:02:13,700
因此，我只想对此发表评论。

33
00:02:13,710 --> 00:02:14,460
开始了。

34
00:02:14,610 --> 00:02:21,150
现在，我们还要导入要在包中选择的库，因为它已被选中

35
00:02:21,180 --> 00:02:22,790
但并非总是如此。

36
00:02:22,800 --> 00:02:27,930
而且，如果您想自动执行机器学习脚本，最好始终使用此脚本行

37
00:02:27,930 --> 00:02:33,540
在这里，您可以通过任何人通过库选择包。

38
00:02:33,540 --> 00:02:37,420
现在，我们可以开始按照第5类开始创建naiv了。

39
00:02:37,860 --> 00:02:39,260
好的，让我们这样做。

40
00:02:39,330 --> 00:02:42,080
和往常一样，我们将创建一个变量分类器

41
00:02:44,670 --> 00:02:52,590
然后我们将使用密集型71库的名称库函数Eaton，只有一个库包含

42
00:02:52,590 --> 00:02:57,550
很多功能，其中之一是创建一个不错的Bies分类器。

43
00:02:58,000 --> 00:03:00,980
好的，请小心大写。

44
00:03:01,020 --> 00:03:04,740
因此，它不是资本，而是资本B。

45
00:03:04,850 --> 00:03:05,090
对。

46
00:03:05,100 --> 00:03:10,920
但是然后您知道在帮助这些术语，因此您将在此处弹出这个幼稚的框，以便您

47
00:03:10,920 --> 00:03:12,430
只需按回车即可。

48
00:03:12,870 --> 00:03:16,430
现在让我们输入参数，所以参数是什么。

49
00:03:16,440 --> 00:03:20,100
让我们看看它们，我们按一下这里看看。

50
00:03:20,760 --> 00:03:22,280
他们在这里。

51
00:03:22,470 --> 00:03:27,120
因此，我们实际上只需要输入两个第一个参数。

52
00:03:27,120 --> 00:03:32,770
我们希望您像其他公式一样输入公式，只有第一个公式才能很好地工作

53
00:03:32,770 --> 00:03:33,630
两个。

54
00:03:33,780 --> 00:03:39,460
因此，您将在此处看到输入X，这是训练集的方式。

55
00:03:39,480 --> 00:03:44,550
所以训练集，但是我们需要删除我们的最后一列，并说，因为X实际上是

56
00:03:44,550 --> 00:03:49,770
您看到的矩阵特征将在此处写入数字矩阵或数据框。

57
00:03:49,800 --> 00:03:56,260
所谓特征矩阵就是自变量矩阵

58
00:03:56,270 --> 00:03:56,440
。

59
00:03:56,550 --> 00:04:01,860
由于训练集既包含自变量又包含因变量，我们需要

60
00:04:01,860 --> 00:04:03,490
排除因变量。

61
00:04:03,540 --> 00:04:10,560
为此，我们将在此处添加括号，并在此处输入减号来删除最后一列

62
00:04:10,680 --> 00:04:17,430
然后最后一列的索引为负3，因此它为负3。

63
00:04:17,430 --> 00:04:19,360
好吧，太完美了。

64
00:04:19,530 --> 00:04:23,310
然后出现，然后是下一个参数。

65
00:04:23,340 --> 00:04:25,890
所以你猜下一个参数是什么。

66
00:04:26,250 --> 00:04:31,620
当然，要尝试对您进行分类，我们需要处于Ben enviro的环境中，而且回应与回应

67
00:04:31,620 --> 00:04:32,890
变得无价

68
00:04:32,910 --> 00:04:36,180
因此，当然Y将成为因变量。

69
00:04:36,180 --> 00:04:39,210
那么为什么要事业并接受它。

70
00:04:39,210 --> 00:04:47,480
我们将以这种方式采取这种方式，我们将在每次追逐中投入火车资金。

71
00:04:47,520 --> 00:04:53,030
所以我选择这样写，因为我们可以清楚地看到我们正在使用因变量

72
00:04:53,030 --> 00:04:53,880
购买。

73
00:04:54,030 --> 00:04:54,700
好吧。

74
00:04:54,720 --> 00:04:57,810
这就是您将看到的，它将可以完美地运行。

75
00:04:57,810 --> 00:05:00,040
所以我现在就告诉你。

76
00:05:00,180 --> 00:05:01,940
让我们选择。

77
00:05:02,010 --> 00:05:02,490
抱歉。

78
00:05:02,520 --> 00:05:07,370
因此，在我们必须选择上面的所有预处理步骤之前，必须进行选择。

79
00:05:07,500 --> 00:05:14,170
因此，命令和控制再加上执行所有正确的预处理工作，我们一切顺利。

80
00:05:14,400 --> 00:05:16,930
现在我们可以创建分类器。

81
00:05:17,190 --> 00:05:19,100
是的，让我们这样做。

82
00:05:19,170 --> 00:05:23,870
如果您没有选择任何一个软件包，您也需要选择它。

83
00:05:23,910 --> 00:05:26,320
因此，指挥官控制我们进入执行状态。

84
00:05:26,510 --> 00:05:35,940
这是我们的分类器纵火因此，正如您所见，分类器创建得很好

85
00:05:36,180 --> 00:05:43,260
仅使用这两个信息，当您考虑得很好时，这就是我们需要训练的分类器

86
00:05:43,260 --> 00:05:49,350
因为我们需要自变量的信息和因变量的信息

87
00:05:49,530 --> 00:05:55,260
这样它就可以学习自变量和因变量之间的相关性。

88
00:05:55,260 --> 00:05:57,020
好的，这样才有意义。

89
00:05:57,270 --> 00:06:03,000
现在我们可以使用分类器上的Predict函数创建预测向量y pred

90
00:06:03,000 --> 00:06:05,490
在我们的新数据上，即测试集。

91
00:06:05,490 --> 00:06:07,150
所以我们开始。

92
00:06:07,170 --> 00:06:08,290
白面包很棒。

93
00:06:08,310 --> 00:06:09,910
让我们快速浏览。

94
00:06:10,080 --> 00:06:11,430
为什么要面包。

95
00:06:11,910 --> 00:06:14,790
因此，我们第一次在这里获得了一些东西。

96
00:06:14,790 --> 00:06:21,150
记住，当我们进入“为什么要显示控制台”之前，我们在控制台中列出了所有预测

97
00:06:21,150 --> 00:06:21,450
。

98
00:06:21,450 --> 00:06:24,430
但是在这里，这个系数为零。

99
00:06:24,960 --> 00:06:27,960
因此，这意味着白面包是因素的载体。

100
00:06:28,110 --> 00:06:29,560
但是没有任何因素。

101
00:06:29,670 --> 00:06:31,640
因此，它基本上是一个空向量。

102
00:06:31,920 --> 00:06:37,860
那是因为任何一个图书馆的海军基础功能都无法识别我们的依赖

103
00:06:37,860 --> 00:06:43,560
作为0和1个因子的分类变量购买的变量向量。

104
00:06:43,560 --> 00:06:49,200
因此，对于库和函数，我们一直使用公认的因变量作为因素。

105
00:06:49,290 --> 00:06:54,450
因此我们的预测没有任何问题，也不必包含因变量

106
00:06:54,450 --> 00:06:56,290
购买因素。

107
00:06:56,430 --> 00:06:58,830
但是这里没有基础。

108
00:06:58,950 --> 00:07:02,580
它不将购买的因变量视为因素。

109
00:07:02,580 --> 00:07:08,790
所以我们需要将其编码为因子，因为您可以看到我们是否尝试计算混淆矩阵

110
00:07:08,790 --> 00:07:12,180
下面我们将收到以下错误消息。

111
00:07:12,240 --> 00:07:14,460
所有参数必须具有相同的长度。

112
00:07:14,730 --> 00:07:20,010
是的，的确确实有问题，因为这里的两个参数是测试集三。

113
00:07:20,010 --> 00:07:22,730
这是测试设备的第三个调用和购买。

114
00:07:23,040 --> 00:07:24,970
然后我们传播它。

115
00:07:25,020 --> 00:07:31,050
因此，由于该集合3具有链接100，而白面包的长度为零，因为它是带有

116
00:07:31,050 --> 00:07:35,660
没有向量，那么显然我们不能计算任何混淆矩阵。

117
00:07:35,970 --> 00:07:42,150
因此，我们需要做的是跳回到预处理的第一步，对我们的因变量进行编码

118
00:07:42,150 --> 00:07:49,230
购买作为因子，则基础函数将识别因变量因子，并将

119
00:07:49,230 --> 00:07:55,890
完全能够创建一个分类器，该分类器将允许预测函数以返回期望值

120
00:07:55,890 --> 00:07:57,540
预测向量y。

121
00:07:57,780 --> 00:08:03,530
因此，很高兴看到这个错误，因为这是我们这种情况下机器中的经典错误

122
00:08:03,540 --> 00:08:09,100
从现在开始，您将在将来犯此错误；如果犯了，您将知道如何解决。

123
00:08:10,160 --> 00:08:15,300
所以现在就开始做吧，在这里添加一个新的部分，称为代码和编码

124
00:08:18,420 --> 00:08:19,260
目标

125
00:08:21,750 --> 00:08:25,250
特征X因子。

126
00:08:26,100 --> 00:08:30,940
好的，现在我们将其矢量化或称其为已购买。

127
00:08:31,180 --> 00:08:32,190
好吧，让我们这样做。

128
00:08:32,370 --> 00:08:35,010
因此，我们将从一开始就采用它。

129
00:08:35,010 --> 00:08:40,590
这意味着我们将获取数据集的已购买因变量列，然后我们将

130
00:08:40,590 --> 00:08:45,720
重新计算所有这些，以在所有集合中的任何地方设置此购买列。

131
00:08:45,720 --> 00:08:47,960
有一个训练集和测试。

132
00:08:48,210 --> 00:08:48,450
好。

133
00:08:48,450 --> 00:08:55,440
因此，我们将从数据集中接听最后一个电话，以便我们键入购买的数据集Dollar

134
00:08:56,760 --> 00:09:02,640
权限等于括号数据集中的因子。

135
00:09:02,700 --> 00:09:07,870
然后我们再次进行购买的通话，这是我们数据集的因变量列

136
00:09:07,920 --> 00:09:07,950
。

137
00:09:07,950 --> 00:09:15,180
因此，我们再次以此处购买的美元符号作为本专栏的内容，然后在这里

138
00:09:15,180 --> 00:09:21,490
指定级别或级别请参见0和1。

139
00:09:21,500 --> 00:09:22,330
好吧。

140
00:09:22,590 --> 00:09:23,230
就是这样。

141
00:09:23,280 --> 00:09:29,490
这就是您编码因变量转换成因子的方式，而这实际上就是我们

142
00:09:29,490 --> 00:09:35,760
在我解释如何编码分类变量并将其设置为时，在Botwin数据预处理中看到了

143
00:09:35,760 --> 00:09:36,490
因素。

144
00:09:36,660 --> 00:09:41,100
好了，在这里我们必须要做，实际上我们将把它留在模板中，我们要离开

145
00:09:41,400 --> 00:09:48,340
编码目标特征因子，因为即使我们编码辩论，envoi也会为

146
00:09:48,360 --> 00:09:55,110
其他分类器，例如VM的逻辑回归和其他分类器，那么它将更好

147
00:09:55,110 --> 00:10:01,680
拥有购买的Culham和codices因素，以便我们可以肯定地知道我们能够识别它

148
00:10:01,890 --> 00:10:03,280
作为一个因素。

149
00:10:03,360 --> 00:10:09,300
好的，对于某些分类器，我们不需要指定它，但是让我们继续这样做，这样我们就永远不会得到任何分类器

150
00:10:09,300 --> 00:10:10,350
错误。

151
00:10:10,350 --> 00:10:12,610
好的，我们现在就从头开始。

152
00:10:12,630 --> 00:10:16,820
因此，要做到这一点，我们可以保留所有内容，以便在这里进行所有操作。

153
00:10:16,980 --> 00:10:24,510
因此，我们正在此处通过一些按下控件A1来清除脚本的数据，就是这样。

154
00:10:24,540 --> 00:10:29,330
现在让我们做整个事情，您将看到我们不会收到错误。

155
00:10:29,520 --> 00:10:32,820
因此，让我们选择这种情况。

156
00:10:32,830 --> 00:10:35,240
这就是所有数据预处理步骤。

157
00:10:35,250 --> 00:10:39,330
现在，编码包括了Exiguus。

158
00:10:39,480 --> 00:10:46,020
现在，我们将使基本模型适合列车组执行。

159
00:10:46,200 --> 00:10:47,290
都好。

160
00:10:47,370 --> 00:10:50,830
现在，我们将创建预测向量，为什么使用Pred。

161
00:10:51,210 --> 00:10:58,530
好的，您将看到如果我在此处输入Y pred，我们将没有预测，因此我们可以进行比较

162
00:10:58,530 --> 00:11:04,860
Y表示这是测试集K的第三列，因此我们可以对其进行比较，但我们不要这样做

163
00:11:04,860 --> 00:11:11,100
让我们来说明一下，现在将要创建一个混淆矩阵

164
00:11:11,100 --> 00:11:12,160
没有任何错误。

165
00:11:12,210 --> 00:11:15,010
因此，让我们选择此执行。

166
00:11:15,030 --> 00:11:20,140
现在，您可以看到创建的混淆矩阵没有任何问题。

167
00:11:20,160 --> 00:11:27,180
现在我们来看一下错误的预测数量，即6:53等于14个错误的预测

168
00:11:27,180 --> 00:11:31,500
测试集的100个观察值中的预测还不错。

169
00:11:31,590 --> 00:11:32,560
好，太棒了。

170
00:11:32,850 --> 00:11:37,070
现在，我们终于可以进入有趣的部分，即可视化结果。

171
00:11:37,080 --> 00:11:42,540
因此，如果您不遵循Python教程，那么一个不错的练习就是尝试预测会发生什么

172
00:11:42,540 --> 00:11:48,570
发生时，根据真正向您解释的直觉来预测您将要看到的内容。

173
00:11:48,570 --> 00:11:51,660
您能猜出什么预测区域。

174
00:11:51,750 --> 00:11:54,200
尤其是预测边界。

175
00:11:54,240 --> 00:11:55,780
你能猜出它会是什么。

176
00:11:55,860 --> 00:12:01,620
是海军别斯将线性分类器分类，然后将是直线分类，还是中殿通过

177
00:12:01,620 --> 00:12:09,060
指定一个窄播变量，那么预测边界将是平滑曲线还是预测

178
00:12:09,060 --> 00:12:12,730
边界像佳能一样有很多不规范之处。

179
00:12:12,990 --> 00:12:19,110
您知道这是一个很好的练习，可以尝试练习您刚刚建立的直觉

180
00:12:19,110 --> 00:12:20,210
现在就练习。

181
00:12:20,450 --> 00:12:20,760
好。

182
00:12:20,760 --> 00:12:27,210
因此，我将暂停您在视频上的播放，现在，我将选择此项以向您展示您的预测

183
00:12:27,210 --> 00:12:28,500
是正确的。

184
00:12:28,560 --> 00:12:29,280
让我们来看看。

185
00:12:29,280 --> 00:12:36,510
命令与控制并执行。

186
00:12:40,320 --> 00:12:44,190
这是我们海军基础上的图形解决方案。

187
00:12:44,340 --> 00:12:48,910
这个预测边界是一条平滑曲线不是很漂亮。

188
00:12:48,990 --> 00:12:54,370
很好地对作为观察数据集的数据集进行分类。

189
00:12:54,420 --> 00:12:56,010
它们都不是可分离的。

190
00:12:56,010 --> 00:12:58,220
这有点像曲线的内核。

191
00:12:58,220 --> 00:13:03,600
您知道这是一条优美的平滑曲线，能够捕捉到那些无法捕捉到的绿色用户

192
00:13:04,080 --> 00:13:09,960
通过线性分类器，因为我们有直线，因此无法抓住绿色用户

193
00:13:09,960 --> 00:13:11,890
在这里，将它们归为绿色类别。

194
00:13:11,950 --> 00:13:17,730
属于红色类别的人认为此曲线可以看到做出的错误预测较少

195
00:13:17,760 --> 00:13:23,360
但是对于这里的这些用户，仍然有些像那些一二三。

196
00:13:23,360 --> 00:13:29,670
这些年长的用户没有线性分类器错误预测的估计工资

197
00:13:30,270 --> 00:13:36,000
然后在这里仍然会犯一些错误，我们希望在这里有一个较低的曲线

198
00:13:36,000 --> 00:13:37,630
就像从这里开始的曲线一样。

199
00:13:37,830 --> 00:13:39,660
那么这就是他们在这里可以做的。

200
00:13:39,660 --> 00:13:41,750
那已经是相当不错的工作。

201
00:13:42,060 --> 00:13:45,790
现在，让我们看看它对测试结果的影响。

202
00:13:45,900 --> 00:13:48,390
这里有测试结果代码。

203
00:13:48,530 --> 00:13:52,080
让我们执行它。

204
00:13:54,990 --> 00:13:56,450
这是一个测试。

205
00:13:56,720 --> 00:14:01,920
如果执行此代码花费的时间过多，则可以尝试采用较低的分辨率，因为

206
00:14:01,920 --> 00:14:07,100
现在，您可以看到此处的分辨率提高了0.01步。

207
00:14:07,170 --> 00:14:10,420
由于此分辨率，我们在这里看不到像素。

208
00:14:10,530 --> 00:14:16,260
如果您一分钱都没有解决，那么此板条箱的执行速度会更快，但是您会看到像素

209
00:14:16,260 --> 00:14:17,730
点就是你想要的。

210
00:14:17,730 --> 00:14:21,380
因此，测试结果实际上也不错。

211
00:14:21,450 --> 00:14:27,660
它在将绿色用户分类到正确的网格类别方面做得很好，但是仍然有些

212
00:14:27,720 --> 00:14:33,250
间接预测会抵制，因为此处的那些绿点停留在红色区域。

213
00:14:33,690 --> 00:14:36,690
好的，这就是基于名称的图形结果。

214
00:14:36,690 --> 00:14:38,280
希望您喜欢您所看到的。

215
00:14:38,280 --> 00:14:41,430
我们将有其他类别的其他不同惊喜。

216
00:14:41,430 --> 00:14:46,470
您会看到，当我们查看决策时，我们将获得截然不同的边界预测

217
00:14:46,470 --> 00:14:49,460
树木分类器和随机森林大火。

218
00:14:49,680 --> 00:14:51,620
因此，我希望向您展示这一点。

219
00:14:51,660 --> 00:14:53,490
直到享受机器学习

