1
00:00:00,630 --> 00:00:03,070
您好，欢迎回到机器学习课程。

2
00:00:03,240 --> 00:00:07,350
今天，我们将掩盖关于海军基础级火力的其他评论。

3
00:00:07,350 --> 00:00:08,580
好吧，让我们看看。

4
00:00:08,620 --> 00:00:10,050
Droguet三件事。

5
00:00:10,050 --> 00:00:13,530
第一个问题是为什么将其称为Neive。

6
00:00:13,560 --> 00:00:18,890
第二点是X的吸引力，以及有时我们如何可能将其删除al-Shehri是一个快速捷径

7
00:00:18,910 --> 00:00:19,250
。

8
00:00:19,560 --> 00:00:24,550
第三，当数据集中包含两个以上的特征时会发生什么。

9
00:00:24,570 --> 00:00:24,920
好吧。

10
00:00:24,930 --> 00:00:26,510
因此，让我们开始吧。

11
00:00:26,520 --> 00:00:31,600
问题为什么将此算法称为本机基础算法。

12
00:00:31,860 --> 00:00:33,410
答案很简单。

13
00:00:33,420 --> 00:00:40,620
答案是因为贝叶斯定理需要一些独立的假设，而基本定理是

14
00:00:40,620 --> 00:00:47,520
海军基础机器学习算法的基础，因此是数据库机器学习

15
00:00:47,550 --> 00:00:54,210
算法还依赖于这些假设，这些假设通常是不正确的，因此这是一种

16
00:00:54,210 --> 00:00:56,920
天真地假设他们将是正确的。

17
00:00:56,940 --> 00:00:58,030
这就是为什么很冷的原因。

18
00:00:58,050 --> 00:01:01,170
让我们回到示例中，看看这意味着什么。

19
00:01:01,170 --> 00:01:02,970
所以这里有年龄和薪水。

20
00:01:03,090 --> 00:01:03,470
对。

21
00:01:03,480 --> 00:01:09,180
基于这些，我们使用Neve基础算法将数据点分类为

22
00:01:09,180 --> 00:01:11,640
上班或步行上班的人。

23
00:01:12,060 --> 00:01:18,600
贝叶斯定理是我们应用的方法，实际上要求年龄和工资必须独立

24
00:01:18,870 --> 00:01:20,400
我们正在使用的变量。

25
00:01:20,400 --> 00:01:25,980
在这种情况下，您的薪水必须是独立的，这就像

26
00:01:25,980 --> 00:01:30,750
基本定理，那么您只能应用它，然后您就无法获得这些概率，依此类推。

27
00:01:30,960 --> 00:01:37,900
但是就我们而言，如果您从根本上考虑它，可能并非如此。

28
00:01:37,920 --> 00:01:44,010
年龄和薪水之间可能存在某种关联，因为随着年龄的增长，

29
00:01:44,100 --> 00:01:49,290
经验增加了他们花费的年数，劳动力也增加了，因此

30
00:01:49,290 --> 00:01:53,050
他们的薪水会增长，因此年龄增长后薪水会自然增长。

31
00:01:53,070 --> 00:01:58,560
这可能不是一个超级强的相关性，因为它不适合所有人，但总体上存在某种

32
00:01:58,560 --> 00:01:59,120
相关性。

33
00:01:59,120 --> 00:02:03,150
因此它们不是绝对独立的变量，您甚至可以从图表中看到

34
00:02:03,480 --> 00:02:08,150
仅仅通过看我们的孩子就可以看出两个变量之间存在某种相关性。

35
00:02:08,490 --> 00:02:13,920
因此，鉴于它们不是独立的，因此您无法真正应用贝叶斯定理，因此

36
00:02:13,920 --> 00:02:19,830
您可以将基础算法应用于机器学习，这就是为什么它被称为Neive一种基础算法

37
00:02:19,830 --> 00:02:26,670
因为即使变量或特征不是独立的或非独立的，通常也会应用它

38
00:02:26,670 --> 00:02:31,020
完全独立，并且仍在应用，并且仍能提供良好的效果。

39
00:02:31,080 --> 00:02:34,860
这就是为什么它被称为天真，因为它是一个天真的假设。

40
00:02:34,860 --> 00:02:35,700
好吧。

41
00:02:35,700 --> 00:02:37,370
第二个P.O. 框。

42
00:02:37,470 --> 00:02:40,440
因此，让我们看一下我们的表现。

43
00:02:40,440 --> 00:02:46,360
有点像倒带并分析我们在证明步骤中所做的事情。

44
00:02:46,380 --> 00:02:50,450
因此，在步骤2中，我们接管了工作。

45
00:02:50,460 --> 00:02:55,400
因此，当我们计算X时，我们在新点周围画了一个圆。

46
00:02:55,400 --> 00:02:58,350
我们只是移动了数据点，所以它不会妨碍您。

47
00:02:58,350 --> 00:03:00,130
然后我们为该区域着色。

48
00:03:00,150 --> 00:03:07,770
那么什么是P.O. 框将剥离是从该数据集中随机选择的点的可能性

49
00:03:08,100 --> 00:03:13,630
将展示与我们将要添加的数据点类似的功能。

50
00:03:13,740 --> 00:03:19,310
正如我们所同意的那样，该圈子中的任何事物都被视为与我们的数据点相似。

51
00:03:19,320 --> 00:03:25,890
另一种思考的方式是，如果我将随机变量或随机数据点放入

52
00:03:25,890 --> 00:03:27,620
此数据集现在。

53
00:03:27,810 --> 00:03:32,820
它掉进圈子里的可能性是多少？

54
00:03:32,880 --> 00:03:39,720
功能类似于我要添加到数据集中的功能。

55
00:03:39,750 --> 00:03:45,750
所以基本上它落入了那个圆圈，所以X的收益是相似观察或相似观察的数量

56
00:03:45,750 --> 00:03:50,240
观察意味着与我们将要添加的点相似的观察。

57
00:03:50,430 --> 00:03:54,390
除以观察总数，在我们的情况下为真。

58
00:03:54,450 --> 00:03:59,620
因此，我们可以看到现在是四分之四，而这个圈子中有四个点被三分。

59
00:03:59,970 --> 00:04:05,520
有趣的是，两次的结果相同。

60
00:04:05,520 --> 00:04:11,320
因此，这是第一步的第二步，我们在这里为步行上班的人们进行计算。

61
00:04:11,340 --> 00:04:13,860
因此，这种能力适合那些走路的人。

62
00:04:13,890 --> 00:04:20,150
都是一样的，所以基本上X不会随着您在脚背一号中计算而改变

63
00:04:20,220 --> 00:04:25,080
计算具有这些特征的人是走路上班的人的可能性，或者

64
00:04:25,080 --> 00:04:29,870
您在“第2步”方案中进行计算，在此情况下，您是否具有该功能的人是

65
00:04:29,870 --> 00:04:33,850
一个开车去上班的人，因此是同一时间。

66
00:04:33,880 --> 00:04:34,560
那是什么意思。

67
00:04:34,560 --> 00:04:38,800
好吧，让我们看一下形式主义者，以便您可以看到在第1步中得出的公式是p off。

68
00:04:38,810 --> 00:04:45,570
表现出他走路的特征X的人的问题走路或上班

69
00:04:45,610 --> 00:04:51,070
Concepcion X中的公式位于底部，而在第二步中，可能会出现

70
00:04:51,070 --> 00:04:55,050
特征X是开车去上班的人。

71
00:04:55,210 --> 00:04:57,190
如您所见，X的证明位于此处的底部。

72
00:04:57,190 --> 00:05:00,300
那么我们在步骤3中做了什么。

73
00:05:00,310 --> 00:05:00,520
对。

74
00:05:00,520 --> 00:05:02,280
因此，让我们从这里继续进行第三步。

75
00:05:02,280 --> 00:05:03,820
第三步，我们比较了两者。

76
00:05:03,820 --> 00:05:10,210
现在，如果我们采用这两个公式，这些公式的右边将它们放入比较中

77
00:05:10,510 --> 00:05:11,210
像那样。

78
00:05:11,220 --> 00:05:14,010
您会在底部看到分母是相同的。

79
00:05:14,020 --> 00:05:18,970
现在，尽管我们知道恶魔提名者不为零，但实际上大于零的概率

80
00:05:19,380 --> 00:05:21,500
不小于零，我们知道它不为零。

81
00:05:21,520 --> 00:05:22,860
这样我们就可以摆脱它。

82
00:05:22,860 --> 00:05:23,180
对。

83
00:05:23,210 --> 00:05:28,030
我们可以将双方乘以X，因此符号不会改变。

84
00:05:28,060 --> 00:05:30,320
而且，我们将摆脱分母。

85
00:05:30,580 --> 00:05:33,930
这样，我们实际上就不必执行该计算。

86
00:05:33,930 --> 00:05:35,640
因此，这是少执行的计算。

87
00:05:35,640 --> 00:05:42,280
因此，您可以只比较这些计算的顶部，这就是很多

88
00:05:42,280 --> 00:05:42,520
时间。

89
00:05:42,520 --> 00:05:47,500
因此，如果您完成了其他有关机器学习的课程，或者已经阅读了一些有关机器的文章，

90
00:05:47,500 --> 00:05:54,490
了解到您会发现这种情况经常发生，有时也没有提到

91
00:05:54,550 --> 00:06:00,070
这种情况正在发生，因此有时可以假设或者可以假设您知道发生了什么

92
00:06:00,070 --> 00:06:00,150
。

93
00:06:00,160 --> 00:06:02,500
因此，请注意这一点。

94
00:06:02,500 --> 00:06:03,940
这是一种伊扎贝拉方法。

95
00:06:03,970 --> 00:06:09,470
正如我们所讨论的那样，这样做是完全有效的，但这仅在您比较两者的情况下才可以。

96
00:06:09,470 --> 00:06:13,870
正确，如果您只比较两者，那么如果您确实想计算

97
00:06:13,870 --> 00:06:14,600
值。

98
00:06:14,620 --> 00:06:16,790
所以我们说75％25％。

99
00:06:16,960 --> 00:06:22,390
您想要计算值无法做到的，因为这将是两个不同的现实，因为您

100
00:06:22,390 --> 00:06:24,640
应该除以某个值。

101
00:06:24,640 --> 00:06:26,100
它不是实际值。

102
00:06:26,110 --> 00:06:33,300
而且，如果您想喜欢它，可以比较但可以计算实际值，也许

103
00:06:33,310 --> 00:06:37,930
执行一些操作，或者您知道您可以据此计算值。

104
00:06:37,930 --> 00:06:42,650
您想将这种情况下的值与另一个问题权的值进行比较。

105
00:06:42,650 --> 00:06:48,430
就像来自另一种情况的值，其中X的p会不同。

106
00:06:48,490 --> 00:06:53,720
并非来自您正在使用的这个特定示例。

107
00:06:53,740 --> 00:06:57,460
您想比较该人成为走路上班的人的可能性。

108
00:06:57,510 --> 00:07:02,980
在此示例中，该人可能是一个从其他地方步行去工作的人

109
00:07:02,980 --> 00:07:06,990
如果您想像这样进行比较，那么示例将有所不同。

110
00:07:07,000 --> 00:07:10,770
这也将不起作用，因为您知道X是不同的，所以请注意这一点。

111
00:07:10,900 --> 00:07:15,110
永远是越来越安全的方法，永远只是为了执行完整的计算。

112
00:07:15,310 --> 00:07:20,560
但是，如果您正在做一个孤儿，或者您只是想节省时间，或者您可能正在阅读

113
00:07:20,560 --> 00:07:26,370
那么其他文献也很高兴知道可以使用分母的方法

114
00:07:26,450 --> 00:07:28,610
只是在两者之间进行比较时。

115
00:07:28,960 --> 00:07:29,220
好吧。

116
00:07:29,230 --> 00:07:34,420
因此，这是另一点，或类似的一种说法，也可能是一种捷径。

117
00:07:34,420 --> 00:07:36,400
因此，这是当天的最后一点。

118
00:07:36,490 --> 00:07:39,730
当我们有两个以上的班时会发生什么。

119
00:07:39,740 --> 00:07:44,500
所以请记住，在这种情况下，我只有两个班，红色，绿色或步行上班的人

120
00:07:44,500 --> 00:07:48,260
当人们拥有更多克拉斯时，人们会努力工作。

121
00:07:48,280 --> 00:07:50,660
挑战有何不同？

122
00:07:50,800 --> 00:07:56,770
好吧，我们什么时候只有两个班级进行比较，因为我们记得我们比较了人们

123
00:07:56,760 --> 00:08:02,050
展示特征X的人基本上可以正常工作，以至于我们添加了新的数据点。

124
00:08:02,080 --> 00:08:05,650
一个人走路去上班的概率是多少？

125
00:08:05,650 --> 00:08:08,100
新数据点是一个努力工作的人。

126
00:08:08,260 --> 00:08:14,770
事实证明，我们将75％与25％进行比较，而75％大于

127
00:08:14,760 --> 00:08:20,160
25％，因此该人成为上班人士的概率为

128
00:08:20,170 --> 00:08:23,150
比那个开车去工作的人更大。

129
00:08:23,170 --> 00:08:27,150
因此，我们决定将他们归类为步行上班的人。

130
00:08:27,310 --> 00:08:29,110
这非常简单。

131
00:08:29,130 --> 00:08:34,750
而且，您会发现，每当您只有两个类时，它总会加起来一个。

132
00:08:34,750 --> 00:08:37,730
因此，我们甚至不必计算第二个。

133
00:08:37,780 --> 00:08:42,990
我们可能只是停止了这一步，因为如果这是75％，那么这一步就自动是25％

134
00:08:43,010 --> 00:08:43,170
。

135
00:08:43,350 --> 00:08:47,780
因此，如果您有两个班级，总会这样，如果您有三个班级，则方式会改变

136
00:08:47,860 --> 00:08:49,360
它变得更有趣吧。

137
00:08:49,390 --> 00:08:51,280
因此，您拥有Calford，而且也有。

138
00:08:51,300 --> 00:08:52,900
然后还有两个。

139
00:08:52,890 --> 00:08:59,020
因此，如果您只计算两个类之一，那么自从计算一个就可以确定

140
00:08:59,010 --> 00:09:04,850
如果大于50％，则立即分配，如果小于50％，则分配该类

141
00:09:04,870 --> 00:09:10,630
另一个班级，如果等于50％，那么您就像平局，而如果您有两个或三个

142
00:09:10,620 --> 00:09:15,270
或更多的班级，仅计算一个班级是不够的，因为那时您还有另外两个班级

143
00:09:15,280 --> 00:09:21,340
您仍然需要计算，至少要计算另外一个，这只是意味着它更有趣

144
00:09:21,580 --> 00:09:28,300
当您有更多类时的选择问题，这几乎是您更改时的主要问题

145
00:09:28,300 --> 00:09:29,660
有更多的课程。

146
00:09:30,010 --> 00:09:31,150
然后我们走了。

147
00:09:31,140 --> 00:09:37,440
今天就这些，我希望您能在朴素的基础班级大火中享受这些额外的技巧。

148
00:09:37,600 --> 00:09:39,530
我期待下一个。

149
00:09:39,540 --> 00:09:41,260
在那之前在德国学习

