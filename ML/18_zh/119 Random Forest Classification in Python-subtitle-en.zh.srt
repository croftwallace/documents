1
00:00:00,270 --> 00:00:06,110
您好，欢迎使用此Python教程，我们终于可以进入分类的最后一轮了

2
00:00:06,130 --> 00:00:11,820
机器学习历程中的冒险，因为今天我们将建立最后一个分类器

3
00:00:11,910 --> 00:00:13,870
这是一个随机森林分类器。

4
00:00:14,130 --> 00:00:18,960
所以您将看到它看起来很像决策树，因为实际运行的森林就像

5
00:00:19,000 --> 00:00:24,840
决策树的范数，每个决策树都预测结果，然后根据多数票决定最终结果

6
00:00:24,840 --> 00:00:26,250
做出预测。

7
00:00:26,400 --> 00:00:31,830
因此，如果您正确理解了决策树，那么理解随机森林和

8
00:00:31,830 --> 00:00:35,830
预测您将在本教程中发现的图形结果。

9
00:00:36,210 --> 00:00:37,020
因此，让我们开始吧。

10
00:00:37,020 --> 00:00:39,850
让我们开始制作这个随机森林分类器。

11
00:00:40,020 --> 00:00:43,880
同时，让我们尝试猜测我们将获得什么。

12
00:00:44,200 --> 00:00:47,460
好的，让我们从将正确的文件夹设置为工作目录开始。

13
00:00:47,460 --> 00:00:51,160
我将在这里找到比机械A-Z文件夹高的专家。

14
00:00:51,330 --> 00:00:57,060
第3部分分类为我们随机分类，您可以看到这是我们最后的分类器

15
00:00:57,060 --> 00:01:02,850
在此处进行构建，因为下一部分将专门评估分类Morell的性能

16
00:01:03,090 --> 00:01:04,480
并改善它们。

17
00:01:04,650 --> 00:01:07,260
是的，我们绝对可以进入最后一轮。

18
00:01:07,410 --> 00:01:12,470
因此，让我们执行此操作，以转到我们的文件夹中，以确保在您提交文件时社交网络能够为您服务。

19
00:01:12,540 --> 00:01:17,380
如果是这种情况，让我们单击此按钮以将该文件夹设置为工作目录。

20
00:01:17,610 --> 00:01:22,950
现在，当然开始了，我们将使用我们的分类模板

21
00:01:22,950 --> 00:01:31,480
将选择从顶部到底部的所有副本，然后将其粘贴到此处。

22
00:01:32,310 --> 00:01:32,840
好吧。

23
00:01:32,850 --> 00:01:35,010
现在让我们进行基本更改。

24
00:01:35,010 --> 00:01:44,260
像这里一样，我们正在填充随机森林分类。

25
00:01:44,490 --> 00:01:48,640
然后，这里的训练集将创建一个随机森林分类器。

26
00:01:48,750 --> 00:01:52,640
然后，让我们在此处更改图的标题，以免以后忘记。

27
00:01:52,680 --> 00:02:02,250
因此，这里我们将用随机森林分类代替分类，因为您知道我们正在做出区分

28
00:02:02,250 --> 00:02:06,790
随机森林回归与森林运行分类之间的关系。

29
00:02:07,830 --> 00:02:10,610
至于决策树。

30
00:02:11,670 --> 00:02:17,850
好吧，现在我们进行了基本更改，现在让我们进行主要和最重要的更改，

31
00:02:17,850 --> 00:02:20,600
是在这里创建分类。

32
00:02:20,640 --> 00:02:23,660
因此，让我们这样做实际上非常简单。

33
00:02:23,820 --> 00:02:29,120
和往常一样，我们将为该作业导入合适的班级，该班级将在美国分类机上运行。

34
00:02:29,250 --> 00:02:31,170
和往常一样，这非常直观。

35
00:02:31,230 --> 00:02:37,140
然后，我们将创建此类的分类器对象，这将是您自己的随机第一类

36
00:02:37,560 --> 00:02:45,330
然后，我们将为我们的气化炉将这个随机变量拟合到我们在此处创建的快速训练集

37
00:02:45,330 --> 00:02:48,050
提醒您，此部分是数据预处理步骤。

38
00:02:48,090 --> 00:02:54,060
在这里，我们导入库，在中央库中导入，在这里我们导入数据集，在这里我们拆分数据集

39
00:02:54,060 --> 00:03:01,320
放入由ex火车和y火车组成的火车组，以及由X检验和Y检验组成的测试组。

40
00:03:01,380 --> 00:03:06,330
在这里，我们进行一些功能缩放，这在这里很重要，因为自从这里开始，我们对我们的称赞感到鼓舞

41
00:03:06,330 --> 00:03:09,530
分辨率为0.01的图形结果。

42
00:03:09,660 --> 00:03:16,230
那就是我们的分类器将以一种分辨率来预测我们图的所有像素点的类

43
00:03:16,230 --> 00:03:17,610
开零1。

44
00:03:17,640 --> 00:03:22,840
我们需要在此处应用功能，以便我们的代码执行得更快。

45
00:03:23,460 --> 00:03:25,600
好的，这是一个快速提醒。

46
00:03:25,620 --> 00:03:27,730
现在让我们创建分类。

47
00:03:27,960 --> 00:03:35,280
因此，正如我刚刚告诉我们的那样，我们将从scaler和sambil导入随机的第一分类类

48
00:03:35,280 --> 00:03:35,910
图书馆。

49
00:03:36,060 --> 00:03:42,660
因此，当他们学习艺术和符号时，我们必须从中键入内容。

50
00:03:42,660 --> 00:03:48,690
在这里是import，这就是我们导入类的位置，并且以这种方式拼写了类名。

51
00:03:48,690 --> 00:03:56,040
将我们的随机资本F用于森林，将资本C用于分类器。

52
00:03:56,520 --> 00:03:57,790
这是我们的任务。

53
00:03:57,960 --> 00:04:03,350
现在让我们创建这个类的新对象，这将是Ranum对我们自己进行分类。

54
00:04:03,390 --> 00:04:04,280
因此，让我们这样做。

55
00:04:04,290 --> 00:04:10,380
像往常一样，我们称其为气化器等于，然后我们将调用该类以创建此对象

56
00:04:10,440 --> 00:04:13,500
就是说复制这里会更快。

57
00:04:13,500 --> 00:04:15,030
复制粘贴。

58
00:04:15,150 --> 00:04:16,290
这是我们的最后一个。

59
00:04:16,470 --> 00:04:20,440
现在让我们加上一些括号，因为我们需要输入一些参数。

60
00:04:20,670 --> 00:04:24,910
在这里，让我们检查此类，以了解我们必须输入哪些参数。

61
00:04:25,020 --> 00:04:32,780
因此，要检查类，请键入命令I，这是有关Ranum第一台气化炉的信息。

62
00:04:33,120 --> 00:04:36,410
因此，让我们看一下我们感兴趣的参数。

63
00:04:36,720 --> 00:04:39,920
确定，因此第一个参数是估计量。

64
00:04:39,930 --> 00:04:42,710
那实际上是森林中树木的数量。

65
00:04:42,720 --> 00:04:49,440
因此，这实际上是可以预测社交网络的每个用户购买还是购买的树的数量。

66
00:04:49,470 --> 00:04:51,200
不买SUV。

67
00:04:51,210 --> 00:04:57,570
好吧，假设我们选择，那么您将拥有10棵树，用于预测每个用户购买还是不购买

68
00:04:57,570 --> 00:05:03,890
SUV，然后基于多数票，Ranum将为我们选择将投票给最终的预测

69
00:05:03,900 --> 00:05:05,600
那是对随机森林的预测。

70
00:05:05,760 --> 00:05:08,680
用户是否购买SUV。

71
00:05:08,940 --> 00:05:12,030
因此，让我们选择默认的树数。

72
00:05:12,060 --> 00:05:17,430
您实际上可以玩转并尝试一些不同数量的树，但让我们选择默认数量

73
00:05:17,430 --> 00:05:19,130
的树木是10棵树。

74
00:05:19,290 --> 00:05:25,690
因此，让我们在这里放置一个估计量，即Becton。

75
00:05:26,160 --> 00:05:30,420
因此，我强烈建议您尝试并尝试不同数量的转帐，因为您会看到

76
00:05:30,420 --> 00:05:37,680
您会得到不同的结果，如果这样做，请务必注意检测过度拟合。

77
00:05:37,860 --> 00:05:44,190
您不记得过度拟合只是当您将过多的分类器拟合到训练集中时

78
00:05:44,580 --> 00:05:50,610
以这样的方式，当您想从新集（例如测试集）中预测观察结果时，

79
00:05:50,610 --> 00:05:55,950
您的课程，否则您会因为在培训课程中使用过多而感到迷失，因为

80
00:05:56,030 --> 00:05:57,240
它太多了。

81
00:05:57,540 --> 00:06:01,650
这就是为什么对您来说有趣的是，您可以玩几棵树，因为您可以学习

82
00:06:01,650 --> 00:06:08,060
如何再次检测过度拟合，那么下一个参数标准的下一个参数是什么。

83
00:06:08,100 --> 00:06:13,830
所以请记住，在决策树部分，我们选择了熵标准，我提醒秘书

84
00:06:13,830 --> 00:06:19,370
以这样的方式测量分割的质量：分割之后，您是否更均匀

85
00:06:19,370 --> 00:06:25,410
知道用户组，熵将减少得更多，因为您知道熵是物理学中的一个概念

86
00:06:25,530 --> 00:06:27,980
在某种程度上代表了疾病。

87
00:06:27,990 --> 00:06:31,730
因此，熵越高，粒子越无序。

88
00:06:31,860 --> 00:06:38,040
好吧，也就是说粒子是用户，并且节点与父节点相比更加同质

89
00:06:38,040 --> 00:06:42,680
节点与熵相比，熵较低。

90
00:06:42,810 --> 00:06:48,840
信息增益是您从拆分之前到拆分之后设法减少了多少熵

91
00:06:48,870 --> 00:06:54,220
因为它是父节点的熵减去子节点的熵之间的差

92
00:06:54,220 --> 00:06:54,550
。

93
00:06:54,780 --> 00:07:00,180
因此，对于这个绅士，我们将在此处选择一个熵小动物，以便我们所有的树

94
00:07:00,180 --> 00:07:05,900
森林中的用户根据熵做出预测，即每个用户是否购买Johnson或SUV

95
00:07:05,910 --> 00:07:06,790
标准。

96
00:07:07,170 --> 00:07:13,530
好的，让我们使用等于熵的准则。

97
00:07:14,210 --> 00:07:14,940
好吧。

98
00:07:15,030 --> 00:07:16,590
现在，下一个论点是什么。

99
00:07:16,650 --> 00:07:17,790
最大功能确定。

100
00:07:17,810 --> 00:07:22,010
这一点不是很重要，在这里我们不需要它来建立随机森林。

101
00:07:22,010 --> 00:07:25,240
以后我们可以使用它来改进并试用我们的模型。

102
00:07:25,250 --> 00:07:27,160
这些其他参数在此相同。

103
00:07:27,260 --> 00:07:30,190
这就是在模型中添加一些差异。

104
00:07:30,480 --> 00:07:35,970
但是我们将像往常一样添加的是此处的随机状态参数，并将其设置为零，以便我们都能

105
00:07:35,970 --> 00:07:37,070
相同的结果。

106
00:07:37,410 --> 00:07:38,590
所以我们把它放在这里。

107
00:07:38,700 --> 00:07:42,390
随机状态等于零且完美。

108
00:07:42,380 --> 00:07:45,680
这就是我们需要为我们的分类器创建随机的所有内容。

109
00:07:46,050 --> 00:07:49,990
现在，准备就绪，让我们将其输入到训练集中。

110
00:07:50,000 --> 00:07:56,880
如您所知，我们将在这里进行随机化分类，然后使用其中一种方法

111
00:07:56,880 --> 00:07:57,040
。

112
00:07:57,210 --> 00:08:03,090
好吧，对我们来说，rennen的方法之一是对一个类进行分类，这当然是适合的拟合方法

113
00:08:03,090 --> 00:08:05,000
此分类器到训练集。

114
00:08:05,390 --> 00:08:15,870
在这种方法适合的情况下，我们输入由X火车组成的火车组，以及为什么要使用此火车

115
00:08:15,870 --> 00:08:16,590
代码行。

116
00:08:16,590 --> 00:08:22,740
这是随机的头等舱，我们将在训练集上学习如何做出一些未来的预测

117
00:08:23,370 --> 00:08:26,510
这实际上是我们的下一个代码部分。

118
00:08:26,510 --> 00:08:29,100
好的，让我们开始执行代码。

119
00:08:29,270 --> 00:08:32,130
我们非常接近发现结果。

120
00:08:32,120 --> 00:08:33,330
我等不及要给你看。

121
00:08:33,330 --> 00:08:35,060
所以现在就开始吧。

122
00:08:35,250 --> 00:08:38,170
让我们从执行数据预处理阶段开始。

123
00:08:38,250 --> 00:08:42,550
因此，我将选择从这里到顶部的所有内容。

124
00:08:42,600 --> 00:08:44,280
好的，我没有任何事要做。

125
00:08:44,270 --> 00:08:47,220
有了模板，一切都已经正确制作了。

126
00:08:47,460 --> 00:08:53,330
因此，让我们在此处执行此操作，因为如果您进入Xexper，可以确保它正确执行，则可以确保

127
00:08:53,340 --> 00:08:54,560
一切顺利。

128
00:08:54,570 --> 00:08:58,190
我们有我们的数据集，我们的训练集包括极限。

129
00:08:58,230 --> 00:09:03,650
以及为什么要在哪个火车上运行并为我们分类，您将学习相关性，然后学习如何

130
00:09:03,650 --> 00:09:05,190
做出未来的预测。

131
00:09:05,460 --> 00:09:11,460
然后我们有一个由X测试和Y测试组成的测试集，它们将位于它们周围

132
00:09:11,460 --> 00:09:14,350
对于我们的分类，我们将做出这些新的预测。

133
00:09:14,370 --> 00:09:14,630
好。

134
00:09:14,630 --> 00:09:20,880
所以现在一切似乎都很好，让我们在这里执行此代码部分来构建美国气化炉的运行

135
00:09:21,060 --> 00:09:23,220
命令和控制加回车执行。

136
00:09:23,220 --> 00:09:23,810
好吧。

137
00:09:23,820 --> 00:09:24,740
完善。

138
00:09:24,750 --> 00:09:29,730
在这里我们可以看到默认情况下我们选择了所有默认参数。

139
00:09:30,060 --> 00:09:34,800
在这里，我们可以确认我们的雨林中有10棵树。

140
00:09:34,830 --> 00:09:40,820
对于社交网络的每个用户而言，这真是太棒了，无论用户是否购买您，我们都会投票给10棵树

141
00:09:40,930 --> 00:09:42,120
了解SUV。

142
00:09:42,120 --> 00:09:43,910
因此结果令人鼓舞。

143
00:09:44,120 --> 00:09:47,740
因此，让我们现在检查一下，让我们做出未来的预测。

144
00:09:47,750 --> 00:09:54,380
因此，在这里我选择此选项是为了预测测试的每个用户是否会购买“是”或“不是” SUV。

145
00:09:54,720 --> 00:09:57,120
因此，让我们按Command或控制我们并告诉您真相。

146
00:09:57,480 --> 00:10:02,610
现在我们有了预测向量，我们要做的就是比较这个向量

147
00:10:02,610 --> 00:10:09,110
无论您使用偏见是还是否，SUV的预测结果都是真实的结果

148
00:10:09,120 --> 00:10:12,780
包含在白色测试向量中。

149
00:10:12,840 --> 00:10:16,280
因此，在这里让我们比较真相和预测。

150
00:10:16,430 --> 00:10:16,800
好吧。

151
00:10:16,800 --> 00:10:18,820
快速提醒一下，这是事实。

152
00:10:18,840 --> 00:10:21,340
这些是现实生活中发生的真实结果。

153
00:10:21,390 --> 00:10:28,260
实际上，用户昨天是或不是，这是围绕他们所做的预测

154
00:10:28,250 --> 00:10:32,650
对我们来说，分类或预测是否是每个用户都购买了SUV。

155
00:10:33,030 --> 00:10:34,760
还可以，到目前为止看起来还不错。

156
00:10:34,770 --> 00:10:37,130
我们可以从前10个观察中看到这一点。

157
00:10:37,250 --> 00:10:43,110
我们只有一个错误的预测，那就是九个，其余的都是正确的，所以这不是太正确

158
00:10:43,110 --> 00:10:43,420
坏。

159
00:10:43,420 --> 00:10:45,720
查看最后10个观察结果。

160
00:10:46,560 --> 00:10:47,400
我们来了。

161
00:10:47,610 --> 00:10:53,220
再一次，当我们查看最后10个观察值时，我们可以看到我们只有一个错误的预测

162
00:10:53,610 --> 00:11:00,620
这是第90位用户，因为实际上该用户购买了SUV，因为在这里我们有一个

163
00:11:00,640 --> 00:11:06,120
白色测试向量中的一个是真实结果的向量，抽头向量中的一个。

164
00:11:06,120 --> 00:11:10,590
我们有一个零，这意味着对我们来说是随机的，它预测该用户不会购买

165
00:11:10,590 --> 00:11:15,400
SUV，因此预测不正确。

166
00:11:15,890 --> 00:11:16,210
好。

167
00:11:16,230 --> 00:11:22,500
因此，这只是对预测的快速浏览，但现在非常有趣的是

168
00:11:22,500 --> 00:11:27,300
混淆矩阵，因为我们将仅获得错误预测的数量。

169
00:11:27,540 --> 00:11:30,310
因此，让我们执行此部分。

170
00:11:30,450 --> 00:11:32,880
命令和控制按Enter键执行。

171
00:11:33,120 --> 00:11:39,720
这是我们双击它的混乱矩阵，是的，这就是我告诉你的。

172
00:11:39,750 --> 00:11:42,320
我们只有三加五等于八。

173
00:11:42,360 --> 00:11:43,890
错误的预测。

174
00:11:43,890 --> 00:11:45,980
因此，这是一项很好的工作，请当心。

175
00:11:45,990 --> 00:11:48,100
我们要防止过度拟合。

176
00:11:48,160 --> 00:11:53,880
因此，我们将拭目以待，看看图形结果将如何发生，我们将感受并发现

177
00:11:54,060 --> 00:11:56,160
如果有一些过拟合。

178
00:11:56,160 --> 00:12:01,290
因此，让我们按“确定”，然后我们现在就可以真正了解到这一点。

179
00:12:01,290 --> 00:12:08,460
因此，我将在此处选择此部分，然后按Command和Control并执行，将获得

180
00:12:08,460 --> 00:12:10,890
训练结果。

181
00:12:10,890 --> 00:12:11,700
他们来了。

182
00:12:11,700 --> 00:12:12,250
哇。

183
00:12:12,300 --> 00:12:12,690
好。

184
00:12:12,690 --> 00:12:15,960
因此很明显，这里可能有些过拟合。

185
00:12:16,020 --> 00:12:18,100
让我们看一下。

186
00:12:19,070 --> 00:12:19,420
好。

187
00:12:19,430 --> 00:12:22,910
快速提醒您这里的要点是真实的结果。

188
00:12:23,000 --> 00:12:27,240
也就是说，每个点对应于社交网络的每个用户。

189
00:12:27,360 --> 00:12:35,370
好的用户位于我们的数据集中，这里的区域是红色区域包含的预测

190
00:12:35,440 --> 00:12:42,150
对我们来说，所有随机的用户Tesfaye预测该用户不会购买SUV，

191
00:12:42,150 --> 00:12:47,370
绿色区域包含所有已分类预测使用状态的用户。

192
00:12:47,460 --> 00:12:52,680
好吧，总之，要点是事实，区域是预测和预测边界

193
00:12:52,680 --> 00:12:57,300
是红色区域和绿色区域之间的限制。

194
00:12:57,300 --> 00:13:03,210
正如我们在这里看到的，我们的预测边界与之前的预测边界大不相同。

195
00:13:03,210 --> 00:13:07,200
分类器，它更接近决策树的分类结果。

196
00:13:07,230 --> 00:13:09,330
那是因为这里发生了什么。

197
00:13:09,330 --> 00:13:15,300
所以现在我们要解释幕后发生的事情是对于这里的每个用户

198
00:13:15,300 --> 00:13:20,420
碰巧有10棵树，因为我们选择在森林中有10棵树。

199
00:13:20,460 --> 00:13:22,740
有十棵预言的树木。

200
00:13:22,740 --> 00:13:25,730
如果用户购买是或否的SUV。

201
00:13:25,800 --> 00:13:27,750
这就是生产部分中的部分。

202
00:13:27,750 --> 00:13:34,560
我们有一棵树来预测，现​​在我们有十棵树来预测每个用户，然后每个

203
00:13:34,560 --> 00:13:37,530
10棵树中的10棵预测为是或否。

204
00:13:37,530 --> 00:13:39,180
通常使用冰。

205
00:13:39,240 --> 00:13:41,460
然后是多数票。

206
00:13:41,610 --> 00:13:48,300
因此，随机森林会计算出赞成使用SUV自行车的树木数量，并计算了

207
00:13:48,300 --> 00:13:54,480
投票否决使用它的树木数量不会购买SUV，然后采用已投票的预测

208
00:13:54,480 --> 00:13:55,740
大多数时候。

209
00:13:56,130 --> 00:14:02,370
例如，这是绿色区域，即10棵树中有更多

210
00:14:02,370 --> 00:14:07,700
昨天投票赞成的树木比投票否决的树木更多。

211
00:14:07,770 --> 00:14:14,280
这通常不会买越野车，因为森林投票赞成，因为这是多数票，因此

212
00:14:14,280 --> 00:14:19,420
做出最终预测，以预测该用户购买了SUV。

213
00:14:19,710 --> 00:14:24,810
这就是幕后发生的事情，这就是我告诉您的内容，如果您了解

214
00:14:24,810 --> 00:14:28,050
决策树起作用，那么您就会了解我们的随机性如何起作用。

215
00:14:28,060 --> 00:14:32,650
这实际上很简单，因此我们可以获得所有这些结果。

216
00:14:32,670 --> 00:14:36,940
如您所见，大多数红色用户显然在这里得到了很好的分类。

217
00:14:36,990 --> 00:14:39,220
大多数绿色用户都在这里分类。

218
00:14:39,330 --> 00:14:44,130
即使是绿色Reijer中的单个点也可以被该团队正确分类

219
00:14:44,130 --> 00:14:49,810
树木，因为您可以在此处看到这个红色的点，就是这个没有购买SUV的用户是

220
00:14:49,860 --> 00:14:56,400
显然在这里是绿色区域，分类器的Ranum设法将其捕获并制成红色区域

221
00:14:56,430 --> 00:14:59,000
这里包含其他一些红点。

222
00:14:59,000 --> 00:15:05,340
有些用户没有购买SUV，但是由于这是一个培训套件，因此我们实际上很少

223
00:15:05,340 --> 00:15:06,720
错误的预测。

224
00:15:06,840 --> 00:15:14,010
现在有趣的是，我们检测到的这种过度填充结果是否会损害测试

225
00:15:14,160 --> 00:15:14,920
结果。

226
00:15:15,120 --> 00:15:16,620
因此，让我们现在检查一下。

227
00:15:16,800 --> 00:15:24,270
让我们关闭它，然后在这里选择最后一部分，即测试结果，然后按Command

228
00:15:24,270 --> 00:15:29,040
或按Enter键执行并放大图形。

229
00:15:29,040 --> 00:15:29,440
好吧。

230
00:15:29,430 --> 00:15:31,100
这些是测试结果。

231
00:15:31,230 --> 00:15:31,500
好。

232
00:15:31,500 --> 00:15:35,260
因此，首先我们可以清楚地看到其中有一些。

233
00:15:35,280 --> 00:15:36,270
这是为什么。

234
00:15:36,300 --> 00:15:41,730
因为例如当我们在这里查看该红色区域时，请记住该红色区域的制作方式

235
00:15:41,730 --> 00:15:48,090
可能会在训练中吸引一些红色用户，因为我们的模型学习了如何对未使用的用户进行分类

236
00:15:48,090 --> 00:15:52,720
买越野车，买了昨天在火车上的车。

237
00:15:53,010 --> 00:15:57,960
因此，这里产生了这个红色区域，以吸引那些没有购买SUV的用户。

238
00:15:58,050 --> 00:16:04,260
但是不幸的是，在此测试集中，该区域包含一些购买了SUV的用户，因为这里

239
00:16:04,260 --> 00:16:06,490
我们可以看到这里有这个绿点。

240
00:16:06,780 --> 00:16:11,400
这个也是一样，但是随机的头等舱我很幸运，因为这是一个绿点

241
00:16:12,060 --> 00:16:14,060
它几乎进入了红色区域。

242
00:16:14,070 --> 00:16:16,050
因此，这几乎是一个错误的预测。

243
00:16:16,110 --> 00:16:18,760
因此，第一个客户正在寻找这个客户。

244
00:16:18,960 --> 00:16:25,230
但这就是过度拟合的想法，它使此处的红色区域吸引了培训集中的用户

245
00:16:25,230 --> 00:16:28,340
因为它学会了如何在火车上进行分类。

246
00:16:28,530 --> 00:16:34,770
但是不幸的是，当我们有一些新的观察结果时，该读数根本没有意义，也不应

247
00:16:34,770 --> 00:16:36,020
在这里

248
00:16:36,150 --> 00:16:41,990
因此，所有这些的结论是，我们获得的最佳舱位票价可能是上校SVM舱位

249
00:16:41,990 --> 00:16:48,930
火灾甚至是海军基础级火灾，因为对于这些级火灾，我们有一个平滑的曲线预测

250
00:16:48,930 --> 00:16:55,740
您知道我们在这里吸引了大多数没有购买SUV并将其投放的红色用户

251
00:16:55,740 --> 00:16:58,890
在红色区域并同时切开。

252
00:16:58,920 --> 00:17:04,500
好吧，购买了SUV并将其放置在正确的绿色区域中的其他用户，而我们没有这些

253
00:17:04,860 --> 00:17:06,320
这里是红色区域。

254
00:17:06,540 --> 00:17:09,820
那是过度适合火车然后是推车。

255
00:17:09,870 --> 00:17:15,250
好吧，他们在训练中使用了它，但是在测试中预测新用户方面做得不好

256
00:17:15,250 --> 00:17:19,750
说，因为他们很不幸在这里这个红色区域没有红色用途。

257
00:17:19,800 --> 00:17:21,720
是的，这就是所有这些的结论。

258
00:17:21,720 --> 00:17:26,470
这就是我们可以开始思考如何为我们的问题选择最佳票价的地方。

259
00:17:26,580 --> 00:17:32,130
它总是与您知道的最大正确数之间的精度之争有关

260
00:17:32,130 --> 00:17:34,960
预测并防止过度拟合。

261
00:17:35,280 --> 00:17:37,830
因此，在这里显然我们要防止过拟合。

262
00:17:37,830 --> 00:17:42,750
我们不想在这里得到从您的观察中得出的毫无意义的结果。

263
00:17:42,930 --> 00:17:48,150
因此，所有这些的结论是，如果我们必须首先在所有类别中选择一个分类器

264
00:17:48,150 --> 00:17:53,760
正如我们针对这个特定问题所看到的，我并不是说这是我们应该始终选择的癌症

265
00:17:54,120 --> 00:18:00,000
这是我们应该选择的星期二，我们的业务问题和情况是我们的核心

266
00:18:00,000 --> 00:18:05,970
V.M. 分类器，因为我们有一定的准确性，但有很多错误的预测，

267
00:18:05,970 --> 00:18:12,690
低，但与此同时，红色区域的红色用户正确捕获，而绿色区域的红色用户

268
00:18:12,690 --> 00:18:18,360
绿色区域可能没有我们这里不规则的区域，对新的区域没有任何意义

269
00:18:18,360 --> 00:18:19,610
观察。

270
00:18:19,920 --> 00:18:25,590
当然，SBM上校的癌症要好于线性癌症或归类为逻辑回归的癌症

271
00:18:25,590 --> 00:18:32,280
分类器或V.M. 分类器，因为请记住我们这里有一些无法正确裁剪的用户

272
00:18:32,280 --> 00:18:38,640
线性分类器，因为他们是贪婪的用户，即购买了SUV的用户，他们结束了

273
00:18:38,640 --> 00:18:39,800
在红色区域。

274
00:18:39,900 --> 00:18:43,900
在该地区，阶级理论预测用户不会购买SUV。

275
00:18:44,160 --> 00:18:49,860
上校有这些绿色用户在这里购买了SUV，并在

276
00:18:49,860 --> 00:18:57,180
绿色区域归功于此曲线，它可以包含红色区域中的红色用户，而无需

277
00:18:57,270 --> 00:19:00,690
包括红色区域中的绿色用户。

278
00:19:01,110 --> 00:19:07,110
所以现在我真的要说祝贺，因为您建立了很多分类器

279
00:19:07,110 --> 00:19:12,000
一些非常强大的交叉施肥技术，可以为您的业务做出一些有力的预测

280
00:19:12,000 --> 00:19:13,020
问题。

281
00:19:13,020 --> 00:19:18,330
因此，现在您加载了很多分类器，并且现在有了一些强大的工具

282
00:19:18,330 --> 00:19:20,580
您的科学和技术语言。

283
00:19:20,700 --> 00:19:22,070
恭喜你

284
00:19:22,200 --> 00:19:25,600
非常感谢您观看有关分类的教程。

285
00:19:25,680 --> 00:19:27,150
我现在将对我们执行相同操作。

286
00:19:27,150 --> 00:19:29,110
对于那些也有兴趣的人。

287
00:19:29,250 --> 00:19:35,340
然后，将有一个奖金部分，专门介绍如何评估效果

288
00:19:35,340 --> 00:19:40,550
您不同的分类器，从而改善道德表现。

289
00:19:40,560 --> 00:19:45,360
因此，我期待在下一个有关运行森林分类的​​教程中与您见面

290
00:19:45,360 --> 00:19:51,050
在我们的或下一部分中，我们将专门评估改进的Morell的性能。

291
00:19:51,240 --> 00:19:52,370
所以我很快会再见。

292
00:19:52,390 --> 00:19:54,430
在此之前，请享受机器学习。

