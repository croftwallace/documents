1
00:00:00,300 --> 00:00:02,610
您好，欢迎来到本Python教程。

2
00:00:02,670 --> 00:00:08,040
因此，在上一节中，我们实现了BCA主成分分析的降维

3
00:00:08,040 --> 00:00:13,550
我提醒的技术是特征提取技术，而不是特征选择技术

4
00:00:13,570 --> 00:00:17,000
我们在实施向后淘汰时在第二部分所做的工作。

5
00:00:17,190 --> 00:00:23,420
现在，在本节中，我们将实现LDA，这是另一种技术或特征提取。

6
00:00:23,490 --> 00:00:28,950
因此，这意味着从我们原始的独立变量开始，它们将是相同的

7
00:00:29,280 --> 00:00:35,310
如上一节所述，我们将提取一些新的自变量，因此

8
00:00:35,310 --> 00:00:40,260
以相关方式降低问题数据集的维数。

9
00:00:40,530 --> 00:00:47,520
因此，在上一节中，这种PCA特征提取技术降低了我们问题的维度

10
00:00:47,520 --> 00:00:51,750
通过提取解释最多变体的变量。

11
00:00:51,870 --> 00:00:52,670
现在我们是LDA。

12
00:00:52,710 --> 00:00:53,920
这是完全不同的。

13
00:00:53,940 --> 00:01:00,210
我们正在提取一些新的自变量，这些变量将分离大多数因变量

14
00:01:00,210 --> 00:01:01,000
变量。

15
00:01:01,170 --> 00:01:06,030
因此，自从这次以来，它考虑了因变量的类。

16
00:01:06,180 --> 00:01:11,380
好吧，这意味着它考虑了因变量以继续进行此特征提取技术。

17
00:01:11,490 --> 00:01:19,410
因此，这使得LDA有监督的降维模型，而无监督的PCa则相反

18
00:01:19,590 --> 00:01:24,540
因为我们不考虑因变量可行，而只考虑自变量。

19
00:01:24,540 --> 00:01:30,960
现在，我们正在寻求提取将大多数类和多个类分开的自变量

20
00:01:31,050 --> 00:01:37,170
是与因变量相关的信息，因此它与因变量相关。

21
00:01:37,170 --> 00:01:39,260
因此LDA是一种特征提取模型。

22
00:01:39,330 --> 00:01:41,380
但是这次是监督。

23
00:01:41,520 --> 00:01:45,930
因此，现在在该部分中，我们将处理相同的业务问题，我们将看到

24
00:01:45,930 --> 00:01:51,790
如果我们设法击败使用PCa获得的准确度，即97％。

25
00:01:51,810 --> 00:01:56,580
换句话说，我们将看到是否获得100％的完美精度。

26
00:01:56,670 --> 00:02:00,680
那就是我们将看到是否没有错误的预测。

27
00:02:00,690 --> 00:02:07,920
所以现在就开始实施LPA，实际上这将非常容易，因为

28
00:02:08,310 --> 00:02:15,360
基本上，我们要做的就是采用我们在上一节中编写的PCA代码，因为

29
00:02:15,360 --> 00:02:19,390
将会看到我们几乎没有什么可更改的。

30
00:02:19,530 --> 00:02:25,350
因此，我将其粘贴到此处，您将看到我们将一一执行不同的部分

31
00:02:25,650 --> 00:02:27,510
几乎什么都没改变。

32
00:02:27,510 --> 00:02:34,830
所以首先是数据预处理阶段嗯，这将与PCa中的相同，因为我们

33
00:02:34,830 --> 00:02:38,800
我们只需要一个训练集和一个测试集，就可以对其进行特征筛选。

34
00:02:38,850 --> 00:02:44,220
这正是我们在这四个代码块中所做的，因此我们已经可以执行

35
00:02:44,220 --> 00:02:46,240
没有任何改变。

36
00:02:46,260 --> 00:02:51,910
因此，在进行操作之前，请不要忘记确保我们拥有正确的文件夹作为工作目录。

37
00:02:51,930 --> 00:02:56,300
因此，让我们进入文件夹的第九部分。

38
00:02:56,430 --> 00:03:02,670
现在，在第44节中，我们将判别分析LDA线性化。

39
00:03:02,720 --> 00:03:03,270
好吧。

40
00:03:03,270 --> 00:03:05,710
现在，请确保在我们提交文件时有风。

41
00:03:05,730 --> 00:03:07,960
如果是这样的话，您就准备好了。

42
00:03:07,980 --> 00:03:15,790
因此，正如我刚才所说，我们准备选择这四个部分，而无需进行任何更改。

43
00:03:15,990 --> 00:03:22,410
因此，让我们开始，基本上让我们继续进行第一个数据预处理工作。

44
00:03:22,410 --> 00:03:23,270
开始了。

45
00:03:23,390 --> 00:03:24,970
全部嵌套在一秒钟。

46
00:03:25,110 --> 00:03:30,870
现在，如果我能避免使用所有资源管理器，那么我们已经有了所有

47
00:03:31,170 --> 00:03:36,870
有关不同风的化学信息以及包含不同风的因变量

48
00:03:36,870 --> 00:03:42,690
集群使该一位企业主可以识别的客户群。

49
00:03:42,690 --> 00:03:43,500
那么完美。

50
00:03:43,520 --> 00:03:44,500
那没问题。

51
00:03:44,640 --> 00:03:55,830
而且我们的训练集可以按比例缩放，但到目前为止包含13个相同的独立变量

52
00:03:55,950 --> 00:03:58,360
就像可扩展的数据集中一样。

53
00:03:58,380 --> 00:04:04,800
当然，我们将获得的是提取的自变量的新选择

54
00:04:05,020 --> 00:04:10,530
这次将要分离的大多数独立类别的自变量

55
00:04:10,530 --> 00:04:12,810
我们在这里看不到的变量。

56
00:04:12,810 --> 00:04:17,120
好吧，这对于X火车和X测试是相同的。

57
00:04:17,160 --> 00:04:20,650
我们的床上有13个乱码，它们也按比例缩放。

58
00:04:20,880 --> 00:04:22,160
所以按OK。

59
00:04:22,290 --> 00:04:26,280
对于因变量而言，这是相同的，我们有光荣的价值观的深端，那就是

60
00:04:26,280 --> 00:04:32,270
客户1 2和3的不同细分针对每个观察，取决于每个观察

61
00:04:32,280 --> 00:04:33,190
信息。

62
00:04:33,380 --> 00:04:34,580
这就是训练集。

63
00:04:34,620 --> 00:04:37,040
我们也有相同的测试。

64
00:04:37,050 --> 00:04:38,010
好吧。

65
00:04:38,010 --> 00:04:40,860
这样数据处理阶段就完成了。

66
00:04:40,860 --> 00:04:48,960
现在让我们进入下一阶段，该阶段不是应用PCA，而是这次是L.D。 线性或判别式

67
00:04:49,020 --> 00:04:50,280
分析。

68
00:04:50,280 --> 00:04:50,580
好吧。

69
00:04:50,580 --> 00:04:56,310
这与PCa相同，我们需要在日期预处理阶段之后立即申请FDA

70
00:04:56,310 --> 00:04:57,640
不久之后就令人恐惧。

71
00:04:57,870 --> 00:05:03,440
而且，在拟合分类模型之前，不必一定要进行逻辑回归

72
00:05:03,450 --> 00:05:06,820
它可以是V.M. 或决策树分类。

73
00:05:06,820 --> 00:05:13,140
但是无论如何，您都需要在将分类模型拟合到训练集之前应用LDA。

74
00:05:13,170 --> 00:05:17,600
好的，现在让我们来处理本节，让我们应用LGF。

75
00:05:17,820 --> 00:05:20,980
因此，首先没有解释方差的问题。

76
00:05:21,000 --> 00:05:27,600
因此，我们在这里摆脱了这一行，因为我们不是在寻找解释

77
00:05:27,600 --> 00:05:28,690
变化最大。

78
00:05:28,720 --> 00:05:33,330
现在，我们正在寻找能够将大多数依赖类分开的自变量

79
00:05:33,330 --> 00:05:33,750
变量。

80
00:05:33,750 --> 00:05:39,300
这是我们的三类客户群，第一客户群，然后转移到最终客户群

81
00:05:39,300 --> 00:05:40,200
第三。

82
00:05:40,200 --> 00:05:43,230
好的，现在让我们导入正确的类。

83
00:05:43,230 --> 00:05:49,740
它不再是PCa，实际上也不再是从定标器和位置获取的。

84
00:05:49,760 --> 00:05:57,920
现在，这是从S-K那里得知，判别式强调分析。

85
00:05:58,160 --> 00:05:58,680
对。

86
00:05:58,820 --> 00:06:03,710
然后据此作为Kaylor和判别分析库来猜测我们需要导入什么。

87
00:06:03,830 --> 00:06:10,990
坏消息不是LDA，而是Linnea是判别分析。

88
00:06:11,000 --> 00:06:11,660
开始了。

89
00:06:11,870 --> 00:06:17,420
所以在L.D. 实际上，但它只是变成了我们的判别分析。

90
00:06:17,520 --> 00:06:18,910
没关系，我们会没事的。

91
00:06:19,010 --> 00:06:25,760
现在，我们需要做与之前相同的操作，即创建该Lenya的判别分析对象

92
00:06:25,760 --> 00:06:26,410
类。

93
00:06:26,470 --> 00:06:29,780
因此，我们将其称为LDA。

94
00:06:29,990 --> 00:06:35,490
因此，此LDA对象将成为此Lydney判别分析类的新对象。

95
00:06:35,720 --> 00:06:47,320
为了使事情更简单，我们将在此处添加为LDA并用LDK替换PCA，以便现在

96
00:06:47,360 --> 00:06:55,020
在我们的判别分析类中创建Lynnie的对象，我们刚刚将LDA重命名为快捷方式。

97
00:06:55,020 --> 00:06:55,590
好吧。

98
00:06:55,850 --> 00:06:57,440
然后在这个班上。

99
00:06:57,560 --> 00:07:00,540
好吧，我们有相同数量的组件。

100
00:07:00,560 --> 00:07:05,840
这就是我们这次将他们称为Lynnie的区别次数，而不是本金。

101
00:07:05,840 --> 00:07:07,670
组件和PCA。

102
00:07:07,760 --> 00:07:13,340
如此众多的组件，这意味着我们要对Lynnie加以区分。

103
00:07:13,340 --> 00:07:18,770
也就是说，将其提取到bin中，并且将要分隔最多类的变量是依赖的

104
00:07:18,770 --> 00:07:19,520
变量。

105
00:07:19,820 --> 00:07:25,940
因此，这次实际上我们不需要构建解释了Varians的向量或任何其他种类的向量

106
00:07:26,180 --> 00:07:28,410
像某种类的可观察性向量。

107
00:07:28,490 --> 00:07:31,040
我们将直接采用等于的最终组件。

108
00:07:31,220 --> 00:07:36,500
我们已经看到如果我们能取得一些好的结果，因为我们的目标是实际

109
00:07:36,500 --> 00:07:42,200
得到一些独立变量，使我们可以直观地看到训练结果和测试结果。

110
00:07:42,350 --> 00:07:45,870
好吧，我们知道我们正在寻找学习判别方法。

111
00:07:45,920 --> 00:07:50,450
因此，我们要做的是直接选择保留判别式，然后我们将看到

112
00:07:50,450 --> 00:07:50,910
我们会得到。

113
00:07:50,930 --> 00:07:54,640
大多数情况下，我们将看到它与我们选择的PCA相比。

114
00:07:54,790 --> 00:07:56,810
确实有两个组成部分。

115
00:07:56,840 --> 00:08:02,120
好的，然后我们在这两行中更改了测试集中的训练集

116
00:08:02,120 --> 00:08:08,430
以我们的LDA对象为例，我们将对其进行更改，然后使用转移方法使其适合该对象

117
00:08:08,430 --> 00:08:13,520
在训练集中并同时进行变换以在判别式中提取工具，以便

118
00:08:13,610 --> 00:08:16,940
Extra成为两个新功能的矩阵。

119
00:08:16,940 --> 00:08:19,550
然后，我们在测试集上使用转移方法。

120
00:08:19,670 --> 00:08:25,820
这样，相同的访问就成为包含两个相同的Lynnie或判别式的要素矩阵。

121
00:08:25,820 --> 00:08:28,430
但是，我们需要在这里更改两件事。

122
00:08:28,430 --> 00:08:33,620
当然，首先我们需要用LDA代替PCA。

123
00:08:33,940 --> 00:08:36,030
因此，这显然是LDA。

124
00:08:36,170 --> 00:08:38,050
然后我们需要在这里再做一件事。

125
00:08:38,060 --> 00:08:39,400
你能猜出这是什么。

126
00:08:39,680 --> 00:08:47,660
嗯，这与LDA是监督模型这一事实有关，因此，当我们适合时，我们可以

127
00:08:47,660 --> 00:08:49,660
训练设置特征矩阵。

128
00:08:49,670 --> 00:08:53,680
极值嘛，因为我们还需要有价值的信息。

129
00:08:53,720 --> 00:08:58,840
为什么要训练，因为我们正在寻求将依附的Roybal的大多数不同类别分开。

130
00:08:58,970 --> 00:09:05,370
好吧，我们还需要包括为什么训练这种Fit传递方法。

131
00:09:06,200 --> 00:09:12,080
因此，现在我们可以清楚地看到区别在于转移方法的特定输入

132
00:09:12,080 --> 00:09:19,340
在我们包括因变量y火车的监督模型和无监督模型之间

133
00:09:19,430 --> 00:09:22,650
我们只包括特征X射线矩阵。

134
00:09:22,700 --> 00:09:29,620
因此，这是Pca的两种非监督模型与LDA的监督模型之间的关键区别。

135
00:09:29,780 --> 00:09:34,680
然后，在传输方法中，您认为我们需要包括测试良好的原因。

136
00:09:34,730 --> 00:09:42,170
不，因为为什么这里只使用火车来将LDA对象拟合到训练集中，而我的意思是

137
00:09:42,440 --> 00:09:47,810
了解培训集的结构和相关信息以实现我们的目标

138
00:09:47,810 --> 00:09:52,510
提取将大多数类和这些必需信息分开的自变量

139
00:09:52,520 --> 00:09:54,170
包含在我的火车中。

140
00:09:54,410 --> 00:10:00,550
然后，因为我们只是使用适合的对象来转换访问权限，所以我们不需要再次适合

141
00:10:00,890 --> 00:10:06,770
因此，我们不需要测试原因的信息，因此在这里很完美，我们只需要进行X次测试

142
00:10:06,920 --> 00:10:12,520
因此，我们准备将LDA应用于我们的数据集。

143
00:10:12,710 --> 00:10:18,220
因此，我们在这里选择这四行并执行。

144
00:10:18,350 --> 00:10:19,130
开始了。

145
00:10:19,130 --> 00:10:19,910
完善。

146
00:10:19,990 --> 00:10:21,690
LDA正确应用。

147
00:10:21,830 --> 00:10:27,580
因此，现在请记住额外的内容。下一个测试包含13个独立变量。

148
00:10:27,770 --> 00:10:35,120
现在我们已经看到它们包含两个自变量，它们是新提取的Lydney

149
00:10:35,120 --> 00:10:40,420
是本L.D.中五角大楼表格中的判别式 特征提取技术。

150
00:10:40,520 --> 00:10:43,870
当然，这些新的自变量也可以缩放。

151
00:10:44,270 --> 00:10:46,380
所以丰胸好吧。

152
00:10:46,610 --> 00:10:50,710
并且存在相同的两个独立变量。

153
00:10:50,720 --> 00:10:51,860
好吧。

154
00:10:52,190 --> 00:10:56,600
因此，现在我们准备将逻辑回归拟合到训练集。

155
00:10:56,600 --> 00:11:01,670
我们将保留此逻辑回归模型，因为我们有兴趣比较效果

156
00:11:01,670 --> 00:11:05,070
我们用PCa获得的性能结果。

157
00:11:05,080 --> 00:11:07,010
我们即将获得FDA认证。

158
00:11:07,130 --> 00:11:13,260
因此，我们将保留此逻辑回归模型，并希望我们可以通过实施

159
00:11:13,260 --> 00:11:17,160
准确度为100％，没有错误的预测。

160
00:11:17,180 --> 00:11:23,450
顺便说一句，因为LDA希望将大多数类别分开，所以我们希望这些类别

161
00:11:23,450 --> 00:11:29,420
将完全分开，因此如果是这种情况，我们应该获得100％的精度

162
00:11:29,690 --> 00:11:32,050
取决于数据集的结构。

163
00:11:32,090 --> 00:11:34,200
但是我们应该非常有信心。

164
00:11:34,200 --> 00:11:37,810
好的，现在让我们就接受培训吧。

165
00:11:37,880 --> 00:11:39,590
我们在这里没有任何更改。

166
00:11:39,590 --> 00:11:41,070
一切都已经准备好了。

167
00:11:41,210 --> 00:11:46,680
因此，我将选择该部分并执行Perfect的部分。

168
00:11:46,880 --> 00:11:53,270
现在我们的逻辑回归模型是基于训练说的，它仅由两个独立的

169
00:11:53,270 --> 00:11:55,740
Linnea是我们的判别变量。

170
00:11:55,820 --> 00:11:56,500
大。

171
00:11:56,510 --> 00:12:00,700
现在让我们进入下一部分，进行设定预测。

172
00:12:00,710 --> 00:12:02,810
同样，我们在这里没有任何更改。

173
00:12:02,810 --> 00:12:04,700
那就是模板的美。

174
00:12:04,940 --> 00:12:12,500
按下Enter键，即可执行执行预测的Wybert向量，我们可以单击它，在这里我们拥有所有

175
00:12:12,650 --> 00:12:13,550
预测。

176
00:12:13,670 --> 00:12:17,760
因此，预计第一个客户属于第一客户群。

177
00:12:17,810 --> 00:12:21,800
测试集的第二个预计将到达第三客户群。

178
00:12:21,890 --> 00:12:25,760
预计属于客户的测试中的第三项表示我们是第二名。

179
00:12:25,970 --> 00:12:34,250
因此，让我们现在看看混淆指标，如果我们没有得到正确的预测，那将非常令人兴奋

180
00:12:34,250 --> 00:12:38,720
看看，因为正是在这个正确的时刻，我们才知道我们是否击败了PCA。

181
00:12:38,720 --> 00:12:41,910
因此，一切都准备就绪，可以执行了。

182
00:12:42,080 --> 00:12:44,210
在这里按Enter键。

183
00:12:44,300 --> 00:12:48,340
准备好混淆矩阵，让我们看一下，然后开始。

184
00:12:48,340 --> 00:12:50,390
没有错误的预测。

185
00:12:50,540 --> 00:12:56,780
如您所见，这里这里都是零，这里都是零，都是正确的预测

186
00:12:56,780 --> 00:12:57,540
在这里。

187
00:12:57,710 --> 00:13:03,900
实际上，我们的测试包含36个观察值，这里有14个加16。

188
00:13:03,920 --> 00:13:07,700
三十六三十六正确的预测。

189
00:13:07,700 --> 00:13:09,900
我们甚至不必计算精度。

190
00:13:09,950 --> 00:13:13,010
我们获得100％的准确性。

191
00:13:13,010 --> 00:13:13,610
太棒了。

192
00:13:13,610 --> 00:13:20,840
实际上，这并不是完全出乎意料的，因为这种完美的100％精度来自于

193
00:13:20,840 --> 00:13:27,440
我们班级的完美Separ能力和Tessa结果的完美可分割性

194
00:13:27,440 --> 00:13:34,670
确实提取了最多的独立变量的LDK的目标是3类0 1

195
00:13:34,670 --> 00:13:37,240
和2就是客户第一，第二和第三。

196
00:13:37,550 --> 00:13:38,650
好吧，太完美了。

197
00:13:38,660 --> 00:13:39,800
让我们按确定。

198
00:13:40,100 --> 00:13:45,130
现在，我们可以看看产生测试结果的培训。

199
00:13:45,170 --> 00:13:51,260
但是，我们将获得完全分开的预测区域以及关于谁会好心的预测

200
00:13:51,260 --> 00:13:55,440
相同的SPCA可以更好地分离类别。

201
00:13:55,490 --> 00:13:56,420
让我们来看一下。

202
00:13:56,510 --> 00:13:58,710
实际上，我们可以在此处进行一些小的更改。

203
00:13:58,720 --> 00:14:04,400
当然可以更改X标签，Waialae会撒尿您想要的标签并用L.D代替。

204
00:14:04,400 --> 00:14:07,600
一个NLD，让我们快速地做到这一点。

205
00:14:07,610 --> 00:14:14,540
这里还有其他人要告诉你一两个。

206
00:14:14,570 --> 00:14:15,690
现在我们准备好了。

207
00:14:15,710 --> 00:14:20,480
我们没有其他要更改的东西，我们有对应于这三个类别的三种颜色，

208
00:14:20,480 --> 00:14:29,140
因此，我们都准备在这里选择整个部分并可视化火车组结果。

209
00:14:30,170 --> 00:14:36,170
好的，就是训练产生的结果，因为我们可以看到预测边界略有不同

210
00:14:36,170 --> 00:14:41,360
从上一节中使用pca获得的预测边界中，因为我们可以清楚地看到

211
00:14:41,510 --> 00:14:45,560
组成此预测边界的每条直线是分开的。

212
00:14:45,560 --> 00:14:50,780
好吧，这里对应于客户群一号的红点到这里的绿点是

213
00:14:50,780 --> 00:14:53,000
对应于相同的客户和第二名的客户。

214
00:14:53,000 --> 00:14:58,460
确实，我们可以看到，如果采用最接近该预测边界直线的红点，并且

215
00:14:58,460 --> 00:15:03,010
假设我们是否将最接近的绿点取到相同的分区边界直线。

216
00:15:03,170 --> 00:15:09,320
好吧，这些红点到这条直线的距离与这些最接近的点大约相等

217
00:15:09,320 --> 00:15:10,150
点在这里。

218
00:15:10,160 --> 00:15:12,880
这就是我对分类能力的定义。

219
00:15:13,010 --> 00:15:18,830
但是如果我们看一下蓝色区域和绿色区域对应的预测边界

220
00:15:18,830 --> 00:15:21,290
分别给三号和二号客户

221
00:15:21,530 --> 00:15:26,660
好了，当我们看这些Close的绿点时，我们可以看到这种聚类变化不太明显

222
00:15:26,660 --> 00:15:28,360
在这里，这是Close的蓝点。

223
00:15:28,490 --> 00:15:34,100
嗯，这实际上只是由于以下事实：LDA模型将这些点视为离群值

224
00:15:34,250 --> 00:15:40,880
因此，LDA道德在此反驳了这些观点，并使它们与这些观点同样遥不可及

225
00:15:40,880 --> 00:15:43,480
从此预测边界指向此处。

226
00:15:43,490 --> 00:15:49,190
因此，如果您研究红色玻璃和蓝色玻璃之间的类可操作性，那将是有趣且相同的

227
00:15:49,190 --> 00:15:49,960
类。

228
00:15:50,000 --> 00:15:55,280
好吧，我们可以看到，这里的预测边界直线实际上是等距的

229
00:15:55,610 --> 00:16:00,020
从最接近的蓝色点和最接近的红色点。

230
00:16:00,020 --> 00:16:06,320
因此，基本上，如果我们在这里没有很好地考虑离群值，则该洛杉矶模型在分离方面做得很好

231
00:16:06,440 --> 00:16:07,600
这三个班。

232
00:16:07,730 --> 00:16:08,980
很好。

233
00:16:08,990 --> 00:16:11,840
现在让我们看一下测试集的结果。

234
00:16:11,840 --> 00:16:14,120
因此，这里没有任何更改。

235
00:16:14,120 --> 00:16:20,390
我们准备通过执行此操作来可视化测试结果。

236
00:16:20,410 --> 00:16:21,840
这是测试结果。

237
00:16:22,000 --> 00:16:27,280
因此，再次将这三个类别很好地分开，我们可以清楚地看到我们获得的结果

238
00:16:27,340 --> 00:16:33,370
在我们的混淆矩阵中，这是0个错误的预测，因为实际上所有红色

239
00:16:33,370 --> 00:16:38,980
葡萄酒在红色区域，所有绿色在绿色区域，所有蓝色在

240
00:16:38,980 --> 00:16:40,150
蓝色区域。

241
00:16:40,150 --> 00:16:41,650
太完美了。

242
00:16:41,650 --> 00:16:47,500
每个人都属于其正确的客户群，因此，这个企业主可以感觉很漂亮

243
00:16:47,500 --> 00:16:53,910
有信心预测每种新酒应细分给哪些客户。

244
00:16:54,190 --> 00:16:59,440
而且他不仅可以很有信心地向合适的顾客推荐新酒，而且还可以

245
00:16:59,440 --> 00:17:04,860
得益于特征提取技术，该技术使他可以在两个维度上可视化结果。

246
00:17:04,860 --> 00:17:09,390
感谢是判别式中的第二个新自变量。

247
00:17:09,530 --> 00:17:15,660
现在，这位葡萄酒企业主可以清楚地说明其不同的客户群，

248
00:17:15,670 --> 00:17:18,530
在每个客户群中，都有不同的葡萄酒。

249
00:17:18,670 --> 00:17:21,480
因此最终可以很方便。

250
00:17:21,820 --> 00:17:23,200
好吧，太完美了。

251
00:17:23,200 --> 00:17:25,810
我们设法建立了一个伟大的法学博士 明天。

252
00:17:25,840 --> 00:17:30,700
因此，我们将在此重点上做完介绍，因此，我们将继续本课程的下一部分

253
00:17:30,700 --> 00:17:33,470
将成为另一种特征提取技术。

254
00:17:33,550 --> 00:17:36,570
但这一次适用于非线性问题。

255
00:17:36,580 --> 00:17:43,900
因此，由于此问题显然是线性问题，因为我们设法非常成功地应用了Lynnie

256
00:17:43,900 --> 00:17:49,980
我们的道德SPCA，现在他们以及Logistic回归模型（也是Lynnie）都很好

257
00:17:49,990 --> 00:17:55,250
它与应用无关，并且与我们将来在此数据集上的提取模型无关。

258
00:17:55,480 --> 00:17:59,640
因此，我们将处理另一个当然是非线性的数据集。

259
00:17:59,950 --> 00:18:05,790
我们将看到的下一个新的特征提取技术将是内核PCA。

260
00:18:05,830 --> 00:18:08,310
因此，我期待在下一节中开始。

261
00:18:08,320 --> 00:18:10,090
直到那时，享受机器学习。

