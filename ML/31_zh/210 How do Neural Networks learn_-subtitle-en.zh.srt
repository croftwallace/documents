1
00:00:00,840 --> 00:00:05,380
您好，欢迎回到本课程和深度学习，因为我们已经看到了您自己的网络在运行

2
00:00:05,440 --> 00:00:08,280
现在该是我们找出他们如何学习的时候了。

3
00:00:08,470 --> 00:00:10,480
因此，让我们开始吧。

4
00:00:10,510 --> 00:00:16,100
它们是使程序执行您想要执行的操作的两种根本不同的方法。

5
00:00:16,240 --> 00:00:24,610
一种是硬编码，您实际上可以告诉程序的特定规则以及您得到什么结果

6
00:00:24,610 --> 00:00:25,120
想。

7
00:00:25,120 --> 00:00:30,940
而您只是在整个过程中进行了指导，并说明了

8
00:00:30,940 --> 00:00:33,130
程序必须处理。

9
00:00:33,310 --> 00:00:41,320
另一方面，您具有神经网络，您可以在其中创建程序的功能

10
00:00:41,800 --> 00:00:43,530
了解自己需要做什么。

11
00:00:43,530 --> 00:00:50,080
因此，您基本上会在提供输入的地方创建这个神经网络，然后告诉您想要什么作为输出

12
00:00:50,110 --> 00:00:53,050
然后让它自己解决所有问题。

13
00:00:53,350 --> 00:00:59,890
两种根本不同的方法，在我们经历这些时要牢记

14
00:00:59,890 --> 00:01:00,850
教程。

15
00:01:00,850 --> 00:01:06,180
我们的目标是创建一个可以自己学习的网络。

16
00:01:06,220 --> 00:01:14,570
我们将避免尝试输入规则，而我现在可以给您的一个很好的例子是

17
00:01:14,680 --> 00:01:18,680
将在课程中走得更远，但这只是一个非常直观的示例。

18
00:01:18,700 --> 00:01:25,690
在左侧所示的过程中，如何区分左侧的猫狗毛皮

19
00:01:25,690 --> 00:01:33,250
你会编程像猫的耳朵这样的东西

20
00:01:33,250 --> 00:01:39,530
这种类型的脸型看起来很像鼻子，这种颜色的脸看起来很像

21
00:01:39,530 --> 00:01:45,310
描述所有这些事情，你会遇到这样的情况，如耳朵是否尖尖，而不是猫耳朵

22
00:01:45,310 --> 00:01:49,600
正在倾斜，可能还有狗等等。

23
00:01:49,600 --> 00:01:55,090
另一方面，对于神经网络，您只需对神经网络进行编码，即可对架构进行编码，

24
00:01:55,090 --> 00:02:01,030
然后将神经网络指向包含所有这些猫狗的文件夹，其中包含猫和狗的图像

25
00:02:01,030 --> 00:02:06,580
已经分类，您告诉它，好吧，我得到了您，我得到了一些猫和狗的图片

26
00:02:06,880 --> 00:02:08,860
去学习猫是什么。

27
00:02:08,860 --> 00:02:10,560
去学习狗是什么。

28
00:02:10,600 --> 00:02:16,000
而且神经网络将自行了解其需要了解的所有内容，然后进一步

29
00:02:16,000 --> 00:02:20,950
一旦训练好，当您给它赋予新的猫或狗形象时，它将能够理解

30
00:02:20,950 --> 00:02:21,600
这是啥。

31
00:02:21,610 --> 00:02:25,600
因此，这是两种根本不同的方法。

32
00:02:25,690 --> 00:02:31,090
今天，我们将慢慢开始了解第二种方法的工作原理。

33
00:02:31,090 --> 00:02:31,530
好吧。

34
00:02:31,570 --> 00:02:33,340
因此，让我们直接开始吧。

35
00:02:33,400 --> 00:02:39,880
在这里，我们有一个非常基本的神经网络，其中一层称为单层前馈神经

36
00:02:39,880 --> 00:02:42,760
网络，也称为感知。

37
00:02:42,760 --> 00:02:47,380
现在，在进行下一步之前，我们需要调整的一件事就是输出值。

38
00:02:47,380 --> 00:02:49,320
现在，您可以看到它只是一个Y。

39
00:02:49,330 --> 00:02:51,160
我们需要戴上帽子。

40
00:02:51,190 --> 00:02:56,500
原因通常是y代表实际值，这就是我们将要使用的值。

41
00:02:56,500 --> 00:03:03,700
那么为什么会变成实际值呢？我们看到实际输出值是预测值

42
00:03:03,700 --> 00:03:05,890
通过神经网络的算法。

43
00:03:05,890 --> 00:03:09,220
为什么是输出值。

44
00:03:09,220 --> 00:03:11,500
基本上，这就是输出值的面额。

45
00:03:11,740 --> 00:03:20,020
弗兰克·罗森布拉特（Frank Rosenblat）于1957年首次发明了这种观念，他的整个想法是

46
00:03:20,170 --> 00:03:25,010
可以真正学习和自我调整的东西。

47
00:03:25,240 --> 00:03:28,010
这就是我们现在要看的。

48
00:03:28,030 --> 00:03:30,230
因此，我们有了戒律。

49
00:03:30,250 --> 00:03:32,070
让我们看看我们的感知如何学习。

50
00:03:32,080 --> 00:03:39,130
假设我们有一些输入值已经提供给感知，或者基本上

51
00:03:39,130 --> 00:03:40,210
我们自己的网络。

52
00:03:40,330 --> 00:03:44,190
然后应用激活功能。

53
00:03:44,200 --> 00:03:49,210
我们有一个输出，现在我们将输出绘制在图表上。

54
00:03:49,210 --> 00:03:51,830
这就是我们的输出。

55
00:03:51,830 --> 00:03:57,520
现在我们需要做的是为了能够学习我们需要将输出值与实际值进行比较

56
00:03:57,520 --> 00:04:01,310
我们希望神经网络正确的价值。

57
00:04:01,600 --> 00:04:04,520
那就是值y。

58
00:04:04,810 --> 00:04:08,230
因此，如果我们将其放在此处，您会发现有些不同。

59
00:04:08,330 --> 00:04:13,510
现在我们要计算一个称为成本函数的函数，该函数被计算为差额的一半

60
00:04:13,510 --> 00:04:17,200
实际值与输出值之间的平方差的平方。

61
00:04:17,200 --> 00:04:20,500
现在，有很多方法可以实现类功能。

62
00:04:20,500 --> 00:04:23,300
您可以使用许多不同的成本函数。

63
00:04:23,320 --> 00:04:30,280
这可能是最常用的调用函数，以及为什么正是该函数

64
00:04:30,280 --> 00:04:34,900
当我们谈论渐变体面时，我们将使用找出更深层次的信息，但现在我们只是

65
00:04:34,900 --> 00:04:39,830
同意这是成本函数，并且基本上成本函数告诉我们的是

66
00:04:40,420 --> 00:04:44,240
您的预测中有什么错误。

67
00:04:44,290 --> 00:04:50,770
我们的目标是最小化成本函数，因为成本函数越低，

68
00:04:50,790 --> 00:04:51,780
是y。

69
00:04:52,150 --> 00:04:54,430
好的，只有我们同意这一点，我们才能继续。

70
00:04:54,430 --> 00:05:00,760
所以基本上从这里发生的事情是有一个成本函数，从这里发生的事情现在是

71
00:05:00,760 --> 00:05:08,950
现在进行比较后，我们将把这些信息反馈到神经网络中。

72
00:05:08,980 --> 00:05:14,170
所以我们去了，那里的信息又回到了神经网络，并且权重很大

73
00:05:14,200 --> 00:05:15,630
权重会更新。

74
00:05:15,700 --> 00:05:20,880
基本上，在这个非常简单的神经网络中，我们唯一可以控制的就是权重

75
00:05:20,900 --> 00:05:23,490
w 1 W2一直到W ..

76
00:05:23,980 --> 00:05:29,370
我们的目标是最小化成本函数，因此我们所能做的就是更新权重。

77
00:05:29,500 --> 00:05:33,690
因此，我们更新了权重并对其进行了一些调整。

78
00:05:33,940 --> 00:05:39,600
我们将如何准确地找到下降点，但现在我们同意拥有权重，然后

79
00:05:39,600 --> 00:05:40,320
我们继续如此。

80
00:05:40,320 --> 00:05:48,870
但是在这里，我放了这张数据的屏幕截图，目的只是为了清楚地说明一点

81
00:05:48,930 --> 00:05:53,990
在整个实验过程中，我们现在正在做的所有事情都只涉及一个角色。

82
00:05:54,000 --> 00:06:00,330
因此，我们要处理的是一行的数据集，例如，我们要处理的是

83
00:06:00,330 --> 00:06:05,720
只要您像我们预测的变量一样研究它，就可以了。

84
00:06:06,180 --> 00:06:08,230
您要参加考试的结果是什么？

85
00:06:08,430 --> 00:06:13,200
我们拥有的因变量是您学习了多少小时

86
00:06:13,200 --> 00:06:15,430
你睡了吗，测验又得到了什么。

87
00:06:15,460 --> 00:06:19,880
在学期中，所以在学期中有一个测验，您达到那里的百分比。

88
00:06:19,880 --> 00:06:26,100
因此，基于这些变量，我们试图预测您的考试分数以及

89
00:06:26,100 --> 00:06:28,010
实际值的93％。

90
00:06:28,010 --> 00:06:29,020
这就是为什么。

91
00:06:29,560 --> 00:06:30,460
所以。

92
00:06:30,660 --> 00:06:36,720
所以我们现在第二次将这三个值再次输入到神经网络中，然后我们将

93
00:06:36,720 --> 00:06:38,980
将结果与白色进行比较。

94
00:06:39,150 --> 00:06:40,690
因此，让我们看看它是如何工作的。

95
00:06:40,800 --> 00:06:43,710
我们将这些值输入神经网络。

96
00:06:43,830 --> 00:06:50,160
一切都进行了调整，权重也得到了调整，这样您就可以看到再次输入值

97
00:06:50,190 --> 00:06:55,480
同样，这里的要点是我们要喂入同一个球，所以我们只尝试滚一圈

98
00:06:55,480 --> 00:06:56,370
我们正在单排训练吗？

99
00:06:56,370 --> 00:06:59,580
这是因为这只是一个非常简单的基本示例。

100
00:06:59,640 --> 00:07:01,610
然后，我们将看到有道德时会发生什么。

101
00:07:01,800 --> 00:07:06,180
因此，我们再次调整了跨功能的行。

102
00:07:06,180 --> 00:07:10,520
如您所见，一切再次沿着这些思路发生。

103
00:07:10,530 --> 00:07:15,030
因此，正如您所说，每次我们调整白帽子都是因为调整了重量。

104
00:07:15,030 --> 00:07:20,550
我所拥有的只是改变衣服的功能，从而再次改变了整体外观，因此我们为之着迷。

105
00:07:20,550 --> 00:07:22,840
为什么要换衣服功能正在改变。

106
00:07:22,920 --> 00:07:27,020
我们将信息反馈给权重，以便权重得到再次调整。

107
00:07:27,030 --> 00:07:31,850
每次调整完所有内容后，我们都会输入相同的值。

108
00:07:31,860 --> 00:07:33,920
还有更多时间输入。

109
00:07:34,020 --> 00:07:34,990
好。

110
00:07:35,730 --> 00:07:40,720
还有一次，我们调整了信息中的感受方式。

111
00:07:40,830 --> 00:07:41,370
然后我们走了。

112
00:07:41,370 --> 00:07:45,990
所以现在这一次白帽子等于y跨函数0。

113
00:07:46,020 --> 00:07:48,410
通常，您不会获得等于零的成本函数。

114
00:07:48,420 --> 00:07:50,720
但这是一个非常简单的例子。

115
00:07:50,820 --> 00:07:57,480
因此，希望每次我们在完全相同的行中输入时，所有这些都有意义，因为在这种情况下

116
00:07:57,480 --> 00:08:01,370
我们只是将这一行处理到我们的神经网络中。

117
00:08:01,400 --> 00:08:06,990
那么权重得到的值就得到有效的供给，通过激活函数的应用

118
00:08:06,990 --> 00:08:12,320
我们得到与Y相比的y，然后看到成本函数如何变化。

119
00:08:12,430 --> 00:08:16,500
反馈和提要，这些信息将Bakker的信息提供给您自己的网络，然后调整权重

120
00:08:16,500 --> 00:08:17,470
再次。

121
00:08:17,850 --> 00:08:21,410
然后，我们再次使用相同的确切行重复相同的过程。

122
00:08:21,570 --> 00:08:23,320
我们正在努力将成本降到最低。

123
00:08:23,520 --> 00:08:26,860
所以到目前为止，我们只处理了那一行。

124
00:08:27,030 --> 00:08:29,470
让我们看看当您有多个角色时会发生什么。

125
00:08:29,490 --> 00:08:31,320
这是完整的数据集。

126
00:08:31,350 --> 00:08:38,610
我们有八排您睡了几个小时，或者也许这些是一天中不同的学生

127
00:08:38,610 --> 00:08:44,070
同一项考试，他们还研究了多少其他小时的时间，才可以参加考试

128
00:08:44,070 --> 00:08:47,300
测验和他们在测试中的最终结果。

129
00:08:47,490 --> 00:08:52,720
正如您在左侧看到的那样，我实际上已经获得了八种感知。

130
00:08:53,100 --> 00:08:55,950
他们都是相同的看法，因此这也很重要。

131
00:08:56,010 --> 00:09:02,600
我只是将其乘以或想重复八次，以便我们可以。

132
00:09:03,330 --> 00:09:04,310
构想就是这样。

133
00:09:04,320 --> 00:09:10,010
但是这里重要的是，我们将使用相同的神经网络将它们馈送到一个Samual中

134
00:09:10,040 --> 00:09:10,380
网络。

135
00:09:10,380 --> 00:09:11,650
因此，让我们开始吧。

136
00:09:11,650 --> 00:09:20,550
因此，您会听到一个机场在提及整个数据集时曾提到过一个飞机场

137
00:09:20,610 --> 00:09:27,410
我们在所有这些角色上训练了我们的神经网络，因此这些列表都是如此。

138
00:09:27,420 --> 00:09:34,410
所以有我们的第一行，还有为什么第一行有第二个角色，这就是为什么我有

139
00:09:34,410 --> 00:09:35,260
第二轮。

140
00:09:35,280 --> 00:09:39,590
因此，它每次都被馈入相同的神经网络。

141
00:09:39,600 --> 00:09:45,070
我将它们复制了几次，以便我们可以直观地看到这种情况。

142
00:09:45,090 --> 00:09:52,320
然后又是第三排第四排，第四排有我们的白头

143
00:09:52,320 --> 00:09:53,010
排等等。

144
00:09:53,010 --> 00:09:56,580
基本上，我们对于其余四行也将获得相同的值。

145
00:09:56,580 --> 00:10:03,440
因此，每当我们连续向神经网络中输入数据时，我们都会得到它。

146
00:10:03,780 --> 00:10:06,930
然后我们将其与实际值进行比较。

147
00:10:06,930 --> 00:10:08,550
因此它们是实际值。

148
00:10:08,760 --> 00:10:11,340
因此，对于每一卷我们都有一个实际值。

149
00:10:11,640 --> 00:10:18,480
现在，基于y hat和我们为什么可以计算成本函数之间的所有这些差异，

150
00:10:18,480 --> 00:10:27,620
是所有为什么，为什么以及如何将所有一半减半的平方差的总和。

151
00:10:28,230 --> 00:10:30,360
还有我们的成本函数。

152
00:10:30,360 --> 00:10:36,750
基本上，现在我们有了完整的成本函数后会做什么，我们返回并更新权重

153
00:10:37,170 --> 00:10:39,480
我们更新W 1 WTW。

154
00:10:39,510 --> 00:10:45,810
这里要记住的重要一点是，所有这些感知都是所有这些神经网络

155
00:10:45,810 --> 00:10:47,340
实际上是一个神经网络。

156
00:10:47,340 --> 00:10:49,420
因此，其中没有八个，只有一个。

157
00:10:49,680 --> 00:10:55,110
当我们更新权重时，我们将基本上更新那个神经网络中的权重

158
00:10:55,110 --> 00:10:57,900
所有行的权重都将相同。

159
00:10:57,930 --> 00:11:00,560
因此，并非每个角色都有自己的作用。

160
00:11:00,580 --> 00:11:07,320
现在所有行共享权重，这就是为什么我们查看成本函数（即总和）的原因

161
00:11:07,620 --> 00:11:15,270
的平方差，然后我们更新权重，现在从这里开始只有一次迭代。

162
00:11:15,270 --> 00:11:19,020
接下来，我们将再次运行整个过程。

163
00:11:19,020 --> 00:11:25,440
我们将把每一行输入神经网络以找出我们的成本函数并执行此操作

164
00:11:25,440 --> 00:11:26,370
整个过程再次。

165
00:11:26,370 --> 00:11:32,090
因此，就像我们之前看到的那样，我们只有一行，而我们又一次又一次地进行所有操作

166
00:11:32,140 --> 00:11:33,590
同样在这里

167
00:11:33,600 --> 00:11:38,880
但是现在我们要做的是Pedros或800行或8000行，无论你有多少行

168
00:11:38,880 --> 00:11:40,590
在您的数据集中。

169
00:11:40,830 --> 00:11:43,700
您执行此过程，然后计算成本函数。

170
00:11:44,220 --> 00:11:51,510
而这里的目标是使成本函数最小化，并在发现成本的一分钟后尽快获得

171
00:11:51,510 --> 00:12:00,210
这是您最终的神经网络的功能，这意味着您已经调整了体重，并且发现了

172
00:12:00,750 --> 00:12:08,550
您开始进行训练的数据集的最佳权重，您可以继续进行

173
00:12:08,550 --> 00:12:11,130
测试阶段或应用阶段。

174
00:12:11,550 --> 00:12:14,920
这整个过程称为反向传播。

175
00:12:15,000 --> 00:12:21,930
因此，您可能需要对成本函数做一些补充阅读，我知道我们刚才谈到了

176
00:12:21,930 --> 00:12:24,840
大约一个，并且有很多不同的。

177
00:12:24,840 --> 00:12:28,690
好文章位于交叉验证中。

178
00:12:28,740 --> 00:12:33,020
它被称为神经网络中与应用程序一起使用的课程功能列表。

179
00:12:33,090 --> 00:12:39,840
因此欧元就在那儿，但是您只需在Google上搜索确切的搜索词或搜索词组，

180
00:12:39,960 --> 00:12:42,150
这将是第一个弹出的窗口。

181
00:12:42,150 --> 00:12:48,660
实际上，它具有一些针对不同成本函数的良好示例，应用程序或用例，因此，如果

182
00:12:48,660 --> 00:12:51,800
您有兴趣了解有关成本函数的更多信息，请查看本文。

183
00:12:51,990 --> 00:12:54,380
并希望您喜欢本教程。

184
00:12:54,420 --> 00:12:56,070
期待下次见到您。

185
00:12:56,070 --> 00:12:58,020
在此之前，请享受深度学习。

