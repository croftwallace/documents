1
00:00:01,130 --> 00:00:06,810
您好，欢迎回来，今天我们当然在深度学习上谈论Kostic梯度下降。

2
00:00:07,220 --> 00:00:14,450
之前我们了解了梯度下降，发现它是一种非常有效的方法

3
00:00:14,450 --> 00:00:19,590
解决我们试图最小化成本函数的优化问题。

4
00:00:19,640 --> 00:00:29,030
从根本上说，我们需要10到57年的能力才能在几分钟或几小时内解决问题

5
00:00:29,480 --> 00:00:30,940
或在一天左右的时间内。

6
00:00:31,100 --> 00:00:37,490
它确实有助于加快处理速度，因为我们可以看到哪条路是下坡路，我们可以进入

7
00:00:37,490 --> 00:00:41,400
指导并采取步骤，更快地达到最低要求。

8
00:00:41,600 --> 00:00:50,030
但是具有梯度下降特性的方法是该方法需要代价函数

9
00:00:50,030 --> 00:00:50,990
凸。

10
00:00:51,140 --> 00:00:57,710
正如您在这里看到的，我们专门选择了一个凸成本函数，基本上凸意味着

11
00:00:58,160 --> 00:01:05,510
该函数看起来类似于我们现在所看到的，只是有点向一个方向延伸

12
00:01:05,510 --> 00:01:09,220
从本质上讲，它具有一个全局最小值。

13
00:01:09,380 --> 00:01:11,560
这就是我们要找到的那个。

14
00:01:11,630 --> 00:01:14,060
但是，如果我们的职能不是凸的，那该怎么办？

15
00:01:14,060 --> 00:01:16,250
如果我们的成本函数不正确怎么办。

16
00:01:16,370 --> 00:01:17,810
如果看起来像这样呢？

17
00:01:18,020 --> 00:01:19,660
首先，这怎么可能发生。

18
00:01:19,880 --> 00:01:27,950
好吧，这可能是因为，如果我们首先选择一个成本函数，它不是平方差

19
00:01:28,010 --> 00:01:33,850
在为什么，为什么或是否选择成本函数之间进行选择。

20
00:01:33,860 --> 00:01:39,650
但是，在多维空间中，它实际上可以变成不凸的东西。

21
00:01:39,780 --> 00:01:45,410
因此，在这种情况下，如果我们尝试应用正常的梯度体面方法会发生什么

22
00:01:45,410 --> 00:01:46,390
这样可能会发生。

23
00:01:46,520 --> 00:01:51,230
我们可以找到成本函数的局部最小值而不是全局函数。

24
00:01:51,230 --> 00:01:57,730
因此，这是最好的一个，我们找到了错误的一个，因此我们的体重不正确。

25
00:01:57,740 --> 00:01:59,940
我们没有优化的神经网络。

26
00:02:00,230 --> 00:02:02,480
我们有一个神经网络。

27
00:02:02,610 --> 00:02:04,470
因此，在这种情况下我们该怎么办。

28
00:02:04,670 --> 00:02:09,110
好吧，这里的答案是随机的。

29
00:02:09,110 --> 00:02:10,050
梯度下降。

30
00:02:10,070 --> 00:02:15,260
事实证明，讽刺的梯度下降不需要因果函数是凸的。

31
00:02:15,380 --> 00:02:20,120
因此，让我们看一下我们讨论的正常梯度下降之间的两个差异

32
00:02:20,150 --> 00:02:21,600
和随机范围。

33
00:02:21,860 --> 00:02:27,920
因此，正常的绿色下降是当我们将所有行都插入神经网络后，

34
00:02:27,920 --> 00:02:33,890
再次在这里，我们已经多次复制了神经网络，但是将行插入

35
00:02:33,890 --> 00:02:36,050
每次都使用相同的神经网络。

36
00:02:36,050 --> 00:02:39,200
所以只有一岁的把戏，这只是出于基塞尔的行动目的。

37
00:02:39,350 --> 00:02:43,880
然后将其插入后，我们便根据正确的公式计算了成本函数，

38
00:02:43,880 --> 00:02:49,400
在底部的图表上，然后我们调整权重，这称为梯度下降

39
00:02:49,400 --> 00:02:54,480
该方法还是恰当的术语是批量梯度下降法。

40
00:02:54,470 --> 00:03:01,940
因此，我们从样本中提取全部批次，然后应用它，然后运行随机梯度

41
00:03:01,940 --> 00:03:03,730
下降方法有些不同。

42
00:03:03,800 --> 00:03:10,880
在这里，我们一行一行地处理，因此我们运行神经网络，然后调整

43
00:03:10,880 --> 00:03:12,020
重量。

44
00:03:12,020 --> 00:03:16,420
然后我们进入第二行，进入第二行，运行神经网络。

45
00:03:16,580 --> 00:03:21,640
我们先看一下成本函数，然后再次调整权重，然后再取另一支Rohtak玫瑰

46
00:03:21,640 --> 00:03:25,430
三，我们运行我们的神经网络会看一下我们调整权重的成本函数。

47
00:03:25,430 --> 00:03:32,660
因此，基本上，我们正在考虑在每一行之后调整权重，而不是进行所有操作

48
00:03:32,660 --> 00:03:36,080
然后测试权重两种不同的方法。

49
00:03:36,230 --> 00:03:39,710
现在，我们将比较两者。

50
00:03:39,710 --> 00:03:42,920
因此，这就是在视觉上记住它们的方法。

51
00:03:42,920 --> 00:03:49,490
因此，您具有最佳的梯度下降效果，在此之后您可以在运行权重之后调整权重

52
00:03:49,490 --> 00:03:55,370
您已经运行了神经网络中的所有行，然后基本上只运行了权重，然后运行了

53
00:03:55,370 --> 00:04:00,500
整个迭代再次迭代迭代迭代在12年级的六年级，您在

54
00:04:00,500 --> 00:04:06,650
一段时间，然后像调整权重一样调整权重，然后再次执行所有操作

55
00:04:06,770 --> 00:04:10,040
一次又一次，这称为讨论。

56
00:04:10,080 --> 00:04:16,580
您说主要的两个区别是讽刺梯度下降法可以帮助您

57
00:04:16,580 --> 00:04:27,470
避免在您发现那些局部末端或局部最小值而不是整体整体的问题

58
00:04:27,470 --> 00:04:28,620
全局最小值。

59
00:04:29,030 --> 00:04:34,850
简而言之，原因是存在随机梯度下降法的视频

60
00:04:35,150 --> 00:04:38,220
波动较大，因为它可以承受。

61
00:04:38,210 --> 00:04:43,650
它一次执行一次迭代或一行，因此波动要大得多，并且

62
00:04:43,650 --> 00:04:49,440
更有可能找到全局最小值，而不仅仅是局部最小值。

63
00:04:49,460 --> 00:04:56,480
关于讽刺梯度下降的另一件事，我认为这是一个不好的梯度，这是因为

64
00:04:56,480 --> 00:05:01,670
就像您可能会给人的第一印象，是因为它每次变慢都会增长

65
00:05:01,730 --> 00:05:09,050
但实际上它速度更快，因为它不必将所有数据加载到内存中

66
00:05:09,080 --> 00:05:12,610
然后运行，直到所有这些规则都完全遵守。

67
00:05:12,710 --> 00:05:16,780
您可以一个一个地滚动它们，因此在这种意义上，它的算法要轻得多，速度要快得多

68
00:05:16,790 --> 00:05:24,020
因此尽管它在某种意义上具有更多优势，因为它比弊端具有更多优势。

69
00:05:24,110 --> 00:05:25,320
梯度下降法。

70
00:05:25,430 --> 00:05:31,310
或域类的主要优点是提供了不良的梯度下降方法是它是一种

71
00:05:31,310 --> 00:05:37,250
确定性算法或除施加梯度下降外的其他方法都是讽刺算法

72
00:05:37,250 --> 00:05:44,570
只要您具有相同的起始权重，它是随机的且具有最佳的梯度和方法

73
00:05:44,570 --> 00:05:45,430
您的神经网络。

74
00:05:45,500 --> 00:05:52,300
每次运行批梯度下降法，您将获得相同的迭代结果

75
00:05:52,300 --> 00:05:57,960
一直为您更新您的体重，以使我们拥有讽刺的梯度

76
00:05:57,980 --> 00:05:58,300
方法。

77
00:05:58,310 --> 00:06:04,550
您不会得到它，因为这是一种随机方法，您可能会随机选择角色并

78
00:06:04,570 --> 00:06:10,940
您正在以讽刺的方式更新您的神经网络，因此您只需要访问每一个

79
00:06:10,940 --> 00:06:15,380
即使您一开始就拥有相同的权重，您一次运行类别还是一个不错的方法

80
00:06:15,380 --> 00:06:20,770
会有不同的过程和不同的迭代才能到达那里。

81
00:06:20,780 --> 00:06:28,100
简而言之，要谴责和不同意见的是，两者之间还有一种方法

82
00:06:28,100 --> 00:06:34,520
称为“最小批梯度下降”方法，您可以将两者结合起来并基本上运行

83
00:06:34,520 --> 00:06:37,640
而不是一次运行一整批。

84
00:06:37,640 --> 00:06:44,150
您运行的行数可能是5 10 100，但是您决定设置许多行来运行该数字

85
00:06:44,150 --> 00:06:47,690
一次更新行数，然后更新单位数等。

86
00:06:47,900 --> 00:06:52,670
如果您想了解有关梯度的更多信息，则称为Mini Bache梯度下降方法

87
00:06:52,670 --> 00:06:56,630
下降有一篇很棒的文章，您可以看一下。

88
00:06:56,660 --> 00:07:04,940
它由13行Python称为神经网络，由Andrew Trask和

89
00:07:04,940 --> 00:07:12,840
下面的链接是一篇很好的12 15文章，写得非常好，非常简单。

90
00:07:12,920 --> 00:07:21,860
关于如何应用绿色体面的水有一些有趣的哲学或只是有趣的想法

91
00:07:22,340 --> 00:07:28,460
您知道优点和缺点，以及在某些情况下如何做事，这样您就可以

92
00:07:28,460 --> 00:07:30,730
一些非常酷的技巧和技巧。

93
00:07:31,370 --> 00:07:33,620
很容易阅读，所以一定要检查一下。

94
00:07:33,800 --> 00:07:37,010
还有另一种更重的阅读。

95
00:07:37,010 --> 00:07:41,930
对于那些想进入数学领域的人，为什么要这么做。

96
00:07:41,930 --> 00:07:45,180
梯度下降是特定的。

97
00:07:45,260 --> 00:07:49,200
什么是推动评分的公式，以及如何计算等等。

98
00:07:49,220 --> 00:07:51,610
查看文章或实际书籍。

99
00:07:51,620 --> 00:07:57,160
这是Michael Nielsen 2015年出版的一本名为《神经网络和深度学习》的免费在线书。

100
00:07:57,160 --> 00:08:02,190
基本上就可以了，您可以继续进行检查。

101
00:08:02,450 --> 00:08:05,870
再次对数学进行了非常软的介绍。

102
00:08:05,870 --> 00:08:12,260
但是对于母亲来说，随着阅读的进行，数学却很沉重

103
00:08:12,530 --> 00:08:13,340
文章。

104
00:08:13,610 --> 00:08:20,240
但与此同时，它也让您陷入那种情绪，我想您的意思是就像一个热身的章节

105
00:08:20,240 --> 00:08:25,370
您首先要对数学进行热身，然后您又对数学产生了浓厚的兴趣，然后这篇文章

106
00:08:25,370 --> 00:08:26,110
要去。

107
00:08:26,540 --> 00:08:32,780
简而言之，这就是格兰尼感与梯度下降之间的区别

108
00:08:32,810 --> 00:08:36,360
以及如何工作。

109
00:08:36,410 --> 00:08:39,830
Tauriel说，就此而言，我们今天要总结。

110
00:08:39,840 --> 00:08:42,000
我期待与您见面。

111
00:08:42,020 --> 00:08:44,090
在此之前，请享受深度学习。

