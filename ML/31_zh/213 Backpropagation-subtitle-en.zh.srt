1
00:00:01,850 --> 00:00:07,340
您好，欢迎回到今天令人遗憾的课程，我们将总结反向传播。

2
00:00:07,340 --> 00:00:11,520
好的，我们就是您，我们几乎需要知道所有事情的一切

3
00:00:11,520 --> 00:00:17,990
我们知道有一个称为正向传播的过程，其中将信息输入到

4
00:00:17,990 --> 00:00:24,620
输入层，然后将其向前传播以获取白帽的输出值，然后进行比较

5
00:00:24,620 --> 00:00:29,240
这些都是我们在训练中所拥有的实际价值。

6
00:00:29,240 --> 00:00:36,110
然后我们计算误差，然后误差反方向通过网络反向传播

7
00:00:36,110 --> 00:00:41,570
方向，这使我们可以通过调整权重来训练网络。

8
00:00:41,660 --> 00:00:49,670
因此，这里要记住的关键一件事是反向传播是一种先进的算法驱动

9
00:00:49,670 --> 00:00:58,890
通过非常有趣和复杂的数学，我们可以调整权重。

10
00:00:59,030 --> 00:01:02,540
所有这些都同时被同时调整。

11
00:01:02,540 --> 00:01:08,990
因此，如果我们手动执行此操作，或者我们提出的算法类型与Even完全不同

12
00:01:08,990 --> 00:01:14,150
如果我们计算了误差，然后试图了解每个权重的影响

13
00:01:14,150 --> 00:01:21,040
对于错误，我们必须以某种方式独立或独立地调整每个权重。

14
00:01:22,000 --> 00:01:29,170
退货的巨大优势，而且要记住的关键是在退货过程中

15
00:01:29,170 --> 00:01:35,910
传播仅仅是因为算法的结构方式。

16
00:01:36,850 --> 00:01:43,990
您可以同时进行所有调整，因此您基本上可以知道每个错误的哪一部分

17
00:01:43,990 --> 00:01:47,400
神经网络负责您的体重。

18
00:01:47,450 --> 00:01:54,220
现在，这是反向传播的关键基本基础原理。

19
00:01:54,220 --> 00:02:02,650
这就是为什么它在1980年代迅速发展的原因，这是一个重大突破。

20
00:02:02,770 --> 00:02:08,890
如果您想了解更多有关该数学以及数学在后台如何工作的信息，那么

21
00:02:09,190 --> 00:02:14,800
我们已经提到的一篇很好的文章是神经网络，而深度学习实际上是

22
00:02:14,800 --> 00:02:16,640
迈克尔·尼尔森（Michael Nielsen）的书。

23
00:02:16,720 --> 00:02:23,610
您会发现所写的数学，它将帮助您理解这是怎么可能的。

24
00:02:23,650 --> 00:02:30,550
但是就我们的目的而言，如果从直觉的角度来看，重要的部分是要记住

25
00:02:31,240 --> 00:02:33,310
这就是退货的做法。

26
00:02:33,310 --> 00:02:36,750
它可以同时调整所有权重。

27
00:02:36,940 --> 00:02:43,300
现在，我们将逐步介绍一下在

28
00:02:43,300 --> 00:02:45,370
神经网络的训练。

29
00:02:45,370 --> 00:02:51,000
好的，所以第一步，我们将权重随机初始化为接近零但不为零的小数。

30
00:02:51,010 --> 00:02:56,830
在直觉教程中，我们并没有真正专注于权重的初始化，但随后我们有了

31
00:02:56,830 --> 00:03:02,610
从某个地方开始，并使用接近零的随机值进行初始化。

32
00:03:02,620 --> 00:03:09,730
然后从那里通过传播的过程来调整这些权重，直到

33
00:03:09,730 --> 00:03:11,690
错误被最小化。

34
00:03:11,970 --> 00:03:13,550
因此成本函数被最小化。

35
00:03:13,820 --> 00:03:19,330
然后，第二步将第一个观测值输入的所有数据集都输入到Lehre的第一行中

36
00:03:19,510 --> 00:03:21,440
每个功能都是一个输入。

37
00:03:21,440 --> 00:03:27,610
因此，基本上将梳子分别放入输入节点，以便从左到右传播

38
00:03:27,610 --> 00:03:27,910
对。

39
00:03:27,910 --> 00:03:32,620
神经元的激活方式是他们在我们的视线中选择的。神经元的激活受到神经元的限制。

40
00:03:32,620 --> 00:03:39,150
权重权重基本上确定了每个神经元激活然后传播的重要性。

41
00:03:39,160 --> 00:03:43,100
激活，直到获得结果为止。

42
00:03:43,150 --> 00:03:43,850
在这种情况下。

43
00:03:43,990 --> 00:03:46,640
因此，基本上，您是从左向右传播的。

44
00:03:46,690 --> 00:03:50,110
一路走，直到到达目的地。

45
00:03:50,320 --> 00:03:52,720
然后将结果与实际结果进行比较。

46
00:03:52,750 --> 00:03:58,140
测量产生的误差，然后从右向左进行倒数，空气是双极性的

47
00:03:58,150 --> 00:03:58,620
再次。

48
00:03:58,630 --> 00:04:02,080
根据它们对错误造成的影响来更新权重。

49
00:04:02,260 --> 00:04:08,500
再次，您可以计算出该值，因为反向传播扰动算法是

50
00:04:08,500 --> 00:04:13,750
结构化的学习率取决于我们将权重更新多少作为学习率参数

51
00:04:13,990 --> 00:04:17,710
您可以在神经网络中进行控制。

52
00:04:17,710 --> 00:04:23,110
步骤6重复步骤1 2 5，并在每次观察后更新权重。

53
00:04:23,320 --> 00:04:30,670
这就是所谓的强化学习，在我们的例子中是随机梯度下降或重复

54
00:04:30,670 --> 00:04:31,490
步骤1至5。

55
00:04:31,510 --> 00:04:37,840
但是那样一来，只有经过一番观察或一批学习之后，它才是一个完整的梯度下降

56
00:04:37,870 --> 00:04:43,150
或徽章绿色的日产汽车或小型批量梯度下降，当整列火车通过时进入第七步

57
00:04:43,150 --> 00:04:49,030
通过人工神经网络使一个纪元重做更多的纪元。

58
00:04:49,040 --> 00:04:55,090
因此，基本上只要继续这样做，然后让神经网络

59
00:04:55,120 --> 00:05:02,510
越来越好地训练，并在最小化成本函数时不断进行自我调整。

60
00:05:02,740 --> 00:05:04,330
所以我们去了。

61
00:05:04,420 --> 00:05:09,770
这些是构建人工神经网络并对其进行训练所需的步骤。

62
00:05:10,030 --> 00:05:16,060
这些都是您将要采取的步骤，直到我在实用教程中享用午餐为止。

63
00:05:16,120 --> 00:05:19,520
祝你好运，我期待下次见到你。

64
00:05:19,540 --> 00:05:21,280
在此之前，请尽情学习。

