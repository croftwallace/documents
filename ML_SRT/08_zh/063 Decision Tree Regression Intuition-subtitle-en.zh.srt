1
00:00:01,110 --> 00:00:05,880
您好，欢迎回到感兴趣的Tauriel机器学习课程，我们正在讨论决策

2
00:00:05,880 --> 00:00:08,160
树木及其背后的直觉。

3
00:00:08,190 --> 00:00:08,610
好吧。

4
00:00:08,640 --> 00:00:12,940
因此，您可能听说过“购物车”一词，代表分类树和回归树。

5
00:00:13,170 --> 00:00:19,070
这是一个笼统的术语，涵盖了两种类型的决策树，好像在猜测

6
00:00:19,140 --> 00:00:23,070
它们是分类树和回归树。

7
00:00:23,070 --> 00:00:28,800
在本课程中，我们将讨论两种类型，但在本节中，我们将重点讨论

8
00:00:28,830 --> 00:00:30,690
在回归树上。

9
00:00:31,020 --> 00:00:36,030
我想马上提一下，回归树比分类要复杂一些

10
00:00:36,030 --> 00:00:40,570
树木，这就是为什么它会更长一些，并且需要额外注意的原因

11
00:00:40,580 --> 00:00:40,820
。

12
00:00:40,950 --> 00:00:48,060
但是，尽管如此，我们仍将把这个有点复杂的话题分解成非常简单的内容

13
00:00:48,060 --> 00:00:51,960
信息元素的大小，因此这一切都说得通。

14
00:00:51,960 --> 00:00:54,870
到最后，您将非常满意回归分析。

15
00:00:54,900 --> 00:00:56,750
因此，让我们直接了解它。

16
00:00:56,760 --> 00:00:57,240
好吧。

17
00:00:57,300 --> 00:01:02,760
因此，在这里，我们有一个散点图，代表了已经提供给我们的Darcis sardar。

18
00:01:02,790 --> 00:01:07,950
散点图的有趣之处在于，我们必须独立设计可穿戴设备x1和x2

19
00:01:07,960 --> 00:01:08,120
。

20
00:01:08,310 --> 00:01:15,210
而我们所预测的是第三个变量，它是白色的因变量，您实际上无法

21
00:01:15,210 --> 00:01:16,270
看到一个。

22
00:01:16,280 --> 00:01:21,030
那是因为这只是一个仅适合两个变量的二维图表。

23
00:01:21,030 --> 00:01:22,710
为什么是第三维。

24
00:01:22,710 --> 00:01:27,440
而且，如果您考虑一下，那就像是伸出屏幕的那一面。

25
00:01:27,440 --> 00:01:31,560
这只是x1 x2平面上所有点的投影。

26
00:01:31,560 --> 00:01:35,070
因此，如果我添加一个尺寸，我将看起来像这样。

27
00:01:35,220 --> 00:01:37,730
但是我们可以再次看到为什么。

28
00:01:37,890 --> 00:01:43,320
有趣的是，我们实际上不需要了解原因，因为我们需要对此进行处理

29
00:01:43,840 --> 00:01:47,260
散点图，以便我们建立决策树。

30
00:01:47,370 --> 00:01:49,610
然后，一旦我们构建完毕，它将返回到y。

31
00:01:49,830 --> 00:01:56,790
现在我想说的一个快速点是，我看到了决策树仅用一个独立的解释

32
00:01:56,940 --> 00:02:04,020
变量x 1或X和Y，然后在这种情况下可以将X-1放在此处

33
00:02:04,050 --> 00:02:04,450
接着。

34
00:02:04,470 --> 00:02:10,080
为什么要去这里，您会有一些不同类型的图表，您将能够解释

35
00:02:10,080 --> 00:02:10,470
这样。

36
00:02:10,470 --> 00:02:16,380
但与此同时，我认为这可能并不能真正推动这一观点的发展，这可能会让人感到困惑

37
00:02:16,380 --> 00:02:20,540
当它被这样解释时，尽管有时它已经完成了。

38
00:02:20,670 --> 00:02:26,880
尽管如此，我认为会全力以赴，全力以赴，并着眼于这个问题

39
00:02:26,990 --> 00:02:32,010
有两个自变量，因为这将是一个更可靠的解释，因此将使它更有意义

40
00:02:32,010 --> 00:02:37,800
复杂，但从长远来看绝对值得，因为那样可以理解决策树

41
00:02:37,830 --> 00:02:42,360
回归和位还是Ashlynn，我会说更好。

42
00:02:42,360 --> 00:02:42,620
好吧。

43
00:02:42,630 --> 00:02:48,630
因此，让我们继续我们得到X1和x2这些是自变量

44
00:02:48,630 --> 00:02:54,510
看不到它添加了第三维，我们实际上会暂时忘记它

45
00:02:54,510 --> 00:02:54,530
。

46
00:02:54,540 --> 00:02:59,940
是的，所以我们将忘记它，因为我们需要使用此散点图来了解

47
00:02:59,940 --> 00:03:01,550
我们的决定将会很棒。

48
00:03:01,800 --> 00:03:10,230
因此，一旦您在回归意义上运行了回归树或决策树算法，将会发生什么

49
00:03:10,260 --> 00:03:18,240
是您的散点图将被分割成几段，让我们看一下算法的运行方式

50
00:03:18,240 --> 00:03:18,990
关于这样做。

51
00:03:18,990 --> 00:03:24,410
因此，算法会创建一个分割，例如在20左右的某个位置。

52
00:03:24,660 --> 00:03:29,090
因此，它基本上会将您的图表或散点图分为两部分。

53
00:03:29,100 --> 00:03:30,240
其他一切都小于20。

54
00:03:30,240 --> 00:03:34,860
一切都很好，X1变量为20，然后将再次拆分。

55
00:03:34,860 --> 00:03:41,780
因此，对于这方面的所有元素，它们将被比较为大于或小于170

56
00:03:41,780 --> 00:03:44,910
在这里分裂，然后可能在这里再分裂。

57
00:03:44,940 --> 00:03:51,630
现在，由算法确定如何进行拆分以及在何处进行拆分。

58
00:03:51,780 --> 00:04:00,570
它实际上涉及到一种称为信息熵的东西，它是一个数学上的

59
00:04:00,570 --> 00:04:02,800
概念非常复杂。

60
00:04:03,030 --> 00:04:11,040
所以这基本上意味着当我执行此拆分权限时，此拆分将增加信息量

61
00:04:11,040 --> 00:04:12,930
关于我们的观点

62
00:04:12,930 --> 00:04:20,670
它实际上是为我们希望对点进行分组并且算法知道的方式增加了一些价值吗

63
00:04:20,670 --> 00:04:26,920
何时停止是何时需要添加一定数量的信息。

64
00:04:27,180 --> 00:04:35,640
而且一旦它无法通过拆分这些叶子将更多信息添加到我们的设置中，它们就被称为

65
00:04:35,640 --> 00:04:39,660
叶子，因此通过拆分这些叶子将每个拆分称为叶子。

66
00:04:40,270 --> 00:04:45,730
它可以添加比停止或进入更多的信息，或者算法可以说当您拥有

67
00:04:45,730 --> 00:04:50,950
如果要进行拆分，则少于5％，那么您将不到5％

68
00:04:51,010 --> 00:04:54,340
该叶子中的总点数，然后将不会创建该叶子。

69
00:04:54,340 --> 00:04:58,660
因此，不同的选择会有不同的变化。

70
00:04:58,690 --> 00:05:02,330
但是看起来最重要的当然是分裂发生的地方。

71
00:05:02,560 --> 00:05:07,570
如果您想了解更多有关信息熵的知识，则需要进一步研究信息熵

72
00:05:07,570 --> 00:05:07,990
。

73
00:05:07,990 --> 00:05:12,850
对于我们来说，我们现在不打算深入到数学深度，足以知道

74
00:05:12,850 --> 00:05:19,600
算法可以处理此问题，并且可以找到我们的数据集在这些叶子中的最佳分割

75
00:05:19,600 --> 00:05:19,620
。

76
00:05:19,630 --> 00:05:25,810
最后的这些称为终叶，然后我们将重点关注实际应用

77
00:05:25,810 --> 00:05:33,190
算法的使用以及为什么我们将使用这些决策树以及这种回归将如何工作

78
00:05:33,190 --> 00:05:33,700
。

79
00:05:33,700 --> 00:05:35,920
好的，希望我们在同一页面上。

80
00:05:35,950 --> 00:05:36,510
让我们继续。

81
00:05:36,510 --> 00:05:42,970
因此，我们将稍微回顾一下所有这些内容，并一一创建这些拆分

82
00:05:42,970 --> 00:05:46,630
同时我们将实际开始绘制决策树。

83
00:05:46,930 --> 00:05:49,280
因此，我们的图表是全新的。

84
00:05:49,510 --> 00:05:51,340
然后我们进行拆分。

85
00:05:51,520 --> 00:05:54,300
因此，现在我们将开始创建决策树。

86
00:05:54,560 --> 00:05:57,640
拆分发生在20点，所以让我们开始绘制。

87
00:05:57,790 --> 00:06:01,260
这是我们的第一个决定，我们有两个选择。

88
00:06:01,270 --> 00:06:02,940
是和否

89
00:06:03,250 --> 00:06:05,410
好的，让我们看看接下来会发生什么。

90
00:06:05,440 --> 00:06:11,530
接下来发生分裂，分裂发生在170，并且仅在大于

91
00:06:11,530 --> 00:06:12,170
20

92
00:06:12,430 --> 00:06:17,480
因此，这意味着您将检查此条件X-1小于20意味着您检查了否。

93
00:06:17,500 --> 00:06:18,360
答案是否定的。

94
00:06:18,490 --> 00:06:25,630
然后检查x 2是否小于170 X-News小于一次，然后拆分3

95
00:06:25,630 --> 00:06:29,200
并检查x 2是否为alyssum 200。

96
00:06:29,320 --> 00:06:37,840
让我们在此处添加X，然后在40处分割4，然后检查x 1是否更大

97
00:06:37,900 --> 00:06:43,430
或小于40，并且仅在回答的点发生分裂的情况发生。

98
00:06:43,520 --> 00:06:45,120
不，它不少于20。

99
00:06:45,130 --> 00:06:47,300
一分为二，他们回答不。

100
00:06:47,300 --> 00:06:50,240
是的，实际上少于170。

101
00:06:50,530 --> 00:06:52,950
因此，不少于20年。

102
00:06:52,960 --> 00:06:53,920
小于170。

103
00:06:53,920 --> 00:06:58,490
然后此拆分将发生四次X-1小于40。

104
00:06:58,510 --> 00:06:59,240
是。

105
00:06:59,530 --> 00:07:01,140
好的，这就是我们的决策树。

106
00:07:01,150 --> 00:07:02,840
绘制完成。

107
00:07:03,070 --> 00:07:04,450
那么接下来会发生什么。

108
00:07:04,450 --> 00:07:07,620
我们如何实际填充到这些框中。

109
00:07:07,840 --> 00:07:11,320
好吧，这是我们需要记住因变量的地方。

110
00:07:11,410 --> 00:07:22,630
第三维和我们需要在此处检查的是如何预测y的值

111
00:07:22,690 --> 00:07:28,130
新的观察结果被添加到我们的散点图或我们的女儿中。

112
00:07:28,150 --> 00:07:35,320
假设我们添加一个观测值，其中x 1等于30，x 2等于50。

113
00:07:35,410 --> 00:07:37,140
它确实落在这里的某个地方。

114
00:07:37,300 --> 00:07:40,660
50就是我们要去的地方或在这里。

115
00:07:40,780 --> 00:07:45,060
因此很明显，它落入了这个末端叶子。

116
00:07:45,070 --> 00:07:47,880
以及如何获得这些信息。

117
00:07:47,890 --> 00:07:53,390
因此，正如您所看到的，我们一直在添加这些板块，我们已经在系统中添加了信息。

118
00:07:53,410 --> 00:07:58,450
那么，现在我们知道的信息如何落入该终端叶中。

119
00:07:58,510 --> 00:08:04,060
这些信息如何帮助我们预测妻子对这一新元素的价值，

120
00:08:04,060 --> 00:08:04,960
我们要添加。

121
00:08:05,230 --> 00:08:08,550
好吧，它的工作原理实际上非常简单。

122
00:08:08,620 --> 00:08:14,860
它的工作方式是您只取每个终端叶子的平均值。

123
00:08:14,950 --> 00:08:21,790
因此，将所有这些点的y取平均值，这就是将要分配的值

124
00:08:21,790 --> 00:08:23,990
到该终端的任何新点。

125
00:08:23,990 --> 00:08:26,860
此终端相同，此终端相同。

126
00:08:26,880 --> 00:08:28,530
与此相同，对我们相同。

127
00:08:28,680 --> 00:08:29,290
让我们来看一下。

128
00:08:29,290 --> 00:08:34,480
假设为什么这里的平均值是65点7明智的平均值是300

129
00:08:34,480 --> 00:08:36,930
点五1023在这里。

130
00:08:36,970 --> 00:08:39,840
负六十四点一零点在这里。

131
00:08:40,030 --> 00:08:48,460
因此，对于这一点，我们刚刚与X Wanny Consoli讨论过，并且x 2 x等于50的预测值

132
00:08:48,460 --> 00:08:49,300
的y。

133
00:08:49,390 --> 00:08:54,280
回归树算法将预测负64点1的值。

134
00:08:54,490 --> 00:08:59,620
如果它落入任何其他终端，那将是那里的预测值。

135
00:08:59,620 --> 00:09:01,950
如您所见，它实际上非常简单。

136
00:09:01,960 --> 00:09:10,480
这很简单，只需取平均值即可，但您需要记住我们正在努力

137
00:09:10,480 --> 00:09:18,060
整个练习的重点是将更多信息添加到系统图表中，以更好地预测

138
00:09:18,070 --> 00:09:18,280
为什么。

139
00:09:18,280 --> 00:09:25,510
因为如果您考虑一下，我们的另一选择是什么，如果默认选项是默认选项，那是什么

140
00:09:25,510 --> 00:09:31,270
在此数据集上进行任何机器学习的路途遥遥无期，就是要抓住所有要点，

141
00:09:31,270 --> 00:09:38,230
在所有点上求平均，无论您指向何处，新的数据元素

142
00:09:38,530 --> 00:09:40,300
会被添加到我们的数据集中。

143
00:09:40,300 --> 00:09:41,330
我们只分配。

144
00:09:41,410 --> 00:09:45,960
对于我们先前已经存在的所有点，它总是平均的。

145
00:09:46,000 --> 00:09:51,490
现在我们要做的是将图表拆分为这些终端，从而离开机器学习算法

146
00:09:51,490 --> 00:09:54,050
已将信息培养添加到我们的系统中。

147
00:09:54,100 --> 00:10:02,200
因此，现在我们可以更准确地预测值或将y的值分配给新出现的元素

148
00:10:02,200 --> 00:10:02,550
。

149
00:10:02,560 --> 00:10:09,700
正如您现在所看到的，它是所有特定部分或细分中的平均值

150
00:10:09,760 --> 00:10:14,810
的散点图，因此它应该是或者应该更准确。

151
00:10:14,830 --> 00:10:17,820
这就是回归树的重点。

152
00:10:17,950 --> 00:10:24,640
现在最后一次要做的就是将值添加到决策树中，因此基本上

153
00:10:24,640 --> 00:10:29,040
只要有新值，就在此处和现在添加这些值。

154
00:10:29,650 --> 00:10:36,670
将会发生的是经过这些检查的算法，最后检查它在哪里

155
00:10:36,670 --> 00:10:38,110
并分配值。

156
00:10:38,440 --> 00:10:39,340
就是这样。

157
00:10:39,340 --> 00:10:45,520
因此，散点图更像是物理的Zeshan概念性目的，因此您可以驾驶一些

158
00:10:45,520 --> 00:10:46,390
来自那里的见解。

159
00:10:46,480 --> 00:10:52,900
但是决策树的核心实际上在这里，这就是为什么将该算法称为回归的原因

160
00:10:52,990 --> 00:10:53,810
树。

161
00:10:53,920 --> 00:10:59,890
我希望您喜欢今天的Statoil，希望我们确实将这个非常复杂的主题分解为一些

162
00:10:59,950 --> 00:11:01,900
简单可行的步骤。

163
00:11:02,020 --> 00:11:04,070
我期待与您下次见面。

164
00:11:04,120 --> 00:11:05,950
在那之前在德国学习

