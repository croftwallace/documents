1
00:00:00,270 --> 00:00:02,460
您好，欢迎来到本Python教程。

2
00:00:02,700 --> 00:00:07,470
因此，在前面的部分中，我们已经介绍了非线性回归模型多项式回归

3
00:00:07,470 --> 00:00:09,810
模型和回归模型。

4
00:00:09,810 --> 00:00:12,680
因此对于多项式回归似乎是最好的。

5
00:00:12,870 --> 00:00:18,180
今天，在这个故事中，我们将再次构建决策树回归模型，并了解其将如何处理

6
00:00:18,180 --> 00:00:18,950
问题。

7
00:00:19,170 --> 00:00:21,240
好的，让我们快速处理基础知识。

8
00:00:21,240 --> 00:00:25,760
我们将最终在这里探索将步枪设置为工作目录。

9
00:00:26,000 --> 00:00:31,800
因此，我们需要进行回归，现在进入决策树回归部分。

10
00:00:32,010 --> 00:00:35,810
因此，让我们继续进行操作，确保在归档时必须定位得最酸。

11
00:00:35,820 --> 00:00:40,830
如果是这种情况，让我们单击此处的这个小按钮以将该文件夹设置为工作目录

12
00:00:40,830 --> 00:00:41,110
。

13
00:00:41,190 --> 00:00:44,850
现在让我们开始构建决策树回归模型。

14
00:00:44,850 --> 00:00:53,130
因此，我们将进入回归模板，并将所有内容从此处移到此处并将其粘贴到此处。

15
00:00:53,130 --> 00:00:59,910
现在让我们在这里更改基础知识，我将其称为决策树回归。

16
00:01:00,650 --> 00:01:05,930
好吧，让我们复制并粘贴到这里。

17
00:01:06,010 --> 00:01:07,300
好吧。

18
00:01:07,920 --> 00:01:10,130
至此基础已经完成。

19
00:01:10,170 --> 00:01:14,370
我们已经准备好更改这里需要更改的唯一的东西。

20
00:01:14,370 --> 00:01:16,320
我们创建侵略者。

21
00:01:16,320 --> 00:01:21,270
因此，让我们删除此注释，现在我们创建回归器。

22
00:01:21,390 --> 00:01:23,410
因此，像往常一样，它将非常简单。

23
00:01:23,430 --> 00:01:28,320
这将需要三行，并且我们将完全像在第一次导入之前一样

24
00:01:28,320 --> 00:01:33,060
某个库中的一个类，然后我们将创建该类的新主题，

25
00:01:33,060 --> 00:01:36,630
是回归或对象或决策树回归。

26
00:01:36,930 --> 00:01:43,350
然后，我们将对该回归或对象使用不同的方法以适合我们或我们的数据集，然后

27
00:01:43,350 --> 00:01:48,960
准备好了，我们只需要一个接一个地执行这些部分，就可以得到最终结果

28
00:01:48,960 --> 00:01:49,210
。

29
00:01:49,230 --> 00:01:50,110
因此，让我们开始吧。

30
00:01:50,290 --> 00:01:54,090
让我们从这里的第一个开始，它是导入适合该工作的类。

31
00:01:54,090 --> 00:02:00,620
因此，实际上此类来自标量点树库。

32
00:02:00,660 --> 00:02:01,410
好吧。

33
00:02:01,410 --> 00:02:07,500
从该库中，我们将导入决策树。

34
00:02:08,560 --> 00:02:14,090
Rick gress非常谨慎地在这里使用资本去耦资本R。

35
00:02:14,160 --> 00:02:17,940
这是因为它是一个类，这是该类的名称。

36
00:02:17,940 --> 00:02:20,420
因此像往常一样是一个非常直观的名称。

37
00:02:20,460 --> 00:02:20,910
好吧。

38
00:02:20,910 --> 00:02:25,910
因此，我们导入了该类，现在让我们创建该类的新对象，这是我们的遗憾或对象

39
00:02:25,910 --> 00:02:32,250
因此，我们将像往常一样将其称为回归器，然后相等，然后将其称为类

40
00:02:32,250 --> 00:02:36,620
我们只是重要，因为这实际上是此类的对象。

41
00:02:36,630 --> 00:02:37,380
开始了。

42
00:02:37,380 --> 00:02:41,590
当然，我们需要添加一些括号，因为我们需要指定一些参数。

43
00:02:41,730 --> 00:02:43,530
因此，让我们看一下这些参数。

44
00:02:43,650 --> 00:02:48,850
让我们按命令控制我们尝试检查决策树回归类。

45
00:02:49,200 --> 00:02:50,910
还有我们这里有什么。

46
00:02:50,940 --> 00:02:56,610
好的，所以我们有判据，困难的是MSCE均方误差。

47
00:02:56,880 --> 00:03:01,530
因此，您知道我们正在采用预测值与实际结果之间的平方差，并且

48
00:03:01,530 --> 00:03:05,370
考虑这些差异中的一些来衡量误差。

49
00:03:05,370 --> 00:03:07,320
因此，这是一个很好的收入标准。

50
00:03:07,320 --> 00:03:11,730
这就是我们将用于决策树回归或对象的方法。

51
00:03:11,880 --> 00:03:14,880
由于这里是默认设置，因此我们无需执行任何操作。

52
00:03:14,970 --> 00:03:15,730
我们都很好。

53
00:03:16,140 --> 00:03:19,580
然后，我们还有其他一些参数，例如splitter emacs功能。

54
00:03:19,690 --> 00:03:24,160
也就是说，如果您想采用更高级的方法来构建决策树。

55
00:03:24,330 --> 00:03:30,090
因此，我们现在就不再关注它，我们将其保留为默认参数，实际上是默认

56
00:03:30,090 --> 00:03:33,000
最好看到的分离器参数。

57
00:03:33,030 --> 00:03:34,440
所以我想我们对此表示满意。

58
00:03:34,590 --> 00:03:41,290
因此，我们实际上将看到有很多参数，因此我们不会放置一些值

59
00:03:41,310 --> 00:03:48,660
一个接一个，但我们不会指定随机状态值，因为我们都希望得到相同的结果

60
00:03:48,660 --> 00:03:49,250
。

61
00:03:49,260 --> 00:03:51,910
好，让我们在这里做。

62
00:03:51,990 --> 00:03:56,820
随机状态等于，因此我们可以选择所需的任何数字。

63
00:03:56,820 --> 00:03:59,480
我通常输入0 42。

64
00:03:59,700 --> 00:04:01,270
好吧，我们在这里放零。

65
00:04:01,380 --> 00:04:02,720
好，太棒了。

66
00:04:02,740 --> 00:04:04,550
现在我们都得到相同的结果。

67
00:04:04,860 --> 00:04:06,460
所以我们在同一页上。

68
00:04:06,480 --> 00:04:11,190
因此，这是从决策树回归类创建回归对象的第二步，并且

69
00:04:11,190 --> 00:04:16,560
现在的最后一步是使该回归对象适合我们的数据集。

70
00:04:16,600 --> 00:04:24,300
和往常一样，我们将像这样的回归对象而不是点，因为我们正在调用方法

71
00:04:24,630 --> 00:04:27,680
然后我们在这里采用fit方法。

72
00:04:27,840 --> 00:04:28,760
对。

73
00:04:28,950 --> 00:04:32,940
现在我们需要添加一些括号，因为它有一些参数，您可以看到这些参数

74
00:04:32,940 --> 00:04:34,310
是x和y。

75
00:04:34,440 --> 00:04:36,700
X是我们的特征矩阵。

76
00:04:36,810 --> 00:04:42,900
以及为什么我们的因变量向量和我们的数据集适合我们的回归或对象，因为

77
00:04:42,900 --> 00:04:48,570
我们的数据集由X的特征矩阵和y的因变量矢量组成。

78
00:04:48,850 --> 00:04:50,350
好，太棒了。

79
00:04:50,400 --> 00:04:56,910
现在，我们实际上已经准备好一步一步地退出各个部分并获得最终结果。

80
00:04:56,910 --> 00:04:58,260
因此，让我们这样做。

81
00:04:58,260 --> 00:05:01,980
所以请记住，因为我们所有人都必须申请，但这只是一个陷阱。

82
00:05:02,190 --> 00:05:04,470
让我们看看它是否仍然是陷阱。

83
00:05:04,480 --> 00:05:06,250
让我们看看是否需要应用它。

84
00:05:06,270 --> 00:05:11,610
我们将首先尝试在没有前途的情况下不执行全部操作，然后我们将看到什么

85
00:05:11,610 --> 00:05:12,450
发生。

86
00:05:12,460 --> 00:05:21,110
好的，所以让我们一步一步地执行库中第一个完成的部分，让我们导入数据

87
00:05:21,110 --> 00:05:23,990
设置并选择右列。

88
00:05:24,020 --> 00:05:24,980
好吧

89
00:05:25,120 --> 00:05:27,440
现在无需将数据集拆分为训练集。

90
00:05:27,460 --> 00:05:30,570
据说这是因为我们的数据集非常小。

91
00:05:30,610 --> 00:05:32,650
无需生活，您可能会得到生活。

92
00:05:32,890 --> 00:05:37,270
现在，我们终于可以进入创建决策树回归的部分。

93
00:05:37,270 --> 00:05:42,240
因此，让我们选择该部分并执行以真正创建它。

94
00:05:42,250 --> 00:05:43,270
开始了。

95
00:05:43,310 --> 00:05:46,950
正确创建并选择了所有默认参数。

96
00:05:47,110 --> 00:05:48,960
好的，到目前为止，一切都很好。

97
00:05:49,150 --> 00:05:54,880
现在，我们要么预测该员工的先前薪水，其先前的薪水水平为6.5

98
00:05:54,880 --> 00:05:56,910
公司或可视化结果。

99
00:05:56,920 --> 00:06:02,690
好吧，让我们尽力做到最好，让我们看看我们对出差前工资的预测。

100
00:06:02,710 --> 00:06:09,100
因此请记住，我们正在与未来将在谈判中被雇用的潜在员工进行谈判

101
00:06:09,100 --> 00:06:10,020
它的报价。

102
00:06:10,210 --> 00:06:14,830
这个人说的是以前的我是160 K，他要更多。

103
00:06:14,870 --> 00:06:20,350
根据先前公司的薪资表上的信息，

104
00:06:20,350 --> 00:06:25,910
信息，该员工的水平为6.5，我们正在尝试预测您的薪水。

105
00:06:25,960 --> 00:06:31,240
所以这个人说的话我是160，让我们看看我们的模型怎么说。

106
00:06:31,810 --> 00:06:32,340
好吧。

107
00:06:32,380 --> 00:06:36,610
现在进行预测，让我们看看获得一个可行的Explorer所需要的。

108
00:06:37,000 --> 00:06:39,940
它是一百五十。

109
00:06:39,970 --> 00:06:40,440
哇。

110
00:06:40,450 --> 00:06:47,270
由于回归，我们第一次得到低于提及薪水的预测

111
00:06:47,280 --> 00:06:53,800
尽管那是150 AK，但还是非常微小，然后我们比这个人说的少了10K

112
00:06:53,800 --> 00:06:55,120
因为这是相当可观的薪水。

113
00:06:55,280 --> 00:06:57,860
但是我想这已经很接近了，这仍然是一个很好的预测。

114
00:06:57,910 --> 00:07:01,500
即使这不是谈判的有利方面，我也认为这很好。

115
00:07:01,510 --> 00:07:07,390
因此，让我们检查一下在图上发生了什么，我们可以看到决策树回归结果

116
00:07:07,390 --> 00:07:07,720
。

117
00:07:07,720 --> 00:07:13,990
因此，我们将在本节中结束并了解决策树回归模型的方式。

118
00:07:14,140 --> 00:07:18,820
处理数据集并设法使其模型适合我们的数据集。

119
00:07:18,870 --> 00:07:19,990
因此，让我们这样做。

120
00:07:20,000 --> 00:07:21,070
这是情节。

121
00:07:21,070 --> 00:07:21,650
哇。

122
00:07:21,670 --> 00:07:22,160
好。

123
00:07:22,250 --> 00:07:30,030
因此，我要去的不仅是现在，我在这里要说的第一个单词是陷阱或危险信号。

124
00:07:30,110 --> 00:07:36,200
现在，根据Karylle解释的对直觉的直觉，我们现在处于新的审判之下

125
00:07:36,200 --> 00:07:36,420
。

126
00:07:36,520 --> 00:07:40,510
您是否认为这是决策树回归模型的真实形状。

127
00:07:40,840 --> 00:07:45,560
好吧，因为您知道Carol向我们解释的决策树回归算法是

128
00:07:45,550 --> 00:07:51,220
再次考虑信息中的熵，它将自变量分为几个

129
00:07:51,520 --> 00:07:52,360
间隔。

130
00:07:52,490 --> 00:07:55,400
因此，根据Tulio的直觉，您有两个独立变量。

131
00:07:55,420 --> 00:08:00,760
因此，差值间隔形成了一些矩形，您可以在其中取因变量的平均值

132
00:08:00,760 --> 00:08:01,470
价值观。

133
00:08:01,510 --> 00:08:06,750
但是在这里，因为我们处于一个维度，这意味着该算法在这里只会占用间隔

134
00:08:06,760 --> 00:08:10,400
自变量的值，例如应为一个间隔。

135
00:08:10,570 --> 00:08:12,840
在这里看起来像是第二个。

136
00:08:13,000 --> 00:08:16,240
这是第三个，这里是第四个。

137
00:08:16,330 --> 00:08:17,020
好。

138
00:08:17,020 --> 00:08:22,220
但是正如您在直觉教程中所了解的那样，它是取每个间隔的平均值。

139
00:08:22,270 --> 00:08:27,970
因此，如果要取平均值，您想如何在这里获得不水平的直线

140
00:08:27,970 --> 00:08:29,780
因为你知道这不是真的。

141
00:08:29,780 --> 00:08:35,830
格蕾琴（Gretchen）所做的是，在每个间隔中，它正在计算因变量工资的平均值

142
00:08:36,250 --> 00:08:41,200
因此，对于此间隔中包含的所有级别，预测值应为

143
00:08:41,200 --> 00:08:45,620
常数等于此间隔中因变量的平均值。

144
00:08:45,740 --> 00:08:50,110
而且正如我们所看到的，它不是一个常数，您知道这里的预测与预测并不相同

145
00:08:50,120 --> 00:08:50,590
这里。

146
00:08:50,710 --> 00:08:57,190
因此，要么考虑在每个无穷大中具有不同常数的区间的无穷大

147
00:08:57,220 --> 00:09:02,700
间隔，或者我们这里有问题，当然这不是首选。

148
00:09:02,840 --> 00:09:07,930
决策树回归不考虑此级别与该级别之间的区间无穷大

149
00:09:07,930 --> 00:09:08,620
水平。

150
00:09:08,890 --> 00:09:11,060
因此，这绝对是第二选择。

151
00:09:11,090 --> 00:09:13,350
现在，您知道问题出在哪里了。

152
00:09:13,660 --> 00:09:21,290
答案是在回归模板中，因为我们在这里观察到的仅是由于分辨率

153
00:09:21,280 --> 00:09:27,680
我们选择绘制这些决策树回归结果，因为我们实际上是在绘制预测

154
00:09:27,970 --> 00:09:30,940
10个级别中的每个级别都增加1。

155
00:09:30,930 --> 00:09:35,600
这意味着在这里您知道它只是在绘制10工资对应的预测

156
00:09:35,610 --> 00:09:41,420
10个级别，然后在这里通过一条直线加入预测，因为它没有预测

157
00:09:41,410 --> 00:09:43,660
在此间隔内绘制。

158
00:09:43,660 --> 00:09:49,960
在这里，如果自变量水平和事实是这种新的非线性回归问题

159
00:09:49,960 --> 00:09:55,840
决策树模型是由于先前非线性回归的一个非常特殊的原因

160
00:09:55,850 --> 00:09:56,140
楷模。

161
00:09:56,140 --> 00:10:01,270
我们可以使用生成此代码的代码，因为道德实际上是连续的。

162
00:10:01,380 --> 00:10:06,830
因此，例如在这些预测与这些预测之间的多项式回归模型中

163
00:10:07,080 --> 00:10:09,730
好吧，这里实际上几乎是一条直线。

164
00:10:09,750 --> 00:10:13,360
但是，现在我们面临着一种新型的回归模型。

165
00:10:13,380 --> 00:10:17,750
请记住，我们研究的第一种回归模型是线性回归模型。

166
00:10:17,820 --> 00:10:23,250
然后我们看到的第二种回归模型是非线性回归模型，现在我们面对

167
00:10:23,370 --> 00:10:25,580
一种新的回归模型。

168
00:10:25,620 --> 00:10:29,880
它是非线性R和非连续回归模型。

169
00:10:30,000 --> 00:10:35,120
确实，所有先前的回归都表明我们看到了是否存在Lynnie。

170
00:10:35,190 --> 00:10:36,800
他们都是连续的。

171
00:10:36,870 --> 00:10:42,690
但是这里的决策树回归模型不是连续的，这是第一个非连续机器

172
00:10:42,900 --> 00:10:44,920
我们一起看到的模型。

173
00:10:45,000 --> 00:10:48,970
那么Dino可视化和非连续回归模型的最佳方法是什么。

174
00:10:49,020 --> 00:10:54,810
就像我告诉您答案的那样，解决方案在我们的回归模板中。

175
00:10:54,810 --> 00:11:01,370
因此，让我们看一下，实际上我们需要看一下可视化回归模型的代码部分

176
00:11:01,380 --> 00:11:03,900
产生更高的分辨率。

177
00:11:04,110 --> 00:11:05,670
因此，让我们来看一下。

178
00:11:05,670 --> 00:11:12,600
实际上，让我们回到决策树回归文件并在此处替换此代码，因为

179
00:11:12,600 --> 00:11:17,520
完全不适合决策树回归模型，因为它是不连续的

180
00:11:17,530 --> 00:11:17,690
。

181
00:11:17,880 --> 00:11:22,850
因此，我们需要用相同的位替换此代码以获得更高的分辨率。

182
00:11:22,860 --> 00:11:24,550
现在让我们检查一下。

183
00:11:24,570 --> 00:11:32,730
该模型实际上已经准备就绪，我们可以用决策树回归代替它。

184
00:11:32,730 --> 00:11:38,400
现在让我们检查一下，您将看到真正的决策树回归模型是什么样的

185
00:11:38,460 --> 00:11:39,250
一天内

186
00:11:39,420 --> 00:11:43,850
因此，让我们选择所有这一切并执行。

187
00:11:44,370 --> 00:11:46,130
在这里。

188
00:11:46,140 --> 00:11:49,880
这就是它的样子，实际上它是一个非连续模型。

189
00:11:49,890 --> 00:11:55,230
我们甚至应该在这里有一些严格的垂直线，您知道它比非连续体更好地代表

190
00:11:55,430 --> 00:11:55,670
。

191
00:11:55,800 --> 00:12:01,530
为此，我们只需要提高分辨率，所以我将设置为0 1，您将看到

192
00:12:01,530 --> 00:12:05,940
我们将获得决策树回归解析的真实表示。

193
00:12:06,240 --> 00:12:08,190
因此，让我们开始吧。

194
00:12:08,640 --> 00:12:14,220
在这里，它几乎是垂直的，清楚地代表了真正的格雷沙姆案的判决。

195
00:12:14,310 --> 00:12:21,630
所以现在让我们放大一下，它现在变得更加有意义，因为正如基于熵的清楚解释

196
00:12:21,630 --> 00:12:27,150
再次在信息中将自变量的整个范围分为不同的时间间隔

197
00:12:27,150 --> 00:12:27,420
。

198
00:12:27,420 --> 00:12:29,770
因此，在这里我们可以清楚地看到间隔在哪里。

199
00:12:29,970 --> 00:12:35,610
正如Karylle在对Tauriel的直觉中所解释的那样，决策树回归模型正在考虑

200
00:12:35,610 --> 00:12:40,110
每个区间中因变量值的平均值。

201
00:12:40,110 --> 00:12:45,540
因此，例如，如果我们在此考虑此时间间隔，则该时间间隔内的平均工资将是

202
00:12:45,540 --> 00:12:48,980
非常简单，实际上是150000。

203
00:12:49,060 --> 00:12:55,670
因此，对于5.5和6.5之间的每个级别，薪水将被预测为100

204
00:12:55,680 --> 00:12:57,390
五万美元。

205
00:12:57,390 --> 00:13:02,920
因此，我们已经知道我们的模型将在这里为6.5水平预测什么。

206
00:13:02,940 --> 00:13:05,770
它将预测一百五十行。

207
00:13:05,790 --> 00:13:10,950
所以说到这个预测，现在我们获得决策树回归图形的结果非常好

208
00:13:11,130 --> 00:13:17,430
让我们实际检查一下该员工在先前公司中的等级为6.5的以前的对不起

209
00:13:17,790 --> 00:13:20,100
真的是103 T.J ..

210
00:13:20,410 --> 00:13:21,270
看看这个。

211
00:13:21,540 --> 00:13:25,270
让我们选择这一行并执行，就在这里。

212
00:13:25,290 --> 00:13:27,810
十五万

213
00:13:27,810 --> 00:13:32,640
所以现在我只想说两件事，以得出决策树回归模型并不有趣

214
00:13:32,640 --> 00:13:38,340
一维建模，但在更多维度上可能是非常有趣且功能强大的模型。

215
00:13:38,340 --> 00:13:41,490
所以这就是为什么您可以在此处使用此代码作为数据的原因。

216
00:13:41,520 --> 00:13:46,020
您在这里有用于构建模型的代码，而在这里可以进行预测，但是您不会

217
00:13:46,020 --> 00:13:50,460
能够使用此代码，因为您可能会有很多自变量，因此

218
00:13:50,460 --> 00:13:54,960
很多时间保险让您知道今天会给您解释，我给您解释

219
00:13:54,960 --> 00:14:00,420
一个给您，这样您现在就可以在脑海中完美地代表决策树回归

220
00:14:00,420 --> 00:14:01,720
以及它是如何工作的。

221
00:14:01,830 --> 00:14:07,020
现在，我们想在下一部分以一个谜题结束本教程，您将在它们周围看到森林

222
00:14:07,440 --> 00:14:09,870
对我们来说，围绕它们真的很简单。

223
00:14:09,870 --> 00:14:12,900
它只是一个由几个决策树组成的团队。

224
00:14:12,900 --> 00:14:19,890
因此，知道这是一棵树的结果，您认为我们将拥有10棵树的团队或

225
00:14:19,920 --> 00:14:22,560
甚至一百或五百棵树。

226
00:14:22,560 --> 00:14:26,230
第一个问题是，您认为我们在这里得到一些楼梯的形状吗？

227
00:14:26,400 --> 00:14:31,800
第二个问题是，您认为我们会得到更准确的预测吗？

228
00:14:31,800 --> 00:14:37,160
那已经非常接近应该是实际工资的一百六十万。

229
00:14:37,440 --> 00:14:40,490
这就是我要考虑的两个问题。

230
00:14:40,560 --> 00:14:43,960
我期待在下一节中为您提供解决方案。

231
00:14:43,980 --> 00:14:45,780
在那之前享受机器学习

