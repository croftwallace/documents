1
00:00:00,270 --> 00:00:05,640
您好，欢迎您在已经介绍过非线性回归的本部分中讨论Tauriel艺术

2
00:00:05,640 --> 00:00:09,470
为多项式回归模型和您的回归模型建模。

3
00:00:09,630 --> 00:00:15,090
今天，我们将介绍一个新的非线性回归模型，即决策树回归

4
00:00:15,210 --> 00:00:15,970
模型。

5
00:00:16,290 --> 00:00:21,330
因此，就我们拥有的数据集而言，最好的是多项式回归

6
00:00:21,330 --> 00:00:21,870
模型。

7
00:00:21,990 --> 00:00:24,220
让我们看看决策树回归将如何进行。

8
00:00:24,450 --> 00:00:27,090
与以前的相比。

9
00:00:27,120 --> 00:00:28,830
因此，让我们从基础开始。

10
00:00:28,830 --> 00:00:30,900
让我们为存在的目录设置权限。

11
00:00:30,900 --> 00:00:37,080
现在我要进入回归文件夹，我们现在进入决策树回归，了解谁在获取

12
00:00:37,080 --> 00:00:38,250
到最后。

13
00:00:38,280 --> 00:00:44,050
因此，这是包含位置服务文件的正确文件夹，请确保该文件已保存，

14
00:00:44,070 --> 00:00:46,700
更多设置为工作目录。

15
00:00:46,890 --> 00:00:52,140
一切都好，现在让我们采用回归模板来高效地构建此模型。

16
00:00:52,200 --> 00:00:57,600
因此，我们将所有内容从这里移到这里并粘贴到这里。

17
00:00:57,750 --> 00:01:03,340
正确，现在我们只需要更改一些内容，所以让我们更改基础知识，让我们替换您的回归模型

18
00:01:03,340 --> 00:01:10,010
将通过决策树回归。

19
00:01:10,010 --> 00:01:10,960
好吧。

20
00:01:11,010 --> 00:01:15,750
我们将复制该标题并将其放在此处。

21
00:01:15,750 --> 00:01:16,170
好吧。

22
00:01:16,180 --> 00:01:18,520
可视化决策的真实侵略结果。

23
00:01:18,750 --> 00:01:20,530
和这里一样。

24
00:01:20,550 --> 00:01:21,270
好吧。

25
00:01:21,270 --> 00:01:26,550
现在，让我们更改最重要的部分，即创建阻遏器的那部分，

26
00:01:26,550 --> 00:01:28,700
将要删除的内容在这里排成一行。

27
00:01:28,920 --> 00:01:31,610
现在，让我们构建决策树回归模型。

28
00:01:31,770 --> 00:01:37,500
因此，与导入包然后使用函数的任何时候都一样

29
00:01:37,500 --> 00:01:40,040
从这个软件包中构建我们的侵略者。

30
00:01:40,260 --> 00:01:44,810
因此，这是一个侵略性的方案，所有方案都将成为我们的公园方案。

31
00:01:44,910 --> 00:01:51,650
对于那些您的包中没有我们零件包的人，我将输入以下命令

32
00:01:51,690 --> 00:01:53,230
行，以便您可以安装它。

33
00:01:53,340 --> 00:01:55,710
因此，您可以看到我的已经在这里了。

34
00:01:55,710 --> 00:01:56,740
只要检查是否有。

35
00:01:56,790 --> 00:01:59,330
如果没有，请在此处键入。

36
00:01:59,360 --> 00:02:05,820
安装软件包，然后在括号中的引号中键入软件包的名称，这是我们的

37
00:02:05,820 --> 00:02:07,280
部分。

38
00:02:07,290 --> 00:02:07,800
好吧。

39
00:02:07,820 --> 00:02:10,120
然后，您只需选择执行此行。

40
00:02:10,140 --> 00:02:12,170
而这个隐形包。

41
00:02:12,210 --> 00:02:15,890
好的，但是我将把这一行作为注释。

42
00:02:16,020 --> 00:02:17,860
只需按Command键即可进行换档。

43
00:02:18,190 --> 00:02:25,500
现在，我们还要添加此行库，并在括号中添加我们要选择的包的名称

44
00:02:25,510 --> 00:02:25,550
。

45
00:02:25,590 --> 00:02:28,580
好吧，导入我们的权利。

46
00:02:28,770 --> 00:02:31,620
如您所见，此处未选择我们的部分。

47
00:02:31,620 --> 00:02:35,770
当执行这行代码时，将在此处选择。

48
00:02:35,950 --> 00:02:39,350
好的，现在我们准备开始构建模型。

49
00:02:39,390 --> 00:02:42,280
因此，正如我刚才所说，我们将从中获取一个功能。

50
00:02:42,300 --> 00:02:46,450
我们的零件库和此功能实际上也是我们的零件。

51
00:02:46,470 --> 00:02:50,690
因此，让我们使用此函数，并且像往常一样，我们将调用每个攻击者回归器。

52
00:02:50,850 --> 00:02:52,360
就是这样

53
00:02:52,560 --> 00:02:55,110
这就是我们的决策树回归再磨先生。

54
00:02:55,320 --> 00:02:58,680
而且，我们不要使用部件功能。

55
00:02:58,680 --> 00:03:01,850
好的，现在让我们看看我们需要输入哪些参数。

56
00:03:01,860 --> 00:03:09,200
最好的方法是在此处按一个，以便我们获得有关零件库的信息。

57
00:03:09,200 --> 00:03:15,210
所以通常我们在这里有，但是我们只需要点击这里的链接，它将给我们信息

58
00:03:15,210 --> 00:03:16,630
关于这个乔木库。

59
00:03:16,790 --> 00:03:17,010
好。

60
00:03:17,010 --> 00:03:18,280
让我们看看我们拥有什么。

61
00:03:18,540 --> 00:03:19,910
第一个参数是公式。

62
00:03:20,010 --> 00:03:21,490
因此，您知道在这里做什么。

63
00:03:21,510 --> 00:03:28,170
我们只需要写公式等于因变量，然后写一个Tilda然后是一个代表

64
00:03:28,230 --> 00:03:29,750
您所有的独立变量。

65
00:03:29,760 --> 00:03:32,420
好的，但是我们非常了解数据。

66
00:03:32,460 --> 00:03:36,790
因此，数据是我们要在其上构建决策树回归模型的数据集。

67
00:03:36,870 --> 00:03:42,740
因此，由于我们没有建立任何训练集，因此数据将完全像多项式一样

68
00:03:42,750 --> 00:03:45,460
回归和回归。

69
00:03:45,470 --> 00:03:51,450
那么权重是一个可选参数，您可以添加一些权重以使模型更高级

70
00:03:51,460 --> 00:03:51,510
。

71
00:03:51,540 --> 00:03:54,990
但是您知道这是更高级的，我们现在将不讨论。

72
00:03:55,290 --> 00:04:00,000
然后，您还有其他一些可选参数，但这些参数可以帮助您使模型均匀

73
00:04:00,000 --> 00:04:00,950
更强大。

74
00:04:00,960 --> 00:04:06,890
或者您知道包含一些正则化技术或zation来防止过度拟合此类问题

75
00:04:06,900 --> 00:04:06,930
。

76
00:04:06,950 --> 00:04:12,720
但是现在我们只想构建一个简单的容器，这根本不是真的，因为我们有一个简单的数据集

77
00:04:13,020 --> 00:04:15,400
所以我们只需要公式和数据

78
00:04:15,420 --> 00:04:19,890
因此，让我们回到我们的观点，并提出这些论点。

79
00:04:19,890 --> 00:04:27,390
所以第一个参数是公式，然后正如我提到的，公式等于薪水，因为这是我们的

80
00:04:27,390 --> 00:04:31,470
因变量比Tilda还要紧。

81
00:04:31,550 --> 00:04:36,280
然后是一个点或实际上是一个水平。

82
00:04:36,600 --> 00:04:41,190
但是您知道我想制作此脚本，该脚本也可以应用于您的数据集。

83
00:04:41,190 --> 00:04:42,560
所以我将使用一个点。

84
00:04:42,570 --> 00:04:43,230
好吧。

85
00:04:43,230 --> 00:04:44,750
这是第一个论点。

86
00:04:44,970 --> 00:04:52,930
现在让我们给第二个第二个，第二个是数据等于数据集的数据。

87
00:04:53,370 --> 00:04:57,130
现在，我们的回归器准备仅使用这两个参数构建。

88
00:04:57,150 --> 00:04:58,140
很简单。

89
00:04:58,140 --> 00:04:59,700
我们开始做吧。

90
00:04:59,710 --> 00:05:00,720
因此，实际上就是一切。

91
00:05:00,750 --> 00:05:08,330
这意味着我们的代码已准备就绪，可以执行，因为我们无需再进行任何更改。

92
00:05:08,620 --> 00:05:14,630
因此，让我们一个接一个地执行这些部分，然后看看决策树回归会发生什么。

93
00:05:15,100 --> 00:05:20,870
因此，我将选择第一部分执行重要的日子。

94
00:05:20,890 --> 00:05:21,810
这里是。

95
00:05:21,820 --> 00:05:27,570
因此，此处的自变量包含10个级别，因变量包含10个薪水

96
00:05:27,610 --> 00:05:29,920
这里与这10个级别相关。

97
00:05:29,920 --> 00:05:34,730
我们的目标是预测6.5级的薪水。

98
00:05:34,780 --> 00:05:36,820
数据不是线性分布的。

99
00:05:36,820 --> 00:05:38,600
这不是线性问题。

100
00:05:38,650 --> 00:05:44,140
这就是为什么我们要建立一些非线性回归模型来正确进行此预测的原因。

101
00:05:44,170 --> 00:05:48,440
因此，无需在测试中将数据集拆分为训练集。

102
00:05:48,440 --> 00:05:53,740
并且因为如您所见，这是一个非常小的数据集，所以不需要获取功能，因为

103
00:05:53,740 --> 00:05:59,050
决策树，我们不需要进行任何特征缩放，因为该模型的构建方式是基于

104
00:05:59,050 --> 00:06:03,390
取决于自变量而不是欧几里得距离。

105
00:06:03,610 --> 00:06:04,990
因此，我们对此表示满意。

106
00:06:04,990 --> 00:06:07,380
我们绝对不需要计划功能扫描。

107
00:06:07,660 --> 00:06:14,770
我们可以继续进行下一步，即创建模型，让我们执行它。

108
00:06:14,770 --> 00:06:15,220
好吧。

109
00:06:15,220 --> 00:06:17,620
创建了完美的回归器。

110
00:06:17,800 --> 00:06:20,750
现在让我们得出最终结论。

111
00:06:20,920 --> 00:06:27,580
据此人说，在英国这么好吧，现在让我们根据我们的预期工资

112
00:06:27,580 --> 00:06:28,290
模型。

113
00:06:28,600 --> 00:06:37,150
这样执行后，我们得到了24.99万美元，比薪水高得多

114
00:06:37,150 --> 00:06:38,420
此人提到的。

115
00:06:38,470 --> 00:06:44,000
但是，我们不要丢下仓促的结论，而让我们看看这里发生的事情。

116
00:06:44,260 --> 00:06:50,260
所以我将选择所有这些，让我们看看决策树回归结果正在发生什么

117
00:06:50,260 --> 00:06:51,880
。

118
00:06:51,900 --> 00:06:53,340
好吧，这就是我的想法。

119
00:06:53,410 --> 00:06:53,890
好。

120
00:06:54,070 --> 00:06:55,860
因此，我们不需要缩放它。

121
00:06:55,860 --> 00:06:57,560
我们清楚地看到这里发生了什么。

122
00:06:57,580 --> 00:07:04,120
就像我们在SVR for Python中看到的一样，它淹没了一条直线。

123
00:07:04,120 --> 00:07:09,040
对于那些不遵循定价教程的人，我们已经遇到了这种情况

124
00:07:09,040 --> 00:07:12,190
当我们得到一条水平直线时。

125
00:07:12,190 --> 00:07:14,230
实际上就像我们一样。

126
00:07:14,230 --> 00:07:20,120
这是由于我们没有将特征缩放应用于数据集。

127
00:07:20,170 --> 00:07:23,140
所以您认为问题出在这里。

128
00:07:23,200 --> 00:07:27,950
您是否认为这是因为我们没有像现在那样应用皮肤特征，而是需要采取行动

129
00:07:27,980 --> 00:07:31,490
就像您将要获得一个适合数据集的模型一样。

130
00:07:31,870 --> 00:07:36,810
就像我在开始学习时提到的那样，我们绝对不需要应用功能

131
00:07:36,830 --> 00:07:42,560
进入决策树，因为决策树回归模型基于变量的条件

132
00:07:42,600 --> 00:07:45,270
与欧几里得距离无关。

133
00:07:45,400 --> 00:07:49,690
而且您知道我们什么时候才需要得到它，是因为仅机器型号基于

134
00:07:49,780 --> 00:07:55,600
欧几里得距离，我们需要将所有自变量放在相同的比例尺上

135
00:07:55,600 --> 00:07:57,960
变量不能控制另一个变量。

136
00:07:58,270 --> 00:08:00,240
但这不是这里的问题。

137
00:08:00,250 --> 00:08:02,300
这与将来的扫描无关。

138
00:08:02,300 --> 00:08:07,810
您可以尝试在此处应用外观蒙皮功能，我们将执行此操作，但是您将遇到同样的问题

139
00:08:07,810 --> 00:08:09,090
一条水平直线。

140
00:08:09,100 --> 00:08:12,130
当然，这实际上是决策树模型。

141
00:08:12,160 --> 00:08:18,340
这实际上是决策树的一种模型，但这当然不是决策树的最佳版本

142
00:08:18,340 --> 00:08:19,800
我们想要得到的回归。

143
00:08:20,020 --> 00:08:26,110
因此，您可以开始在这里看到问题了吗，尤其是在观看了直觉教程之后

144
00:08:26,110 --> 00:08:29,100
您可以发现问题所在吗？

145
00:08:29,350 --> 00:08:36,480
我要告诉你这个问题与分割数有关，因为您知道分割的方法

146
00:08:36,490 --> 00:08:42,010
真正的侵略更多地是因为它根据不同的条件进行了一些分裂，因此

147
00:08:42,010 --> 00:08:45,160
您自己的变量中包含的条件越多，拆分的越多。

148
00:08:45,160 --> 00:08:50,650
在这里，我们显然没有位置，因为您知道所有预测都等于200

149
00:08:50,650 --> 00:08:52,450
五万美元

150
00:08:52,450 --> 00:08:57,250
因此，您知道这里花了10个不同级别的所有不同薪水，并得出了平均值

151
00:08:57,520 --> 00:09:00,200
并给出了所有级别的平均值。

152
00:09:00,280 --> 00:09:08,200
因此，这里没有条件，没有分裂，因此这绝对是没有意思的，特别是对于潜在的

153
00:09:08,200 --> 00:09:09,530
这个国家可以拥有的

154
00:09:09,700 --> 00:09:15,550
因此，我们现在要做的是在此处添加一个参数，该参数将为拆分设置条件。

155
00:09:15,580 --> 00:09:17,060
你知道那是我告诉你的。

156
00:09:17,080 --> 00:09:23,410
我们在这个Arbre库中有几个参数，可以使用这些可选参数来改进

157
00:09:23,500 --> 00:09:25,270
我们的模型并使它更强大。

158
00:09:25,420 --> 00:09:27,480
好吧，这正是我们现在要做的。

159
00:09:27,490 --> 00:09:30,340
我们将回到我们的部分。

160
00:09:30,340 --> 00:09:32,460
我要在这里按一个。

161
00:09:32,650 --> 00:09:34,810
实际上这一次我们的党正在露面。

162
00:09:34,810 --> 00:09:35,110
好。

163
00:09:35,110 --> 00:09:36,340
很好。

164
00:09:36,370 --> 00:09:37,240
我们的董事会在这里。

165
00:09:37,300 --> 00:09:44,380
正如我提到的那样，我们的部分具有几个参数，可以用来使参数更加健壮，并且

166
00:09:44,380 --> 00:09:49,550
我们现在感兴趣的一个参数将纠正我们在这个地方遇到的这个问题

167
00:09:49,560 --> 00:09:49,960
。

168
00:09:49,990 --> 00:09:53,380
因此，此参数实际上是控制参数。

169
00:09:53,380 --> 00:09:57,970
现在，我将为您提供一些技巧，以解决拆分问题。

170
00:09:57,970 --> 00:10:04,880
我们刚刚在这里获得了，所以我将添加您知道的第三个可选参数以改进我们的模型

171
00:10:04,880 --> 00:10:04,900
。

172
00:10:04,900 --> 00:10:09,730
目前，我们正在改善模型性能，这是机械科学家要做的事情

173
00:10:09,790 --> 00:10:11,320
在工作中经常做。

174
00:10:11,410 --> 00:10:16,660
因此，我们不要获得更高级的部分，尤其是当我们讨论交叉验证以查找

175
00:10:16,660 --> 00:10:22,870
最好的道德选择最好的参数，但是这里我们只是做一些简单的模型性能改进

176
00:10:23,230 --> 00:10:27,680
我们将只添加此控制参数，然后我将向您提供这个小技巧。

177
00:10:27,700 --> 00:10:33,430
因此，诀窍是再次使用我们的零件库。

178
00:10:33,430 --> 00:10:38,390
因此，我们在这里进行部分控制，这是一个函数，在该函数中，我们将添加一个

179
00:10:38,410 --> 00:10:41,750
正如您在此黄色矩形上看到的那样。

180
00:10:41,740 --> 00:10:46,320
在这里，我们有一个第一个论点是男装，这正是我们感兴趣的。

181
00:10:46,360 --> 00:10:51,790
这就是解决我们问题的方法，因为您实际上知道我们在这里没有任何分歧，因为

182
00:10:51,790 --> 00:10:53,290
这只是平均水平。

183
00:10:53,290 --> 00:10:57,310
就像我们对自变量没有条件也没有分割一样。

184
00:10:57,370 --> 00:11:03,040
因此，为了确保我们对自变量有一些分解和一些条件，我们实际上

185
00:11:03,160 --> 00:11:08,800
将mini splits设置为1即可解决问题。

186
00:11:08,800 --> 00:11:09,560
因此，让我们尝试。

187
00:11:09,670 --> 00:11:14,470
选择该部分以创建新的攻击者并不是真正的选择。

188
00:11:14,680 --> 00:11:18,290
所以我们的聚会已经开始进行了，所以我不需要再喜欢了。

189
00:11:18,520 --> 00:11:21,320
让我们创建这个侵略者。

190
00:11:21,710 --> 00:11:22,520
OK完成。

191
00:11:22,540 --> 00:11:25,760
完善您的侵略者创造的一切正确。

192
00:11:25,980 --> 00:11:26,340
好。

193
00:11:26,350 --> 00:11:32,410
现在，让我们先对结果进行可视化，以了解我们的模型现在是否正确，然后再进入

194
00:11:32,410 --> 00:11:33,490
最终裁决。

195
00:11:33,760 --> 00:11:37,010
因为您知道我们现在需要验证模型。

196
00:11:37,690 --> 00:11:40,920
因此，我选择了该部分，然后看看我们能到达哪里。

197
00:11:40,960 --> 00:11:43,860
让我们用手指交叉。

198
00:11:43,900 --> 00:11:44,610
开始了。

199
00:11:44,620 --> 00:11:46,440
我要放大情节。

200
00:11:47,200 --> 00:11:52,810
现在，我在这里首先要说的是陷阱或危险信号。

201
00:11:52,810 --> 00:11:55,560
现在，我们就在Neutra的前面。

202
00:11:55,690 --> 00:11:56,920
改进了模型。

203
00:11:56,920 --> 00:12:00,860
绝对可以肯定，我们在这里有多个吐口水。

204
00:12:00,970 --> 00:12:04,360
例如，这里是一个拆分，这是此处和此处的另一个拆分。

205
00:12:04,480 --> 00:12:07,940
好了，我们解决了拆分数的问题。

206
00:12:08,150 --> 00:12:12,020
这是根据Karylle在对直觉的直觉中所解释的。

207
00:12:12,040 --> 00:12:16,020
您是否认为这是决策树回归模型的真实形状。

208
00:12:16,330 --> 00:12:21,550
好吧，因为您知道Carol所解释的决策树回归算法是

209
00:12:21,550 --> 00:12:27,730
信息中的熵再次将自变量分为几个区间

210
00:12:27,740 --> 00:12:27,870
。

211
00:12:28,000 --> 00:12:32,740
因此，在直觉教学中，您有两个独立变量，因此不同的间隔形成了一些

212
00:12:32,740 --> 00:12:36,910
取因变量值平均值的矩形。

213
00:12:37,000 --> 00:12:42,280
但是在这里，因为我们处于一个维度，这意味着该算法在这里只会占用间隔

214
00:12:42,280 --> 00:12:45,960
自变量的值，例如应为一个间隔。

215
00:12:46,060 --> 00:12:48,520
在这里看起来像是第二个。

216
00:12:48,520 --> 00:12:49,970
这是第三个。

217
00:12:49,980 --> 00:12:51,840
这是第四个。

218
00:12:51,850 --> 00:12:52,230
好。

219
00:12:52,270 --> 00:12:58,140
因此，基本上看来，我们有四个条件，并且每个间隔都在进行分割。

220
00:12:58,330 --> 00:13:03,530
但是正如您在直觉教程中所了解的那样，它是取每个间隔的平均值。

221
00:13:03,580 --> 00:13:09,270
因此，如果要取平均值，您想如何在这里获得不水平的直线

222
00:13:09,280 --> 00:13:10,410
因为你知道

223
00:13:10,400 --> 00:13:16,060
侵略不是真的，这是因为它在每个时间间隔内都在计算受抚养人的平均值。

224
00:13:16,060 --> 00:13:22,150
可变薪资，因此对于此间隔中包含的所有级别，预测值

225
00:13:22,150 --> 00:13:26,900
常数应等于此间隔内因变量的平均值。

226
00:13:27,040 --> 00:13:31,420
而且正如我们所看到的，它不是一个常数，您知道这里的预测与预测并不相同

227
00:13:31,420 --> 00:13:31,880
这里。

228
00:13:31,990 --> 00:13:38,500
因此，要么考虑在每个无穷大中具有不同常数的区间的无穷大

229
00:13:38,530 --> 00:13:44,650
间隔，或者我们这里有问题，当然这不是决定的第一选择

230
00:13:44,650 --> 00:13:50,150
树回归未考虑此级别与此级别之间的区间无穷大。

231
00:13:50,200 --> 00:13:52,390
因此，这绝对是第二选择。

232
00:13:52,390 --> 00:13:54,650
现在，您知道问题出在哪里了。

233
00:13:54,940 --> 00:14:02,590
答案就在我们的回归模板中，因为我们在这里观察到的仅仅是由于分辨率

234
00:14:02,590 --> 00:14:08,980
我们选择绘制这些决策树回归结果，因为我们实际上是在绘制预测

235
00:14:09,250 --> 00:14:12,220
10个级别中的每个级别都增加1。

236
00:14:12,220 --> 00:14:16,940
这意味着在这里您知道它只是在绘制10工资对应的预测

237
00:14:16,940 --> 00:14:18,040
10个级别。

238
00:14:18,040 --> 00:14:23,410
然后在此处通过一条直线将其加入预测，因为它没有要绘制的预测

239
00:14:23,620 --> 00:14:27,220
在这个不可侵犯的水平区间。

240
00:14:27,550 --> 00:14:33,160
对于这个新的非线性回归模型来说，决策树模型是一个问题

241
00:14:33,160 --> 00:14:37,450
由于先前非线性回归模型的非常具体的原因。

242
00:14:37,450 --> 00:14:42,670
因为模型实际上是连续的，所以我们可以使用生成该图的代码。

243
00:14:42,670 --> 00:14:48,130
因此，例如在这些预测与这些预测之间的多项式回归模型中

244
00:14:48,280 --> 00:14:51,030
好吧，这里实际上几乎是一条直线。

245
00:14:51,040 --> 00:14:54,520
但是，现在我们面临着一种新型的回归模型。

246
00:14:54,700 --> 00:14:59,110
请记住，我们研究的第一种回归模型是线性回归模型。

247
00:14:59,110 --> 00:15:03,230
然后，我们看到的第二种回归是非线性回归模型。

248
00:15:03,370 --> 00:15:06,900
现在，我们面临着一种新型的回归模型。

249
00:15:06,940 --> 00:15:11,520
这是非线性和非连续回归模型。

250
00:15:11,680 --> 00:15:16,670
实际上，我们看到的所有先前回归模型都是线性的还是线性的，

251
00:15:16,870 --> 00:15:18,440
他们都是连续的。

252
00:15:18,520 --> 00:15:24,370
但是这里的决策树回归模型不是连续的，这是第一个非连续机器

253
00:15:24,580 --> 00:15:26,380
我们一起看到的模型。

254
00:15:26,680 --> 00:15:30,640
因此，什么是可视化非连续回归模型的最佳方法。

255
00:15:30,700 --> 00:15:36,390
就像我告诉你的，答案是我们的回归模板中的解决方案。

256
00:15:36,460 --> 00:15:43,030
因此，让我们看一下，实际上我们需要看一下可视化回归模型的代码部分

257
00:15:43,030 --> 00:15:45,790
产生更高的分辨率。

258
00:15:45,790 --> 00:15:47,180
因此，让我们来看一下。

259
00:15:47,350 --> 00:15:54,250
实际上，让我们回到决策树回归文件并在此处替换此代码，因为

260
00:15:54,250 --> 00:15:59,740
完全不适合决策树回归模型，因为它是非连续模型，并且

261
00:15:59,740 --> 00:16:02,560
所以我们需要用相同的代码代替

262
00:16:02,600 --> 00:16:06,190
但是对于高分辨率，现在让我们看看。

263
00:16:06,250 --> 00:16:12,550
该模型实际上已经准备就绪，我们可以用决策树回归代替它。

264
00:16:14,380 --> 00:16:20,050
现在让我们检查一下，您将看到真正的决策树回归模型是什么样的

265
00:16:20,140 --> 00:16:21,060
一天内

266
00:16:21,070 --> 00:16:25,500
因此，让我们选择所有这一切并执行。

267
00:16:26,050 --> 00:16:27,660
在这里。

268
00:16:27,820 --> 00:16:31,550
这就是它的样子，实际上它是一个非连续模型。

269
00:16:31,570 --> 00:16:37,270
我们甚至应该在这里有一条严格的垂直线，您知道它比连续性更好。

270
00:16:37,480 --> 00:16:40,350
为此，我们只需要提高分辨率即可。

271
00:16:40,360 --> 00:16:46,150
所以我将设在0 1上，您将看到我们将获得决策的真实表示

272
00:16:46,150 --> 00:16:47,870
树回归结果。

273
00:16:47,920 --> 00:16:50,290
因此，让我们开始吧。

274
00:16:50,290 --> 00:16:52,180
现在这里几乎是垂直的。

275
00:16:52,210 --> 00:16:55,950
这是对格雷森（Gresham mo）判决的明确表示。

276
00:16:55,990 --> 00:16:57,350
现在让我们放大一下。

277
00:16:57,550 --> 00:17:04,030
现在，它变得更加有意义，因为正如Carol根据信息中的熵所解释的那样

278
00:17:04,020 --> 00:17:08,960
再次将自变量的整个范围分为不同的时间间隔。

279
00:17:09,100 --> 00:17:15,160
因此，在这里我们可以清楚地看到第一个间隔是从1到6.5。

280
00:17:15,190 --> 00:17:19,040
第二个时间间隔是从6.5到8.5。

281
00:17:19,150 --> 00:17:23,450
然后，第三个间隔是从八点五到9.5。

282
00:17:23,590 --> 00:17:27,200
最后，最后一个间隔是从九点五到十。

283
00:17:27,370 --> 00:17:33,100
这样我们就可以清楚地看到间隔，正如Karylle在对Tauriel的直觉中所解释的

284
00:17:33,400 --> 00:17:39,130
决策树回归模型正在考虑每个变量中因变量值的平均值

285
00:17:39,190 --> 00:17:40,280
的间隔。

286
00:17:40,360 --> 00:17:43,540
这一个这一个这个和这个

287
00:17:43,540 --> 00:17:48,970
因此，例如，如果我们在此考虑此时间间隔，则该时间间隔内的平均工资将是

288
00:17:48,970 --> 00:17:52,440
非常简单，实际上是250000。

289
00:17:52,490 --> 00:18:01,390
因此，对于6.5和8.5之间的每个级别，预计薪金为250000美元。

290
00:18:01,390 --> 00:18:06,940
因此，我们已经知道我们的模型将在这里为6.5水平预测什么。

291
00:18:06,970 --> 00:18:09,630
预计将达到250K。

292
00:18:09,790 --> 00:18:14,950
所以说到这个预测，现在我们获得决策树回归图形的结果非常好

293
00:18:15,130 --> 00:18:21,400
让我们实际检查一下该雇员的先前薪水是否在其先前的6.5水平

294
00:18:21,400 --> 00:18:25,830
公司实际上是250 k，让我们检查一下。

295
00:18:26,110 --> 00:18:28,490
让我们选择这一行并执行。

296
00:18:28,730 --> 00:18:29,830
在这里。

297
00:18:29,830 --> 00:18:32,130
25万。

298
00:18:32,170 --> 00:18:36,360
就像我们预测的一样，因为我们可以在该图中清楚地看到这一点。

299
00:18:36,520 --> 00:18:38,770
所以现在我只想说两件事。

300
00:18:38,890 --> 00:18:44,320
决策树回归模型并不是一个有趣的模型，但它可能是一个非常有趣的模型

301
00:18:44,320 --> 00:18:47,040
以及在更多维度上非常强大的模型。

302
00:18:47,080 --> 00:18:50,200
因此，这就是为什么您可以在此处将此代码用于数据集的原因。

303
00:18:50,260 --> 00:18:54,170
您这里有建立模型的代码，而这里有进行预测的代码。

304
00:18:54,190 --> 00:18:58,690
但是那样您将无法使用此代码，因为您可能会有很多自变量

305
00:18:58,750 --> 00:19:02,550
因此，您知道的许多尺寸都可以在二维中为您提供解释。

306
00:19:02,590 --> 00:19:07,150
我正在为您解释其中的一小部分，您现在可以完全代表

307
00:19:07,150 --> 00:19:10,270
您的脑袋将进行决策树回归及其工作原理。

308
00:19:10,570 --> 00:19:15,780
现在，我们想在下一部分以一个谜题结束本教程，您将在它们周围看到森林

309
00:19:16,150 --> 00:19:21,640
对我们来说，围绕它们实际上非常简单，它只是一个由几个决策树组成的团队。

310
00:19:21,640 --> 00:19:28,630
因此，知道这是一棵树的结果，您认为我们将拥有10棵树的团队或

311
00:19:28,630 --> 00:19:31,300
甚至一百或五百棵树。

312
00:19:31,300 --> 00:19:34,940
第一个问题是，您认为我们会在这里得到这种形状的楼梯吗？

313
00:19:35,110 --> 00:19:40,540
第二个问题是，您认为我们会得到更准确的预测吗？

314
00:19:40,540 --> 00:19:46,160
这已经非常接近应该是实际工资的160K。

315
00:19:46,180 --> 00:19:49,220
这就是我要考虑的两个问题。

316
00:19:49,300 --> 00:19:52,700
我期待在下一节中为您提供解决方案。

317
00:19:52,720 --> 00:19:54,520
在此之前，享受任务学习

