1
00:00:00,390 --> 00:00:03,140
您好，欢迎回到机器学习课程。

2
00:00:03,300 --> 00:00:07,560
在本节中，我们讨论的是K均值聚类算法。

3
00:00:07,560 --> 00:00:11,490
在本教程中，我们将讨论Kamins背后的直觉。

4
00:00:11,490 --> 00:00:17,850
因此，Kamins是一种算法，可让您克隆数据，我们将看到它非常方便

5
00:00:17,850 --> 00:00:23,550
用于发现数据集中您可能不会想到的组类别的工具

6
00:00:23,550 --> 00:00:24,390
你自己的。

7
00:00:24,600 --> 00:00:31,320
在本节或本特定教程中，我们将学习如何在直觉上理解k均值

8
00:00:31,320 --> 00:00:33,780
级别，我们将看到Hardwick的示例。

9
00:00:33,780 --> 00:00:35,240
因此，让我们直接研究它。

10
00:00:35,280 --> 00:00:39,230
让我们使这个复杂的主题变得简单。

11
00:00:39,240 --> 00:00:41,610
好的，所以这里有一个散点图。

12
00:00:41,790 --> 00:00:43,740
以及这个机器人是如何变成的。

13
00:00:43,740 --> 00:00:49,810
好吧，让我们想象一下，我们的数据集中有两个变量，我们决定绘制这两个变量

14
00:00:49,810 --> 00:00:52,910
在x和y轴上，因此这里是一个变量。

15
00:00:53,010 --> 00:00:53,910
那是另一个。

16
00:00:53,970 --> 00:01:00,210
这就是根据这两个变量配置观察结果的方式。

17
00:01:00,210 --> 00:01:05,060
因此，这里的问题是我们可以在所有变量中确定某些组。

18
00:01:05,190 --> 00:01:07,360
我们也将如何去做。

19
00:01:07,370 --> 00:01:09,780
我们能否将其识别为一个。

20
00:01:09,810 --> 00:01:14,820
这是一组，这是第二组，或者这是第二组，也许有四个

21
00:01:14,820 --> 00:01:16,950
组或这三个组中的五个。

22
00:01:16,950 --> 00:01:22,190
我们如何识别组的数量我们如何识别组本身。

23
00:01:22,200 --> 00:01:29,700
因此，Kamins为您服务的是，它消除了此决策过程中的复杂性，并且

24
00:01:29,730 --> 00:01:36,210
使您可以非常轻松地识别出那些群集，这些群集实际上是您的数据点的群集

25
00:01:36,210 --> 00:01:36,850
数据集。

26
00:01:36,870 --> 00:01:41,220
因此，在这种情况下，他们将进入三个类别，这当然是一个非常简化的示例

27
00:01:41,220 --> 00:01:46,860
我们这里只有二维，所以两个变量Kamins可以与多维对象一起使用

28
00:01:46,860 --> 00:01:49,050
可以使用三四五十的尺寸

29
00:01:49,050 --> 00:01:54,630
本示例仅用于说明目的，因此我们可以直观地看到发生了什么，但实际上

30
00:01:54,630 --> 00:02:00,360
它们可以是多达10或100个任意数量的变量，Kamins会进行复杂的计算

31
00:02:00,360 --> 00:02:00,620
为了你。

32
00:02:00,620 --> 00:02:05,240
因此，这是一种旨在为您找到这些聚类的算法。

33
00:02:05,310 --> 00:02:10,100
今天，我们将进一步了解Kamins算法的工作原理。

34
00:02:10,440 --> 00:02:12,480
那么逐步的过程是什么。

35
00:02:12,480 --> 00:02:13,340
我最喜欢的部分。

36
00:02:13,350 --> 00:02:19,110
我们将按照如此简单的逐步流程将您细分，这将非常直观且

37
00:02:19,110 --> 00:02:20,340
简单易懂。

38
00:02:20,340 --> 00:02:26,670
好吧，Kamins算法的第一步是选择簇数K，我们将讨论

39
00:02:26,670 --> 00:02:29,360
有关如何选择最佳群集数的更多信息。

40
00:02:29,360 --> 00:02:31,120
在本部分课程的后面。

41
00:02:31,230 --> 00:02:36,420
但是现在，让我们想象一下，我们已经针对某些挑战达成了共识，让我们

42
00:02:36,420 --> 00:02:40,820
说我们必须同意，这五个集群或两个类别中有三个集群。

43
00:02:41,070 --> 00:02:47,610
完成此操作后，请继续执行第二步，即随机选择k个点

44
00:02:47,610 --> 00:02:53,280
是聚类的质心，这些点不一定必须来自数据集。

45
00:02:53,280 --> 00:02:57,480
如您所见，我们有一个散点图，您可以选择任何点和该散点图。

46
00:02:57,630 --> 00:03:02,430
它们不必一定是头皮上A型图的观察结果的一部分，但它们可以是任意随机x

47
00:03:02,430 --> 00:03:08,610
散点图上的y和y值，只要您选择一定数量的质心即可

48
00:03:08,610 --> 00:03:12,130
等于您决定的集群数量。

49
00:03:12,290 --> 00:03:14,580
是的，我们现在会在实践中看到这一点。

50
00:03:14,580 --> 00:03:22,050
然后进入第3步，将每个数据点分配给最近的质心，

51
00:03:22,080 --> 00:03:23,780
K立即立即群集。

52
00:03:23,780 --> 00:03:28,770
所以是的，您正在启动集群，然后将有一个迭代过程来优化这些集群

53
00:03:29,220 --> 00:03:30,690
我们将看到实际效果。

54
00:03:30,690 --> 00:03:35,370
但基本上，您只需要检查数据集中的每个点即可

55
00:03:35,370 --> 00:03:42,390
确定并进行第2步，其中哪一个最接近您的太平洋数据点，然后就是

56
00:03:42,390 --> 00:03:48,500
这个数据点将被分配到最接近的位置在这里是一个模糊的术语，因为什么样的距离

57
00:03:48,530 --> 00:03:54,540
您是通过欧几里得距离或其他可以更好地定义的距离来进行测量的

58
00:03:54,540 --> 00:03:55,240
在您的业务中。

59
00:03:55,260 --> 00:03:59,580
但是出于这些教程的目的，我们将讨论欧几里得距离。

60
00:03:59,730 --> 00:04:02,670
因此，这基本上是几何距离。

61
00:04:02,700 --> 00:04:04,930
简单来说。

62
00:04:04,950 --> 00:04:05,370
好吧。

63
00:04:05,370 --> 00:04:11,990
继续进行第三步的第四步是计算并放置新的质心，其中

64
00:04:12,060 --> 00:04:13,770
我们将立即看到如何完成此操作。

65
00:04:13,770 --> 00:04:18,930
因此，一旦找到簇，您便会重新计算质心，这将更容易理解

66
00:04:18,930 --> 00:04:20,580
我们来看一个例子。

67
00:04:20,740 --> 00:04:25,280
然后在步骤5中，将每个数据点重新分配给新的课程质心。

68
00:04:25,290 --> 00:04:32,020
因此，基本上您将再次执行步骤3，但是在这里我们将其称为步骤5，然后进行任何分配

69
00:04:32,090 --> 00:04:33,660
重复步骤4。

70
00:04:33,750 --> 00:04:37,330
因此，它成为第4步生存的迭代过程。

71
00:04:37,560 --> 00:04:41,370
如果诺里亚（Noria），有人可以放置并完成您的模型。

72
00:04:41,640 --> 00:04:47,520
我知道这可能看起来有些复杂，但是现在让我们看一个直观的示例，以便我们理解

73
00:04:47,520 --> 00:04:49,320
所有这些都非常直观。

74
00:04:49,320 --> 00:04:53,680
然后，您始终可以引用幻灯片或分步规则。

75
00:04:53,760 --> 00:04:59,280
当您实际执行k时，进一步向下表示聚类或当您想要了解它的方式时

76
00:04:59,340 --> 00:05:02,310
Kamins集群背景下发生的事情或正在发生的事情。

77
00:05:02,310 --> 00:05:08,820
因此，在讨论了我们将要进行的视觉练习之后，此幻灯片将是一个很好的参考点

78
00:05:08,820 --> 00:05:09,620
刚刚拥有。

79
00:05:09,880 --> 00:05:11,920
因此，让我们继续前进。

80
00:05:12,060 --> 00:05:18,750
好了，这是我们的散点图。在这里，我们在数据集中得到了观测值，并对其进行了绘制

81
00:05:18,750 --> 00:05:26,040
针对两个变量，马上第一个问题是您是否可以在视觉上快速识别

82
00:05:26,280 --> 00:05:29,220
您认为最终的最终集群。

83
00:05:29,310 --> 00:05:30,300
很难，不是吗。

84
00:05:30,300 --> 00:05:36,660
即使我们只有两个变量，也可以想象情况或挑战的复杂性

85
00:05:36,660 --> 00:05:42,180
如果我们有三个动词或五个变量，我们就无法绘制五个维度

86
00:05:42,540 --> 00:05:43,680
这样的散点图。

87
00:05:43,890 --> 00:05:49,340
这就是集群发挥作用的地方，而该算法将帮助我们

88
00:05:49,340 --> 00:05:50,850
简化过程。

89
00:05:50,850 --> 00:05:53,970
因此，让我们看看它如何发挥作用。

90
00:05:53,970 --> 00:06:00,300
因此，在这种情况下，我们实际上将手动执行相同的均值聚类算法。

91
00:06:00,400 --> 00:06:05,880
因此，通常由您选择的工具（例如我们的Python或任何其他工具）完成

92
00:06:05,880 --> 00:06:06,410
工具。

93
00:06:06,570 --> 00:06:10,800
但是在这种情况下，我们可以非常满意地使用此算法。

94
00:06:10,830 --> 00:06:13,750
我们将手动进行操作，并确切了解其工作方式。

95
00:06:13,770 --> 00:06:14,060
好吧。

96
00:06:14,070 --> 00:06:15,340
因此，让我们逐步进行。

97
00:06:15,420 --> 00:06:22,530
第1步选择簇数K，假设我们以某种方式确定了最优簇数

98
00:06:22,530 --> 00:06:24,070
群集等于2。

99
00:06:24,210 --> 00:06:27,960
再一次，我们将讨论如何在S.中找到相对数量的群集。

100
00:06:28,050 --> 00:06:32,130
但是现在让我们同意，对于这一挑战，它只是两个集群。

101
00:06:32,160 --> 00:06:32,580
好吧。

102
00:06:32,610 --> 00:06:39,870
然后是随机选择k个点的步骤，这将是聚类的质心，不一定

103
00:06:39,870 --> 00:06:40,530
从数据集中

104
00:06:40,530 --> 00:06:46,650
因此，这意味着它们可以是数据集中的实际点，也可以只是随机点

105
00:06:46,680 --> 00:06:50,000
在散点图中，选择它们的位置无关紧要。

106
00:06:50,010 --> 00:06:53,920
假设我们选择这两个质心分别是蓝色质心和红色质心。

107
00:06:54,360 --> 00:06:55,810
下一步是第三步。

108
00:06:55,830 --> 00:07:00,490
给每个点分配一个最接近的质心的点，它将形成K个簇。

109
00:07:00,630 --> 00:07:06,200
因此，基本上，对于数据集中的每个数据点，我们需要做的是确定两个质心中的哪个

110
00:07:06,200 --> 00:07:07,930
蓝色或红色是最接近的。

111
00:07:07,980 --> 00:07:11,400
因此，对于这个数据点来说，显然蓝色比红色更近。

112
00:07:11,400 --> 00:07:15,000
因此，它将为此数据点分配给蓝色群集。

113
00:07:15,000 --> 00:07:19,140
对于此数据点，蓝色再次比红色近。红色比蓝色近。

114
00:07:19,320 --> 00:07:25,150
因此，我们可以对每个数据点继续这样做，并将其分配给两个中心之一。

115
00:07:25,290 --> 00:07:31,530
但是我们将在这里使用快速技巧，我们将使用从几何学中学到的东西

116
00:07:31,530 --> 00:07:36,090
我们要做的是将这两个质心与这样的线连接起来，

117
00:07:36,090 --> 00:07:41,070
然后我们将找到光的中心，在这里，我将准确地放置一个垂直光

118
00:07:41,070 --> 00:07:42,600
通过这样的中心。

119
00:07:42,870 --> 00:07:49,980
因此，从琼斯·德鲁（Jones-Drew）那里，我们从中学的几何学中就知道了，或者也许我正在上大学，即使您没有

120
00:07:49,980 --> 00:07:56,370
具有几何形状，这是一个非常简单的概念，该绿线上的任何点都是等距的

121
00:07:56,370 --> 00:07:57,360
从蓝色和红色。

122
00:07:57,360 --> 00:08:03,110
因此，如果我采用这一点，则到蓝色质心和红色质心的距离相同。

123
00:08:03,240 --> 00:08:06,270
如果我这样说的话，它与蓝色和红色的距离相同。

124
00:08:06,270 --> 00:08:10,860
因此，整条线与其中的蓝色等距，这就是我们构造它的方式。

125
00:08:10,890 --> 00:08:17,550
因此，基于此，很明显，散点图中我们在绿线上方的任何点都是

126
00:08:17,550 --> 00:08:23,760
靠近蓝色，下面的任何点都是绿线，靠近红色，这就是

127
00:08:23,760 --> 00:08:25,390
我们将为所有点着色。

128
00:08:25,410 --> 00:08:28,400
这是给它们着色的快速方法。

129
00:08:28,590 --> 00:08:33,390
同样，您可以完全遍历每个点，并分别确定哪个质心指向哪个质心

130
00:08:33,390 --> 00:08:34,110
最接近。

131
00:08:34,200 --> 00:08:39,480
但是仅出于我们的考虑，如果我们进行此练习，它将更快

132
00:08:39,480 --> 00:08:40,540
破解方法。

133
00:08:40,710 --> 00:08:47,250
因此，在继续操作之前，我们先对质点或散点图上的点进行了着色

134
00:08:47,250 --> 00:08:52,740
在这里提到最接近的东西是非常非常的，尽管看起来很简单

135
00:08:52,740 --> 00:08:58,740
模棱两可的术语，因为当您在散点图中可视化事物时最接近

136
00:08:58,740 --> 00:09:00,350
那是距离。

137
00:09:00,390 --> 00:09:06,690
是的，但同时在数学和数据科学中，还有许多不同类型的距离

138
00:09:06,690 --> 00:09:12,030
就像我们使用的是欧几里得距离，问题是您应该使用欧几里得距离还是

139
00:09:12,030 --> 00:09:17,370
应该使用为您的挑战定义的其他距离类型，这取决于

140
00:09:17,370 --> 00:09:18,180
你来决定。

141
00:09:18,180 --> 00:09:23,490
您可以为算法指定要设置的距离类型

142
00:09:23,490 --> 00:09:24,170
使用。

143
00:09:24,480 --> 00:09:29,480
但是为了我们的缘故，将使用欧几里德距离进行说明。

144
00:09:29,670 --> 00:09:36,090
基本上，这只是两个不同点之间非常简单的简单几何距离

145
00:09:36,090 --> 00:09:36,680
点。

146
00:09:36,810 --> 00:09:41,130
因此，这里只是一个警告，如果您想进一步研究距离和其他种类

147
00:09:41,130 --> 00:09:45,750
您可以使用的距离只是要确保调查一下，因为有时会遇到一些挑战

148
00:09:45,750 --> 00:09:50,790
可能要使用其他非欧氏距离非欧氏距离是以其他特定方式定义的。

149
00:09:50,790 --> 00:09:51,060
好吧。

150
00:09:51,060 --> 00:09:54,940
考虑到这一点，让我们继续执行第四步。

151
00:09:55,190 --> 00:09:58,360
因此，对于计算和放置每个群集的新质心。

152
00:09:58,500 --> 00:10:03,540
所以基本上现在我们有了新的点，旧的蓝色的点不包括质心

153
00:10:03,810 --> 00:10:06,310
和所有红色的，不包括质心本身。

154
00:10:06,480 --> 00:10:12,120
我们需要找出蓝点的新质心和新质心在哪里

155
00:10:12,120 --> 00:10:13,920
重心为红点。

156
00:10:14,210 --> 00:10:20,100
而思考的方式是想象质心本身没有重量，而其他

157
00:10:20,100 --> 00:10:22,740
点他们有一定的分量说。

158
00:10:22,830 --> 00:10:24,540
假设每个一公斤。

159
00:10:24,630 --> 00:10:30,300
然后，您需要找到这些质心的重心或重心，并且需要

160
00:10:30,300 --> 00:10:31,970
将其精确定位在散点图上。

161
00:10:32,070 --> 00:10:35,730
因此，对于蓝色更近的物体，它将位于红色集群的某个位置。

162
00:10:35,730 --> 00:10:37,170
它会在这里的某个地方。

163
00:10:37,320 --> 00:10:43,620
在二维散点图上思考它的方式可以直观地看到

164
00:10:43,620 --> 00:10:51,090
它在哪里，或者您只需查看所有蓝点的x y坐标并找到中心

165
00:10:51,090 --> 00:10:55,580
y坐标的重力，它将在此处某处，然后对x坐标做同样的事情

166
00:10:55,590 --> 00:11:00,620
找到重心，或者像这里的X Cornus的平均值一样。

167
00:11:00,660 --> 00:11:04,210
这就是您获得红色蓝色簇的新质心的方式。

168
00:11:04,260 --> 00:11:07,520
因此，我们仅将质心移入它们。

169
00:11:07,520 --> 00:11:09,890
接下来，执行步骤5。

170
00:11:09,930 --> 00:11:14,050
因此，将每个数据点重新分配给新的最接近的质心。

171
00:11:14,280 --> 00:11:17,150
如果发生了任何重新分配，请返回到步骤4。

172
00:11:17,250 --> 00:11:21,630
否则，您可能需要完成算法，这意味着它已经收敛。

173
00:11:21,630 --> 00:11:27,870
因此，让我们来看看现在如何将数据点重新分配给新的质心。

174
00:11:28,080 --> 00:11:33,870
因此，如果我们将线穿过散点图，您会发现上面有一个点实际上是三个点

175
00:11:33,870 --> 00:11:34,440
点。

176
00:11:34,440 --> 00:11:39,530
这一点接近蓝色和红色，而这两个点实际上接近红色和红色。

177
00:11:39,540 --> 00:11:40,000
蓝色。

178
00:11:40,260 --> 00:11:42,180
所以现在我们将为它们重新着色。

179
00:11:42,510 --> 00:11:43,170
好了

180
00:11:43,170 --> 00:11:48,970
因此，现在我们有了一个新的群集，并且确实进行了一些重新分配。

181
00:11:48,990 --> 00:11:51,820
因此，我们将返回到步骤4。

182
00:11:52,140 --> 00:11:55,800
因此，我们将回到步骤4计算并为每个群集放置新的质心。

183
00:11:55,800 --> 00:11:56,280
一样。

184
00:11:56,280 --> 00:12:01,930
找到该质心的中心质量找到该质心的质量中心

185
00:12:02,040 --> 00:12:08,430
在这些位置，然后重复步骤5，将每个数据点重新分配给新的更近的质心。

186
00:12:08,580 --> 00:12:13,070
因此，我们将线穿过图表，以使两个质心之间的距离相等。

187
00:12:13,410 --> 00:12:18,390
如您所见，这一点实际上现在在蓝十字中，而不是在红色中重新分配

188
00:12:18,390 --> 00:12:19,210
点。

189
00:12:19,470 --> 00:12:26,900
接下来发生的是我们再次回到步骤4，所以如果我回到这里，您会看到该区域有人

190
00:12:26,910 --> 00:12:33,060
确实要回到步骤4，这就是我们要放置新集群的地方

191
00:12:33,150 --> 00:12:34,000
重心。

192
00:12:34,080 --> 00:12:35,240
将它们移到那里。

193
00:12:35,250 --> 00:12:40,650
重复第5步，您会看到它是一个迭代过程，它将一直这样进行，直到

194
00:12:40,650 --> 00:12:42,430
该算法收敛。

195
00:12:42,450 --> 00:12:46,340
因此，存在一条等距线，因为它可以看到需要重新分配一个点。

196
00:12:46,410 --> 00:12:47,600
它被重新分配。

197
00:12:47,730 --> 00:12:54,290
因此，现在我们回到第4步，为每个群集移动新的质心，并移动质心，

198
00:12:54,300 --> 00:12:57,900
现在分配每个数据点或重复五个。

199
00:12:58,170 --> 00:13:03,790
正如您所看到的，这次可能有远距离的线没有任何意义。

200
00:13:03,840 --> 00:13:10,230
重新分配表示他可以看到所有点已在其正确的群集中，这意味着无需重新分配

201
00:13:10,560 --> 00:13:15,600
发生在此步骤中，因此我们可以继续完成算法。

202
00:13:15,600 --> 00:13:18,150
这意味着算法已经收敛。

203
00:13:18,150 --> 00:13:18,930
所以我们走了。

204
00:13:18,930 --> 00:13:25,560
这些是我们的集群，模型已经准备就绪，所以现在我们可以删除质心和等距

205
00:13:25,560 --> 00:13:28,200
行，那是我们的最终结果。

206
00:13:28,200 --> 00:13:32,340
因此，如您所见，Kamins算法的工作原理非常直观，直接

207
00:13:32,350 --> 00:13:33,900
只是一个反复的过程。

208
00:13:34,080 --> 00:13:37,660
如果我们将其与一开始所拥有的进行比较，那就是我们所拥有的。

209
00:13:37,680 --> 00:13:41,880
如您所见，并不是很清楚如何安排克拉斯派。

210
00:13:41,880 --> 00:13:47,100
例如，您可能以为这可能是一个集群，而这可能是

211
00:13:47,100 --> 00:13:52,110
一个群集，或者可能是该底部是群集，而在顶部是群集，但基于

212
00:13:52,110 --> 00:13:59,220
在Kamins算法上，我们得到的结果是这两个集群，因此每个集群中的点

213
00:13:59,220 --> 00:14:05,310
彼此并不完全相似，但这就是K表示算法所希望的

214
00:14:05,340 --> 00:14:06,520
喜欢故事情节。

215
00:14:06,600 --> 00:14:13,350
希望现在我们已经揭开了Kamins算法的神秘面纱，并分解为简单的术语，

216
00:14:13,350 --> 00:14:15,200
期待看到一个教程。

217
00:14:15,210 --> 00:14:17,130
在此之前确保机器学习

