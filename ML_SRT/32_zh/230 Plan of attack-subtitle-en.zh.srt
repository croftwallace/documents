1
00:00:00,360 --> 00:00:04,170
您好，欢迎来到卷积神经网络这一节。

2
00:00:04,170 --> 00:00:07,600
非常高兴您也加入本节。

3
00:00:07,740 --> 00:00:13,650
今天，我们将取消进攻计划，我们将如何学习本节中的所有内容。

4
00:00:13,650 --> 00:00:15,630
有很多东西要学。

5
00:00:15,660 --> 00:00:17,490
让我们看看我们将如何处理这个问题。

6
00:00:17,490 --> 00:00:19,590
好的，我们在本节中将学到什么。

7
00:00:19,590 --> 00:00:25,140
首先，我们将讨论哪些卷积网络实际上非常重要

8
00:00:25,140 --> 00:00:28,770
在实际开始努力之前要实现的最终目标。

9
00:00:28,770 --> 00:00:32,090
因此，我们将听一些示例来了解哪些功能。

10
00:00:32,090 --> 00:00:36,750
将在图像识别方面将人脑与人工神经网络进行比较。

11
00:00:36,840 --> 00:00:42,630
因此，这将是一个有趣的轻型教程，帮助我们开始整个部分。

12
00:00:42,630 --> 00:00:44,810
然后，我们将讨论步骤1。

13
00:00:44,850 --> 00:00:47,410
直接跳入其中可以进行滚动操作。

14
00:00:47,420 --> 00:00:56,280
因此，本课程的这一部分包含几个步骤，我们需要完成这些步骤才能构建卷积

15
00:00:56,310 --> 00:00:59,040
神经网络，这就是这些内部结构将如何分解的方式。

16
00:00:59,040 --> 00:01:06,510
因此，这将是进化操作的第一步，我们将学习有关特征检测器的所有知识。

17
00:01:06,510 --> 00:01:08,790
我们将讨论哪些也是过滤器。

18
00:01:08,790 --> 00:01:11,030
我们将讨论未来的地图。

19
00:01:11,070 --> 00:01:16,110
而且您知道不同的参数分别是什么意思，并了解一些视觉效果。

20
00:01:16,110 --> 00:01:17,360
例子。

21
00:01:17,520 --> 00:01:26,010
然后，我们将讨论步骤1 B部分Lehre或真正分层的领域，然后将其精简

22
00:01:26,130 --> 00:01:34,830
单元，我们将讨论为什么线性度不好，以及如何在网络中增加非线性度

23
00:01:34,830 --> 00:01:36,700
用于图像识别。

24
00:01:36,870 --> 00:01:42,420
然后，我们将讨论步骤2的池化，并将了解池化的工作方式，我们将专门讨论

25
00:01:42,420 --> 00:01:48,960
最大池化，我们还提到了一些有关池化或某些池化和其他方法的信息

26
00:01:48,960 --> 00:01:51,960
您可以进行合并的过程。

27
00:01:51,960 --> 00:01:58,650
同样在本讲座中，我们将有一个非常酷的示例，因此将有一个非常直观的交互式工具

28
00:01:58,650 --> 00:01:59,730
我们将要研究的。

29
00:01:59,730 --> 00:02:05,190
因此，请务必坚持到该课程的结尾，因为这将为您增加很多价值

30
00:02:05,190 --> 00:02:06,810
您的学习过程。

31
00:02:06,810 --> 00:02:09,110
我们将在此处最后讨论什么。

32
00:02:09,120 --> 00:02:10,650
第三步展平。

33
00:02:10,680 --> 00:02:16,290
因此，在这里，我们将成为一个快速教程，介绍如何从池化层继续到

34
00:02:16,380 --> 00:02:19,560
压平巢穴，然后我们将讨论完全连接。

35
00:02:19,560 --> 00:02:26,130
这是一个非常丰富的教程，将所有内容放在一起，并把所有内容都放在透视图上，

36
00:02:26,130 --> 00:02:29,680
实际上向您展示了一切。

37
00:02:29,700 --> 00:02:35,100
归根结底，这些最终的神经元如何理解如何对Umich进行分类非常重要。

38
00:02:35,100 --> 00:02:42,340
Tauriel并希望能为您总结或整理所有内容。

39
00:02:42,540 --> 00:02:46,260
最后，我们将获得一个摘要，其中将总结我们所讨论的所有内容。

40
00:02:46,560 --> 00:02:52,260
作为一个额外的小功能，我包括了一个关于软最大和交叉熵的教程。

41
00:02:52,260 --> 00:02:57,780
因此，您不必参加本教程，但我认为这将是对知识的极大补充，因为

42
00:02:57,780 --> 00:03:02,010
这些是您在处理卷积神经网络时会遇到的术语。

43
00:03:02,070 --> 00:03:08,130
因此，也许马上采取行动，也许当您遇到这些术语时，您将永远知道

44
00:03:08,130 --> 00:03:13,800
您可以返回本课程并学习本教程，以更好地了解什么是Max和cross

45
00:03:13,800 --> 00:03:14,530
熵是。

46
00:03:14,760 --> 00:03:20,610
而且像往常一样，在这些社论中，会有很多推荐的阅读材料供您阅读

47
00:03:20,670 --> 00:03:23,100
进一步升级并获得更多知识。

48
00:03:23,370 --> 00:03:28,250
而且，我迫不及待想在第一个教程中看到它，这将非常有趣和令人兴奋。

49
00:03:28,310 --> 00:03:31,260
S.直到下一次享受深度学习。

