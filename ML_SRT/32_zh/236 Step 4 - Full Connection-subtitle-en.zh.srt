1
00:00:00,520 --> 00:00:02,800
您好，欢迎回到本课程和深度学习。

2
00:00:02,800 --> 00:00:06,140
今天，在完全连接之前，我们终于要进入STEPNELL。

3
00:00:06,310 --> 00:00:08,210
那么，这一步到底是怎么回事。

4
00:00:08,440 --> 00:00:16,990
好吧，在这一步中，我们将整个人工神经网络添加到卷积神经网络中

5
00:00:17,000 --> 00:00:22,450
因此，到目前为止，我们已经完成了所有工作，包括卷积池和展平。

6
00:00:22,510 --> 00:00:24,910
现在，我们添加了一个全新的页面。

7
00:00:24,930 --> 00:00:28,990
然后，在那背后是那么强烈。

8
00:00:28,990 --> 00:00:32,350
那就是那绝对是某事。

9
00:00:32,590 --> 00:00:36,730
所以在这里，我们有了输入层，有了完整的连接计划。

10
00:00:36,740 --> 00:00:42,580
我将在那里放置，顺便说一下，我们以前在人工神经网络中使用完全连接的Lehre

11
00:00:42,580 --> 00:00:47,650
称它们为隐藏层，这里我们称它们为完全连通，因为它们是隐藏的巢穴，但

12
00:00:47,650 --> 00:00:53,500
同时，它们是更特定类型的提琴手，它们在人工神经网络中完全相连

13
00:00:53,500 --> 00:00:57,510
网络隐藏的字母不必完全连接。

14
00:00:57,520 --> 00:01:02,230
而在卷积神经网络中，我们将使用完全连接的字母，这就是

15
00:01:02,230 --> 00:01:05,640
为什么它们通常被称为完全互联劳拉。

16
00:01:05,770 --> 00:01:11,620
基本上，经过整平后，我们传递的整个输出列或向量

17
00:01:11,620 --> 00:01:18,160
在这里学习到的输入中，我们得到了一个非常简化的示例，仅用于说明目的。

18
00:01:18,160 --> 00:01:26,770
人工神经网络的主要目的是将我们的特征组合成更多的属性

19
00:01:26,770 --> 00:01:28,960
可以预言克拉斯会更好。

20
00:01:28,960 --> 00:01:37,660
因此，我们已经将输出结果向量平展化为实际结果

21
00:01:37,660 --> 00:01:43,750
完成后，我们就在该向量的数字中编码了一些特征，它们可能已经可以

22
00:01:43,750 --> 00:01:51,730
在预测我们要看的克劳斯是什么方面的工作非常出色

23
00:01:51,730 --> 00:01:53,840
它是肿瘤还是不是肿瘤等等。

24
00:01:53,890 --> 00:02:00,610
但同时我们知道我们拥有称为人工神经网络的结构

25
00:02:00,610 --> 00:02:07,810
其目的是处理属性并显示或处理特征并显示

26
00:02:07,810 --> 00:02:16,120
新属性并将属性组合在一起，以更好地预测我们正在尝试的事情

27
00:02:16,120 --> 00:02:20,360
进行预测，而我们从前面的部分知道这一点，那么为什么不利用它。

28
00:02:20,440 --> 00:02:22,750
这正是这里的计划。

29
00:02:22,750 --> 00:02:29,140
那么我们如何将这些值传递到人工神经网络中，并使其进一步优化

30
00:02:29,140 --> 00:02:30,350
我们正在做的一切。

31
00:02:30,640 --> 00:02:31,900
这就是我们要做的。

32
00:02:31,900 --> 00:02:36,390
但是让我们看一个更现实的例子，因为这个例子太简单了。

33
00:02:36,610 --> 00:02:43,990
因此，这里有一个外观更好的人工神经网络，其中输入有五个属性

34
00:02:43,990 --> 00:02:51,040
除非我们在第二个或第二个完全连接的神经元中有六个神经元

35
00:02:51,040 --> 00:02:55,510
拉里有八个神经元，然后我们有两个输出，一个用于狗，一个用于猫。

36
00:02:55,630 --> 00:03:02,240
因此，让我们在这里谈论的一件重要事情是，为什么我们有两个输出。

37
00:03:02,240 --> 00:03:09,100
我们习惯于在人工神经网络中只有一个输出，而其中一个输出用于

38
00:03:09,100 --> 00:03:14,740
在运行回归类型时在打印时预测数值的类型

39
00:03:14,740 --> 00:03:15,480
问题。

40
00:03:15,760 --> 00:03:22,840
但是，当您进行分类时，您需要一个输出Proclus，但例外情况是您

41
00:03:22,840 --> 00:03:27,940
只有两个集群，就像我们在这里有两个类，狗和猫，我们可以只完成一个输出

42
00:03:27,970 --> 00:03:33,790
并使其成为二进制输出，并说一个是一只狗，将一只猫清零，那将完全起作用

43
00:03:33,790 --> 00:03:38,760
很好，实际上，您将在午餐中看到的午餐是在实际教程中做到的，这就是他们的做法

44
00:03:38,770 --> 00:03:39,250
结构化。

45
00:03:39,250 --> 00:03:46,090
但是同时如果您有两个以上的类别，例如狗，猫和鸟，那么您

46
00:03:46,090 --> 00:03:52,420
每个类别都必须有一个神经元，这就是为什么我们要练习两个类别

47
00:03:52,420 --> 00:03:58,320
在此示例中，这样我们就知道如果我们拥有两个以上的类别会期望什么。

48
00:03:58,550 --> 00:04:00,010
因此，这里将要发生的事情。

49
00:04:00,010 --> 00:04:05,260
因此，我们已经完成了所有基础工作，进行了卷积，并进行了合并和展平

50
00:04:05,620 --> 00:04:10,570
现在信息将通过人工神经网络传递，让我们看一下

51
00:04:10,570 --> 00:04:12,300
其他一切如何发生。

52
00:04:12,340 --> 00:04:18,460
从处理图像的那一刻起就存在着信息，

53
00:04:18,610 --> 00:04:23,920
卷积的卷积卷积然后压平，然后通过人工神经网络将所有四个

54
00:04:23,920 --> 00:04:30,720
步骤，然后进行预测，我们将很快看到这种情况将非常有趣。

55
00:04:30,730 --> 00:04:32,920
但是现在我们只能说做出了预测。

56
00:04:32,920 --> 00:04:36,070
例如，有80％的狗是狗。

57
00:04:36,070 --> 00:04:40,610
但事实证明，这只是一只猫，然后计算出一个误差。

58
00:04:40,610 --> 00:04:40,990
一种。

59
00:04:41,200 --> 00:04:47,720
好吧，我们以前在人工神经网络中称成本成本函数为

60
00:04:47,740 --> 00:04:51,460
错误或常见的错觉神经网络。

61
00:04:51,460 --> 00:04:57,630
这称为损失函数，为此我们使用了交叉熵函数。

62
00:04:57,640 --> 00:04:59,870
我们将讨论交叉熵和均方误差。

63
00:05:00,130 --> 00:05:02,820
在单独的教程中以及如何进行所有操作。

64
00:05:02,820 --> 00:05:08,730
但就您所知，您说我们缺少一种功能，它告诉我们网络的运行状况如何

65
00:05:08,730 --> 00:05:13,560
我们正在尝试对其进行优化或对其进行最小化，以优化我们的网络。

66
00:05:13,750 --> 00:05:19,470
这样就可以计算出误差，然后将其通过网络反向传播，就像在人工环境中一样

67
00:05:19,470 --> 00:05:26,700
神经网络向后传播，并调整网络中的某些内容以帮助优化

68
00:05:27,000 --> 00:05:31,670
性能和需要调整的事物通常是人工神经网络中的权重

69
00:05:31,670 --> 00:05:34,910
是您在此处看到的蓝线中的蓝线的一部分。

70
00:05:35,340 --> 00:05:43,950
然后还要调整的另一件事是特征检测器，因此我们知道我们正在寻找

71
00:05:43,950 --> 00:05:46,140
功能，但是如果我们正在寻找错误的功能该怎么办。

72
00:05:46,140 --> 00:05:51,570
如果由于特征不正确而无法解决该问题，那该记住的特征检测器怎么办？

73
00:05:51,570 --> 00:05:53,860
我们拥有的那些小矩阵。

74
00:05:54,250 --> 00:05:57,270
那就是三乘三矩阵。

75
00:05:57,270 --> 00:06:03,240
他们进行了调整，以便下次可能会更好，让我们看看会发生什么。

76
00:06:03,360 --> 00:06:03,860
事物的类型。

77
00:06:03,870 --> 00:06:11,040
但是当然，在很多数学的背景下，这一切都是通过大量科学完成的，而这一切

78
00:06:11,040 --> 00:06:14,580
通过反向传播的梯度梯度下降来完成。

79
00:06:14,580 --> 00:06:20,880
因此，这不仅是随机扰动，还包括如何进行操作的真正思考。

80
00:06:21,210 --> 00:06:27,630
但是尽管如此，特征检测器仍已调整，重量也已调整，整个过程

81
00:06:27,630 --> 00:06:30,710
再次发生，然后错误再次传播回来。

82
00:06:30,720 --> 00:06:32,610
而且，这种情况一直持续下去。

83
00:06:32,760 --> 00:06:37,950
这就是我们的网络如何优化，这就是我们的网络如何训练数据的方式。

84
00:06:37,950 --> 00:06:43,800
因此，重要的是数据从一开始到整个过程都遍历整个区域。

85
00:06:43,800 --> 00:06:44,410
最后。

86
00:06:44,430 --> 00:06:49,950
然后比较误差，以便计算误差，然后将其反向传播。

87
00:06:49,950 --> 00:06:56,520
因此，与人工神经网络相同的故事要稍长一点，因为首先

88
00:06:56,520 --> 00:06:58,320
我们已经有了三个步骤。

89
00:06:59,040 --> 00:07:04,440
现在让我们看一下有趣的部分，真正有趣的部分是这两个类如何

90
00:07:04,440 --> 00:07:10,050
之所以起作用，是因为这两个输出神经元是如何工作的，因为在我们总有一个输出之前

91
00:07:10,050 --> 00:07:11,840
神经元当我们有两个时会发生什么。

92
00:07:11,840 --> 00:07:17,490
这种分类或图像的情况如何表现出来。

93
00:07:17,670 --> 00:07:21,610
好吧，让我们先从顶级神经元开始，先从狗开始。

94
00:07:22,080 --> 00:07:28,950
我们主要的目的是如何做的首先我们需要了解要分配的权重

95
00:07:28,950 --> 00:07:36,000
所有与狗有关的课程提要，以便我们知道以前的哪些神经元是

96
00:07:36,000 --> 00:07:38,910
实际上对狗很重要，让我们看看如何做到。

97
00:07:38,910 --> 00:07:46,460
因此，假设我们在先前完全连接的上一层获得了这些数字。

98
00:07:46,500 --> 00:07:47,980
在最后的完全连接层中。

99
00:07:48,120 --> 00:07:51,010
同样，这些数字绝对可以是任何数字。

100
00:07:51,030 --> 00:07:56,490
他们不必一定是任何数字，但仅出于论证的目的，我们将同意

101
00:07:56,490 --> 00:08:01,890
我们专门查看0到1之间的数字。

102
00:08:02,280 --> 00:08:09,840
因此，我们对这些事情进行辩论和理解更容易，并且这意味着神经元非常自信

103
00:08:09,840 --> 00:08:15,960
它找到了这个功能，零表示神经元没有找到一个功能

104
00:08:15,960 --> 00:08:23,580
之所以这样，是因为在一天结束时，这些神经元就像这个左侧的其他东西一样

105
00:08:23,610 --> 00:08:25,470
只看图像的特征。

106
00:08:25,470 --> 00:08:27,490
这已经是非常非常的过程。

107
00:08:27,510 --> 00:08:32,940
但是在我们开始之前，它仍然在检测图像上的某个特征或特征组合

108
00:08:33,700 --> 00:08:34,590
可以进化的一步。

109
00:08:34,590 --> 00:08:39,060
我们在池集中有一些可识别的功能，它们比甚至变得不易识别

110
00:08:39,060 --> 00:08:40,850
在展平的图像中难以识别。

111
00:08:40,850 --> 00:08:42,550
然后他们合并起来，依此类推。

112
00:08:42,570 --> 00:08:48,720
但是，尽管如此，我们在这里谈论的是存在图像或它们的组合的某些特征。

113
00:08:48,720 --> 00:08:54,480
因此，一个已经通过并且很重要的动物已经传递给了狗和猫

114
00:08:54,480 --> 00:08:57,020
同时输出到两个神经元。

115
00:08:57,150 --> 00:09:06,180
因此，对于我们来说，这意味着对于我们的论证，这意味着该神经元已经激发了它的存在。

116
00:09:06,180 --> 00:09:11,850
快速检测您知道可能是眉毛的特征，可能正在检测此眉毛

117
00:09:11,870 --> 00:09:15,170
为了简单起见，再次检测该眉毛。

118
00:09:15,270 --> 00:09:20,310
并传达给狗奔向猫神经元，说我可以看到我的眉毛，我可以看到我的眉毛。

119
00:09:20,310 --> 00:09:25,240
然后，取决于狗和猫的神经元对他们意味着什么。

120
00:09:25,290 --> 00:09:25,860
对。

121
00:09:25,890 --> 00:09:30,840
因此，在这种情况下，哪些神经元正在激发这三个神经元在激发眉毛，

122
00:09:30,830 --> 00:09:36,120
说鼻子就是说我可以看见我可以看见一个大鼻子，可以看见我的软盘耳朵。

123
00:09:36,270 --> 00:09:40,540
因此，它的意思是对狗，对猫，然后对狗。

124
00:09:40,560 --> 00:09:43,390
然后发生的事情是我们知道这是一只狗。

125
00:09:43,440 --> 00:09:49,920
所以狗神经元知道答案是它实际上是狗，因为最后我们将

126
00:09:49,920 --> 00:09:53,640
图片或图片上的标签以及当另一只狗时。

127
00:09:53,640 --> 00:09:56,310
所以基本上狗神经元会说Aha。

128
00:09:56,310 --> 00:09:58,820
因此，在这种情况下应该触发我。

129
00:09:58,830 --> 00:10:04,790
这些是神经元，它们告诉这个信号，它们既向我发送给狗，也向我发送。

130
00:10:04,790 --> 00:10:09,000
猫实际上对我来说表明它是狗。

131
00:10:09,020 --> 00:10:13,940
在这很多很多的重复过程中，狗会多次学习

132
00:10:13,940 --> 00:10:19,580
当特征属于狗时，这些神经元确实会触发。

133
00:10:19,670 --> 00:10:24,260
另一方面，猫的神经元会知道它不是猫，并且它将知道此功能是

134
00:10:24,260 --> 00:10:28,210
激发起来，这个神经元告诉我它可以看到懒散的耳朵。

135
00:10:28,370 --> 00:10:31,040
但同时它也不是猫。

136
00:10:31,040 --> 00:10:36,980
所以基本上对我来说，这是一个信号，我应该像这样忽略这个神经元，并且发生的更多

137
00:10:36,980 --> 00:10:41,960
猫的神经元将更多地忽略这个关于松软耳朵的神经元。

138
00:10:42,440 --> 00:10:49,100
因此，基本上，如果这种情况经常发生，那就是如何进行大量迭代的方法。

139
00:10:49,100 --> 00:10:54,170
因此，这只是一个例子，但是如果这种情况经常发生，则可能是一个0.8或0.9也许有时

140
00:10:54,170 --> 00:11:02,090
起火，但总体上来说，当确实是一只狗时，该神经元经常发光。

141
00:11:02,090 --> 00:11:05,920
将开始将更高的重要性归因于该神经元。

142
00:11:05,930 --> 00:11:06,590
因此，我们去了。

143
00:11:06,590 --> 00:11:08,430
这就是我们要表示的方式。

144
00:11:08,450 --> 00:11:14,570
我们要说的是，这三个神经元与我一起经历了许多这样的迭代过程

145
00:11:14,570 --> 00:11:20,210
许多样本，许多样本都记住了，因此样本是数据集中的一行，Apoc就是您需要的时间

146
00:11:20,210 --> 00:11:25,150
遍历整个数据集，一次又一次地有很多迭代。

147
00:11:25,220 --> 00:11:34,010
这只狗的神经元了解到这是眉毛神经元，大鼻子神经元和懒散的耳朵神经元

148
00:11:34,340 --> 00:11:43,040
它们似乎都为寻找的东西和哪个东西的分类做出了很大的贡献。

149
00:11:43,040 --> 00:11:44,350
是一条狗。

150
00:11:44,480 --> 00:11:45,730
这就是它的工作方式。

151
00:11:45,740 --> 00:11:55,130
再一次，这些耳朵，鼻子和眉毛非常近似或牵强

152
00:11:55,130 --> 00:12:01,640
例子，因为到了这个阶段，在整个卷积常规神经网络中

153
00:12:01,640 --> 00:12:07,400
无法识别他们在寻找什么，但同时这也是狗的特征

154
00:12:07,400 --> 00:12:09,020
或猫或您对其进行分类的任何内容。

155
00:12:09,410 --> 00:12:11,130
然后，让我们继续下一个。

156
00:12:11,150 --> 00:12:15,860
现在我们要看猫的神经元，但是这些我们要记住，这些重量就是你

157
00:12:15,860 --> 00:12:17,900
知道我们如何整理狗。

158
00:12:17,900 --> 00:12:22,970
所以狗有点像是忽略了所有其他所有的神经元一二三四或五，但是

159
00:12:22,970 --> 00:12:26,510
确实要注意这三个神经元在说什么。

160
00:12:26,570 --> 00:12:28,330
现在猫在听什么。

161
00:12:28,490 --> 00:12:30,830
好吧，只要它实际上是一只猫。

162
00:12:30,970 --> 00:12:32,530
对。

163
00:12:32,710 --> 00:12:35,600
这是当它实际上是一只猫时的一种情况。

164
00:12:35,600 --> 00:12:42,980
因此，您将看到这三个神经元0.9 0.9，其中一个在说他们在说的话

165
00:12:42,980 --> 00:12:44,590
狗和猫都有东西。

166
00:12:44,600 --> 00:12:49,510
这一点同样重要，请记住，因此该输出信号在两种情况下都是正确的。

167
00:12:49,520 --> 00:12:55,520
是对狗对猫说一个，然后由狗对猫决定是否

168
00:12:55,520 --> 00:13:00,220
考虑到该信号并从中学习。

169
00:13:00,500 --> 00:13:05,810
狗和猫都可以看到这是一张照片，我应该在这里放一张猫的照片，但是

170
00:13:05,810 --> 00:13:10,030
基本上可以想象一张猫的照片，狗和猫都可以看到这实际上是猫。

171
00:13:10,190 --> 00:13:20,150
所以基本上狗是可以的，所以这些胡须，尖尖的三角形耳朵和小巧的狗

172
00:13:20,420 --> 00:13:28,250
是的，或者也许是这种类型的，您知道猫眼中的东西如何，它们的眼睛就像小孩子一样

173
00:13:28,310 --> 00:13:33,350
它们不是圆圈，线条或猫眼之类的东西。

174
00:13:33,350 --> 00:13:37,460
基本上，这些猫眼肯定对我不起作用。

175
00:13:37,460 --> 00:13:42,980
他们不会帮助我预测，因为每次这些神经元点亮时，预测并不会

176
00:13:42,980 --> 00:13:44,240
我在寻找什么。

177
00:13:44,240 --> 00:13:46,910
另一方面，猫就像嗯，很有趣。

178
00:13:46,910 --> 00:13:51,620
每次这些点亮时，大多数时候都会点亮。

179
00:13:51,620 --> 00:13:55,310
它符合我的期望，符合我的期望。

180
00:13:55,310 --> 00:13:55,630
好。

181
00:13:55,640 --> 00:13:58,050
我要听的比这更多。

182
00:13:58,160 --> 00:14:02,710
每次点亮或在大多数情况下点亮，都是同一件事。

183
00:14:02,810 --> 00:14:09,100
我碰巧得到一个好东西，因为碰巧做对了，所以我因我的预测而得到了回报。

184
00:14:09,110 --> 00:14:09,760
它是猫。

185
00:14:09,770 --> 00:14:10,080
好。

186
00:14:10,130 --> 00:14:11,440
我要多听他的话。

187
00:14:11,450 --> 00:14:17,930
你知道这对我没用，因为他实际上不是你，你知道像他是他甚至都没有照明

188
00:14:17,930 --> 00:14:21,040
上面是一只猫，但那是他没有发光，所以情况正相反。

189
00:14:21,050 --> 00:14:24,410
这个人很好，是个cad，但他不肯放弃，所以我不会听他的。

190
00:14:24,410 --> 00:14:31,250
但是当他走到那只猫的眼睛上时，这是什么？我们可以看到，我可以看到，

191
00:14:31,250 --> 00:14:31,850
一只猫。

192
00:14:31,850 --> 00:14:36,440
它在大多数情况下都匹配，所以我将从中学习，然后听这三个家伙的讲话

193
00:14:36,980 --> 00:14:38,750
经常比没有。

194
00:14:38,750 --> 00:14:44,810
因此，基本上，猫在听这三个，而忽略了其他五个，这就是

195
00:14:45,350 --> 00:14:54,830
这些最终神经元了解最终完全连接的Lehre中的哪些神经元，以收听输出的神经元

196
00:14:54,830 --> 00:14:58,460
了解哪个完全连接是最终的完全连接。

197
00:14:58,670 --> 00:15:00,030
有神经元要听。

198
00:15:00,180 --> 00:15:02,530
他们就是这样理解的。

199
00:15:02,790 --> 00:15:08,930
基本上，这就是特征如何通过网络传播并传递到输出的方式。

200
00:15:08,970 --> 00:15:14,070
因此，即使这些功能对它们没有太大的意义，例如松软的耳朵或

201
00:15:14,070 --> 00:15:14,900
晶须。

202
00:15:15,210 --> 00:15:21,860
同时它们确实具有一些独特之处，它们是该特定类别的独特特征

203
00:15:21,870 --> 00:15:27,270
这就是训练网络的方式，因为我们在回传过程中还记得

204
00:15:27,270 --> 00:15:33,750
我们还调整了特征检测器，因此，如果某个特征对输出无用，它将用于该输出

205
00:15:33,750 --> 00:15:39,600
可能会被忽略，因为在一两个故事中不会发生这种情况

206
00:15:39,600 --> 00:15:41,000
和数千次迭代。

207
00:15:41,040 --> 00:15:46,620
因此随着时间的流逝，对网络无用的功能将被忽略，并被功能取代

208
00:15:46,620 --> 00:15:52,830
很有用，所以到了最后，在神经元的最后这一层中，您可能会有很多

209
00:15:53,070 --> 00:15:59,730
影像中确实可以代表或描述影像的特征或特征组合

210
00:15:59,730 --> 00:16:01,320
狗和猫。

211
00:16:01,710 --> 00:16:06,660
因此，一旦对您的网络进行了培训，我们便可以应用它。

212
00:16:06,660 --> 00:16:09,340
因此，这是下一步，就像我们经过培训会网络一样。

213
00:16:09,350 --> 00:16:13,020
让我们看看应用此网络时会发生什么。

214
00:16:13,020 --> 00:16:15,660
假设我们传递了一条狗的图像。

215
00:16:16,410 --> 00:16:20,340
这些值通过网络传播，我们得到了某些值。

216
00:16:20,610 --> 00:16:26,880
因此，这次狗和猫的神经元不知道它们在这里没有狗的形象，

217
00:16:26,880 --> 00:16:28,470
不知道它是狗还是猫。

218
00:16:28,470 --> 00:16:35,380
他们不知道它是什么，但是他们已经学会了听这里显示的内容。

219
00:16:35,380 --> 00:16:35,660
对。

220
00:16:35,670 --> 00:16:40,440
他们学会了听狗，听这三个神经元，猫神经听这些

221
00:16:40,440 --> 00:16:40,910
三。

222
00:16:40,950 --> 00:16:44,850
因此，狗的神经元看着一二三，哈哈，这些都很高。

223
00:16:44,940 --> 00:16:50,430
因此，我的可能性很高，那就是猫神经元看着这三只狗说“好”

224
00:16:50,470 --> 00:16:53,670
这些这个很高，但是很低。

225
00:16:53,670 --> 00:16:54,320
有趣。

226
00:16:54,320 --> 00:16:56,990
所以我的机率是0.05。

227
00:16:57,130 --> 00:16:58,950
然后是。

228
00:16:58,980 --> 00:17:00,110
这就是您得到预测的地方。

229
00:17:00,120 --> 00:17:04,490
因此，此神经网络的首选是狗。

230
00:17:04,500 --> 00:17:06,900
第二选择是猫，差不多就是它了。

231
00:17:06,900 --> 00:17:11,690
因此答案是狗，当您传递猫的图片时，也会发生同样的事情。

232
00:17:11,910 --> 00:17:16,580
您获得了新的价值，并且您可以看到，即使这个价值很高，这些价值也很低。

233
00:17:16,770 --> 00:17:20,560
对于猫来说，这很高，这很高，这有点低。

234
00:17:20,670 --> 00:17:25,850
因此，这里的可能性可能不如以前大，但您仍然可以看到它是

235
00:17:25,860 --> 00:17:26,810
79％。

236
00:17:26,940 --> 00:17:30,230
因此，神经网络将投票认为它是猫。

237
00:17:30,270 --> 00:17:33,240
因此，基本上所有的神经网络都会得出结论，那就是猫。

238
00:17:33,330 --> 00:17:40,710
投票是这些人使用的术语，因此这些神经元在最终完全连接的Lehre中

239
00:17:40,710 --> 00:17:41,510
投票。

240
00:17:41,520 --> 00:17:42,810
这些是他们的选票。

241
00:17:42,870 --> 00:17:47,160
再一次，我们只是出于论证的缘故，将值放在0到1之间。

242
00:17:47,160 --> 00:17:54,480
这些值可以是任何值，但它们可以投票，然后这些权重才是投票的重要性。

243
00:17:54,480 --> 00:18:00,540
这就是这些紫色的重量，它们是狗神经元如何看待他们的选票的。

244
00:18:00,540 --> 00:18:04,820
它赋予这些神经元和那些票多少重要。

245
00:18:04,830 --> 00:18:12,810
这就是猫的神经元大小对这些神经元的票数和

246
00:18:12,810 --> 00:18:18,840
因此，这些神经元会根据所学的体重来决定要听谁的声音，从而对猫狗进行投票

247
00:18:18,840 --> 00:18:23,490
然后他们做出预测，然后通过神经网络得出结论，这就是

248
00:18:23,490 --> 00:18:29,190
一只猫，然后是那然后是你的结论，这就是你如何获得这样的图像的地方

249
00:18:29,490 --> 00:18:37,200
你有一个猎豹，然后你有一个非常高概率的猎豹爪，所以这

250
00:18:37,200 --> 00:18:40,080
您知道网络预测的概率吗？

251
00:18:40,080 --> 00:18:44,430
这些是法律，但它们仍然存在，因为它们仍然像是很小的机会

252
00:18:44,430 --> 00:18:49,710
神经元也在听选民的声音，他们在说哦，也许这实际上是豹子和

253
00:18:49,710 --> 00:18:50,580
子弹头列车。

254
00:18:50,580 --> 00:18:51,400
非常非常可能。

255
00:18:51,400 --> 00:18:52,470
我听到剪刀了。

256
00:18:52,470 --> 00:18:57,600
您知道这一个，但是手持玻璃杯是紧随其后的五分便士听诊器，因为您

257
00:18:57,600 --> 00:19:03,960
可以看到像这个家伙这样的神经元剪刀神经元输出系列神经元听了

258
00:19:03,960 --> 00:19:07,070
选民，它在总体上具有主导地位。

259
00:19:07,080 --> 00:19:10,190
但是随后，手杯也取得了不错的成绩。

260
00:19:10,200 --> 00:19:16,450
因此，我们开始了完整连接的工作方式以及所有这些如何一起发挥作用。

261
00:19:16,680 --> 00:19:18,810
希望您喜欢今天的教程。

262
00:19:18,810 --> 00:19:21,320
我们还将在摘要中总结所有这些内容。

263
00:19:21,420 --> 00:19:22,860
我下次见。

264
00:19:22,860 --> 00:19:24,720
在此之前，请享受深度学习。

