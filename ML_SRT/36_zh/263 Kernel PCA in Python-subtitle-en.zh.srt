1
00:00:00,240 --> 00:00:02,710
您好，欢迎来到本Python教程。

2
00:00:02,790 --> 00:00:06,360
因此，现在我们知道了如何实现特征提取技术。

3
00:00:06,360 --> 00:00:11,690
这些是PCa和LDA，但是这些在Lynnie上起作用的特征提取技术是有问题的。

4
00:00:11,700 --> 00:00:14,460
那就是数据是线性可分离的。

5
00:00:14,670 --> 00:00:19,380
在本节中，我们将看到一种新的特征提取技术，但这一次适用于

6
00:00:19,380 --> 00:00:23,260
数据不可线性分离的非线性问题。

7
00:00:23,280 --> 00:00:30,090
因此，该技术称为上校PCA内核PCa是PCa的内核生存版本，我们将

8
00:00:30,090 --> 00:00:32,760
使用内核技巧将数据扩展到更高的维度。

9
00:00:32,790 --> 00:00:37,470
然后从那里提取一些新的主要成分，我们将看到它如何管理

10
00:00:37,470 --> 00:00:39,620
处理非线性问题。

11
00:00:39,690 --> 00:00:44,950
因此，我们不会像上一节那样处理一个数据上的相同问题

12
00:00:44,950 --> 00:00:50,280
设置，但我们将使用与第3部分分类中使用的相同的数据集，因为现在

13
00:00:50,280 --> 00:00:51,080
我们需要视觉效果。

14
00:00:51,120 --> 00:00:58,320
我们需要清楚地看到发生了什么，我们需要看到当前的PCA如何设法提取一些新的独立

15
00:00:58,320 --> 00:01:02,300
即使问题是非线性的，也可以改变主成分。

16
00:01:02,310 --> 00:01:08,250
那就是当数据不是线性可分离的并且我们在第3部分中使用的这个数据集是社交网络时

17
00:01:08,280 --> 00:01:09,500
添加数据集。

18
00:01:09,630 --> 00:01:15,720
记住，这显然是一个非线性问题，因为非线性分类器表现出更好的性能。

19
00:01:15,720 --> 00:01:21,370
因此，让我们采用该数据集，并应用Chornobyl PCA来看看它将如何处理被提名人。

20
00:01:21,390 --> 00:01:27,530
因此，让我们在第一个文件资源管理器中找到该数据集，然后我们运行它的机器就是那个文件夹，然后

21
00:01:27,530 --> 00:01:32,680
端口9降维和第45部分内核。

22
00:01:32,990 --> 00:01:38,510
如您所见，我们发现数据集社交网络拥挤，就像我们在第3部分中发现的一样。

23
00:01:38,550 --> 00:01:40,370
确保文件夹中有它。

24
00:01:40,500 --> 00:01:42,960
如果是这样的话，您就准备好了。

25
00:01:43,350 --> 00:01:49,530
现在我们要做的是采用这个逻辑回归模型，因为您知道这个逻辑

26
00:01:49,530 --> 00:01:55,170
回归模型是线性分类器，因此不适合我们的问题，因为

27
00:01:55,380 --> 00:01:57,470
我们的数据不是线性可分离的。

28
00:01:57,480 --> 00:02:04,530
所以我们要做的是在这里进行线性分类，但是我们将在内部应用内核PCA

29
00:02:04,530 --> 00:02:08,850
看看内核PCA将如何保存情况。

30
00:02:08,850 --> 00:02:13,920
因此，即使我们应用线性模型，您也会看到嗯，感谢我们管理的PCa上校

31
00:02:13,920 --> 00:02:19,280
提取适用于此非线性可分离数据的新主成分。

32
00:02:19,380 --> 00:02:21,870
好吧，您将看到它将获得惊人的结果。

33
00:02:21,870 --> 00:02:29,340
所以现在让我们将整个模型从上到下复制到底部，然后面对现实

34
00:02:29,400 --> 00:02:31,800
在我们的代码PCA文件中。

35
00:02:31,900 --> 00:02:39,380
好的，现在基本上我们唯一要做的就是在正确的位置应用所有PCa。

36
00:02:39,480 --> 00:02:44,700
但是在我们这样做之前，我想再次想像一下为什么这种线性模型不合适

37
00:02:44,880 --> 00:02:46,610
对于此数据集。

38
00:02:46,620 --> 00:02:51,940
所以我们要做的是从这里开始进行所有操作，因为您知道这将使培训形象化

39
00:02:51,950 --> 00:02:55,800
通过绘制预测区域和预测边界得出结果。

40
00:02:55,830 --> 00:03:01,830
因此，我们将从头到尾介绍所有内容，您知道重要的数据集已应用了预处理

41
00:03:01,830 --> 00:03:07,220
将阶段finta logistic回归到训练集，并最终绘制结果训练图。

42
00:03:07,380 --> 00:03:12,900
因此，让我们做起来让我们非常快速地再次可视化它，这将使我们有动力进行应用。

43
00:03:12,900 --> 00:03:15,700
PCX上校我们来了。

44
00:03:15,760 --> 00:03:17,290
所有执行正确。

45
00:03:17,590 --> 00:03:18,640
那就是结果。

46
00:03:18,640 --> 00:03:20,250
让我们放大一下。

47
00:03:20,440 --> 00:03:21,420
好的，这是。

48
00:03:21,580 --> 00:03:24,490
提醒一下，要点是真实的观察。

49
00:03:24,490 --> 00:03:30,500
那就是我们社交网络中的真实客户，由他们的年龄和估计的工资代表。

50
00:03:30,700 --> 00:03:32,770
因此，这是一个真正的观察点。

51
00:03:32,890 --> 00:03:36,200
这些区域代表了我们的预测。

52
00:03:36,220 --> 00:03:38,540
这里的红色区域和这里的绿色区域。

53
00:03:38,620 --> 00:03:44,890
基本上，这个红色区域是我们的模型预测客户不会点击广告的地方

54
00:03:44,890 --> 00:03:49,960
因为我们这里的零，这里的绿色区域是模型预测

55
00:03:49,960 --> 00:03:52,950
客户将点击广告并购买SUV。

56
00:03:53,410 --> 00:04:00,010
所以请记住问题是，这里的直线实际上是生成的预测边界

57
00:04:00,010 --> 00:04:01,790
通过逻辑回归模型。

58
00:04:01,840 --> 00:04:07,000
但是由于逻辑回归模型是线性分类器，因此这里必须是一条直线

59
00:04:07,000 --> 00:04:11,820
分离数据，因此请记住问题是它无法做出某种曲线

60
00:04:11,830 --> 00:04:16,150
在这里捕获这些应该位于绿色区域的绿色用户。

61
00:04:16,150 --> 00:04:21,760
现在，它们在红色区域中，而这里的绿豆也在红色区域中。

62
00:04:21,760 --> 00:04:26,710
因此，这清楚地表明了我们的数据不可线性分离的事实，因为我们可以

63
00:04:26,710 --> 00:04:32,560
看到这里的预测边界起着分隔符的作用，应该分开

64
00:04:32,650 --> 00:04:33,770
这两个班。

65
00:04:33,860 --> 00:04:38,860
好吧，它不能正确地将两个类分开，因为您可以看到这些用户不在正确的位置

66
00:04:38,860 --> 00:04:39,570
区域。

67
00:04:40,050 --> 00:04:45,250
因此，现在我们要做的不是像我们在第三部分中那样做一个非线性分类器

68
00:04:45,250 --> 00:04:49,080
知道何时为我们创建基于内核VM的基础决策树。

69
00:04:49,270 --> 00:04:56,320
好吧，我们现在要做的是黑色的旧PCA，以便我们将直线作为分隔符

70
00:04:56,610 --> 00:04:59,110
作为线性分类器的预测边界。

71
00:04:59,170 --> 00:05:04,330
逻辑回归模型仍然会有预测边界，但由于我们

72
00:05:04,330 --> 00:05:10,180
要应用内核PCA，那么这将设法应用一些技巧，其中的技巧实际上是

73
00:05:10,180 --> 00:05:17,350
内核技巧将数据映射到更高的维度，然后应用PCA提取新的组件

74
00:05:17,350 --> 00:05:20,250
将是可以解释最多变体的新尺寸。

75
00:05:20,410 --> 00:05:22,170
但是要感谢这个内核技巧。

76
00:05:22,330 --> 00:05:28,570
好吧，您将看到我们将设法获得一些新的维度，在这些维度中数据将可以线性分离

77
00:05:28,780 --> 00:05:31,860
即使是线性分类器（例如逻辑回归）。

78
00:05:31,870 --> 00:05:32,880
因此，让我们现在看看。

79
00:05:32,890 --> 00:05:35,010
我等不及要向您展示。

80
00:05:35,080 --> 00:05:36,250
我要关闭这个。

81
00:05:36,370 --> 00:05:40,720
现在让我们在正确的位置应用内核PCa。

82
00:05:40,750 --> 00:05:42,590
所以您已经知道这个位置是什么。

83
00:05:42,610 --> 00:05:45,360
实际上与以前没有什么不同。

84
00:05:45,400 --> 00:05:51,430
在填充分类器之前，我们需要在数据预处理阶段之前直接应用内核PCA。

85
00:05:51,430 --> 00:05:53,930
例如逻辑回归到我们的培训中心。

86
00:05:53,980 --> 00:06:04,250
所以基本上我们需要在这里应用当前的PCA，因此在美国使用Kerno PCa，然后我们开始

87
00:06:04,450 --> 00:06:05,380
我们开始做吧。

88
00:06:05,380 --> 00:06:07,030
因此，实际上我们要提高效率。

89
00:06:07,060 --> 00:06:12,040
我们将采用我们的代码，因为您将看到它非常相似。

90
00:06:12,040 --> 00:06:18,520
因此，我将在此处采用四行，而不仅仅是此处的这行，并解释方差向量

91
00:06:18,790 --> 00:06:20,800
因为在这里我们显然需要两个组成部分。

92
00:06:20,860 --> 00:06:27,940
因此，让我们复制此内容，让我们回到pca，在这里直面它。

93
00:06:27,940 --> 00:06:31,180
好吧，首先，我们需要导入什么类。

94
00:06:31,180 --> 00:06:32,640
是PCA吗？

95
00:06:32,710 --> 00:06:37,900
好吧，实际上，PCA上校就这么简单。

96
00:06:38,380 --> 00:06:43,740
而且我们仍然从缩放器和分解中获取此内核PCA类。

97
00:06:43,870 --> 00:06:49,650
所以第一行，然后第二行，我们创建此内核PCA类的对象。

98
00:06:49,690 --> 00:06:55,500
因此，我们不称此对象为D-CA，这次将其命名为Kerl PCA的K PCA。

99
00:06:55,720 --> 00:07:03,120
当然，在这里我们要做的是用内核PCA替换该PCA类。

100
00:07:03,490 --> 00:07:04,150
好吧。

101
00:07:04,150 --> 00:07:10,630
在当前的PCA类中，我们愿意提供比经典PCA更多的论点

102
00:07:10,630 --> 00:07:14,280
实际是通过按命令这里看看。

103
00:07:14,560 --> 00:07:15,650
现在我们开始。

104
00:07:15,670 --> 00:07:19,470
毕生（Pythia）上校，我们有两个关于不同参数的不同信息。

105
00:07:19,660 --> 00:07:24,880
如您所见，仍然有第一个参数和组件，这当然是

106
00:07:24,880 --> 00:07:26,370
您想要的尺寸。

107
00:07:26,590 --> 00:07:30,020
当然，从现在开始，我们正在研究一种用于机器学习的内核方法。

108
00:07:30,130 --> 00:07:34,500
好了，我们拥有与内核SVM完全相同的内核参数。

109
00:07:34,660 --> 00:07:41,890
实际上，我们有相同的内核选择，可用于将内核树应用于Pca和

110
00:07:41,890 --> 00:07:48,010
我们将要使用的一个是我们最常用的B-F高斯核

111
00:07:48,010 --> 00:07:49,870
在大多数数据集上都很好。

112
00:07:49,870 --> 00:07:54,590
因此，让我们在这里结束这个论点，实际上我们在这里还有其他论点。

113
00:07:54,590 --> 00:08:00,040
我再次质疑您的O'Connel武器Alpha是否适合我们有很多参数的约束

114
00:08:00,370 --> 00:08:07,070
但是这些并不是最重要的，在这里您将看到仅添加此内核参数即可

115
00:08:07,090 --> 00:08:08,460
获得出色的结果。

116
00:08:08,470 --> 00:08:14,130
所以我们的B.F.就是Galchen内核，基本上我们的对象已经准备好了。

117
00:08:14,440 --> 00:08:19,720
现在我们需要做的就是在火车上使用PC对象，并通过应用此对象进行访问

118
00:08:19,720 --> 00:08:25,920
对象的转移方法Anax火车和转移方法Anax就像我们对PCa所做的一样。

119
00:08:26,110 --> 00:08:33,560
因此，我们当然只需要在这里用Cape's yea替换PCA即可，对于X测试也是如此。

120
00:08:33,560 --> 00:08:34,290
好吧。

121
00:08:34,330 --> 00:08:40,630
现在实际上我们已经准备好将内核PCA应用于我们的数据集，因此会发生什么

122
00:08:40,630 --> 00:08:44,840
是我们的数据集将使用内核Treyc映射到更高的维度。

123
00:08:44,890 --> 00:08:50,110
这样就已经为许多新的特征空间创建了一些新的维度，在这些维度上，数据实际上是

124
00:08:50,110 --> 00:08:51,010
可分离。

125
00:08:51,100 --> 00:08:56,290
然后，由于我们处于这个新的特征空间，因此数据是线性可分离的，因此PCA将是

126
00:08:56,290 --> 00:09:01,420
通过提取新的主成分来减少尺寸。

127
00:09:01,420 --> 00:09:04,110
好的，现在让我们应用相同的整个代码。

128
00:09:04,150 --> 00:09:11,770
但是这次这部分包括我们在其中应用内核PCA的位置，因此我们需要重置我们的内核

129
00:09:11,770 --> 00:09:14,630
控制台，因为最好从头开始。

130
00:09:14,650 --> 00:09:18,900
因此，让我们单击此处的此工具按钮，然后重新启动内核。

131
00:09:18,910 --> 00:09:20,380
您确定要重启通话吗？

132
00:09:20,380 --> 00:09:21,110
是。

133
00:09:21,280 --> 00:09:22,300
现在我们开始。

134
00:09:22,540 --> 00:09:25,310
重新启动内核准备就绪。

135
00:09:25,310 --> 00:09:29,560
好的，现在我们可以真正地执行包含此部分的整个代码。

136
00:09:29,780 --> 00:09:31,200
这就是我们要做的。

137
00:09:31,490 --> 00:09:39,830
因此，我们从可视化火车组结果部分的末尾一直到顶部，

138
00:09:39,860 --> 00:09:46,430
执行此操作将执行所有步骤，应用内核PCA进行数据处理阶段中的步骤

139
00:09:46,430 --> 00:09:52,710
到我们的数据集，将逻辑回归模型拟合到训练集，从而创建测试结果

140
00:09:52,710 --> 00:09:59,770
混乱的矩阵，最主要的是我们很高兴看到培训的可视化

141
00:09:59,780 --> 00:10:00,840
结果。

142
00:10:00,860 --> 00:10:01,800
因此，让我们开始吧。

143
00:10:02,030 --> 00:10:04,300
实际上，这不是您所期望的。

144
00:10:04,430 --> 00:10:09,170
好吧，也许您可​​以猜测会发生什么，但是正如我告诉您的那样，我们并不期待非线性

145
00:10:09,170 --> 00:10:11,590
Tesfaye具有预测边界的曲线。

146
00:10:11,580 --> 00:10:16,280
我们仍然期待一条直线，但是您会发现这次直线是完美的

147
00:10:16,280 --> 00:10:18,610
分开我们的数据集。

148
00:10:18,680 --> 00:10:23,360
数据集中的两个类，这要归功于这些新提取的功能。

149
00:10:23,360 --> 00:10:26,680
现在，让我们执行整个代码。

150
00:10:26,690 --> 00:10:27,460
开始了。

151
00:10:27,500 --> 00:10:29,220
这就是我们所获得的。

152
00:10:29,270 --> 00:10:35,870
正如我告诉您的那样，这实际上是一种新功能，我将对其进行放大并描述结果。

153
00:10:35,960 --> 00:10:36,320
好吧。

154
00:10:36,320 --> 00:10:43,000
因此，这些是上校PCA结合到我们应用于非线性的Logistic回归模型的结果

155
00:10:43,010 --> 00:10:44,980
可分离的数据集。

156
00:10:44,990 --> 00:10:50,480
因此，首先，该图的所有不同元素都表示相同的东西，红点是

157
00:10:50,480 --> 00:10:56,750
那些实际上没有在SUV上点击广告的客户绿点是那些

158
00:10:56,750 --> 00:11:03,980
实际上，点击广告即可购买SUV，红色区域是我们的模型预测的区域

159
00:11:03,980 --> 00:11:10,340
客户不购买SUV，绿色区域是模型预测客户的区域

160
00:11:10,340 --> 00:11:11,520
买SUV。

161
00:11:11,540 --> 00:11:13,350
所以那还是一回事。

162
00:11:13,370 --> 00:11:19,100
但是，现在最重要的变化是，我们需要在这里强调的事实是

163
00:11:19,100 --> 00:11:26,270
现在，这两个直线将红色和绿色两类更好地分开了

164
00:11:26,270 --> 00:11:26,710
这里。

165
00:11:26,870 --> 00:11:31,820
我们在红色区域中还有一些绿点，在绿色区域中有一些红点，但是我们可以

166
00:11:31,820 --> 00:11:37,490
感觉到我们处于一个新的特征空间中，这里两个不同类别的观察点

167
00:11:37,700 --> 00:11:39,380
现在更好地分开了。

168
00:11:39,620 --> 00:11:45,500
而我们现在所拥有的这个新功能空间是由

169
00:11:45,500 --> 00:11:47,670
通过PCA上校提取。

170
00:11:48,000 --> 00:11:51,070
因此，实际上这不是估算工资的年龄。

171
00:11:51,190 --> 00:11:55,640
现在，这是一台和一台PC作为主要组件一和二。

172
00:11:55,850 --> 00:12:02,600
这些就是这个新功能空间的维度，在这里我们的数据现在可以很好地分离了

173
00:12:02,780 --> 00:12:04,700
被这条直线隔开。

174
00:12:04,700 --> 00:12:08,090
这就是逻辑回归分类器的预测边界。

175
00:12:08,410 --> 00:12:13,940
因此，我们可以体会到所得结果的简单性与复杂性之间的对比

176
00:12:13,940 --> 00:12:19,220
幕后发生的事情，因为在这两个方面，我们确实得到了非常简单的结果

177
00:12:19,220 --> 00:12:24,740
用直线分隔的类与幕后发生的事情是我们的原始数据

178
00:12:24,740 --> 00:12:31,430
使用内核技巧将我们原始特征空间中的集合映射到更高维度，以避免

179
00:12:31,430 --> 00:12:33,480
高度计算密集型计算。

180
00:12:33,710 --> 00:12:39,350
然后通过将我们的数据集和原始特征空间映射到这个高维，对我们来说，

181
00:12:39,350 --> 00:12:46,100
创建了一些新的尺寸，并且大多数情况下创建了一个新的特征空间，然后我们的数据线性地

182
00:12:46,100 --> 00:12:46,970
可分离。

183
00:12:47,180 --> 00:12:51,330
但是这样做可以使尺寸比原始尺寸更多。

184
00:12:51,350 --> 00:12:56,870
因此，我们仍然需要应用PCA降维技术以减少数量

185
00:12:56,870 --> 00:12:58,020
尺寸。

186
00:12:58,040 --> 00:13:03,350
因此，然后将PCa应用于此新特征空间，在该空间中数据是线性可分离的，并且通过

187
00:13:03,840 --> 00:13:09,350
您提取的一些自变量是创建的，除了主要成分

188
00:13:09,440 --> 00:13:10,260
PCa。

189
00:13:10,400 --> 00:13:16,850
最后，我们获得了由这两个新提取的主成分形成的新特征空间

190
00:13:17,060 --> 00:13:23,510
来自PCa的结果，现在我们的数据是线性可分离的，并且通过线性更好地分离

191
00:13:23,510 --> 00:13:24,890
分类器。

192
00:13:25,040 --> 00:13:27,090
因此，我希望您喜欢结果。

193
00:13:27,190 --> 00:13:32,810
要知道即使原始数据线性可分离度为0％，您也可以获得相同的结果。

194
00:13:32,910 --> 00:13:40,220
例如，如果您所有的红色点都被绿色点圈包围，甚至

195
00:13:40,220 --> 00:13:42,120
此原始数据集。

196
00:13:42,330 --> 00:13:49,520
通过使用PCA呼叫，您将能够获得一条直线，将红色点和绿色点分开

197
00:13:49,520 --> 00:13:50,330
点。

198
00:13:50,370 --> 00:13:56,150
因此，这是一项非常强大的技术，我强烈建议您在PCA无法提供良好效果的情况下尝试

199
00:13:56,150 --> 00:14:01,730
结果在您的数据集上，因此可能是非线性问题的数据集。

200
00:14:01,730 --> 00:14:03,860
好的，对于Kerno PCA来说就是这样。

201
00:14:03,950 --> 00:14:07,550
这也是该零件尺寸减小的结束。

202
00:14:07,700 --> 00:14:14,900
我将在pca结果之前和之后给您留下这些，我将在下一部分和道德方面再见

203
00:14:14,900 --> 00:14:20,570
选择和提高本课程的最后一部分将涵盖机器中非常令人兴奋的算法

204
00:14:20,570 --> 00:14:23,270
学习被称为极限提升。

205
00:14:23,300 --> 00:14:27,620
因此，我期待在下一部分见到您，然后再学习。

