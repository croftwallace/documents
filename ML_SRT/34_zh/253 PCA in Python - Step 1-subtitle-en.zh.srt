1
00:00:00,500 --> 00:00:05,600
您好，欢迎阅读此Python教程，欢迎阅读第9部分降维。

2
00:00:05,630 --> 00:00:12,170
因此，我们将从PCA原理的第一个降维技术开始这一部分

3
00:00:12,170 --> 00:00:13,690
组件分析。

4
00:00:13,850 --> 00:00:19,290
因此，您知道在降维中有两种技术：特征选择和特征提取。

5
00:00:19,370 --> 00:00:25,190
当我们实现向后消除模型以选择

6
00:00:25,190 --> 00:00:27,800
功能矩阵中最相关的功能。

7
00:00:27,800 --> 00:00:30,150
这就是最能说明问题的功能。

8
00:00:30,200 --> 00:00:34,940
因变量，现在我们开始使用降维的新技术，

9
00:00:34,940 --> 00:00:41,500
特征提取是一种方法，而PCA主成分分析是一种特征提取技术。

10
00:00:41,660 --> 00:00:46,430
提醒一下，假设您的要素矩阵具有独立变量。

11
00:00:46,430 --> 00:00:52,970
PCA要做的是，它将提取较少数量的自变量，但是

12
00:00:52,970 --> 00:00:58,520
将成为新的自变量，例如新的维度，并提取这些新的自变量

13
00:00:58,820 --> 00:01:03,970
将会是一些新的自变量，这些变量可以最大程度地解释数据集的差异。

14
00:01:04,100 --> 00:01:07,010
那就是无论您一天都无懈可击。

15
00:01:07,040 --> 00:01:13,790
在我们不考虑因变量中的因变量的意义上，这使得PCA成为无监督模式

16
00:01:13,790 --> 00:01:14,840
该模型。

17
00:01:14,840 --> 00:01:15,930
这就是PCA。

18
00:01:15,980 --> 00:01:20,490
并请记住在第2部分和第3部分中，我们使用了一个或两个独立变量。

19
00:01:20,630 --> 00:01:22,940
那是出于两个特定目的。

20
00:01:22,940 --> 00:01:27,220
第一个目的是我们需要图形化显示结果。

21
00:01:27,320 --> 00:01:32,840
并且由于每个自变量都对应绘图中的一个维度，因此我们可以将其可视化

22
00:01:32,840 --> 00:01:36,170
我们的结果最多包含两个自变量。

23
00:01:36,170 --> 00:01:40,850
第二个原因是由于采用了PCA降维技术。

24
00:01:41,060 --> 00:01:46,790
好吧，即使一开始我们有很多自变量，也可以减少很多

25
00:01:46,850 --> 00:01:52,850
自变量，但是那将是相关的自变量，因为这些自变量

26
00:01:52,850 --> 00:01:56,690
变量将解释数据集的最大差异。

27
00:01:56,810 --> 00:02:02,030
因此，由于我们可以减少自变量的数量，所以我们可以得到两个或两个

28
00:02:02,030 --> 00:02:07,000
三个独立变量，因此像第3部分中一样可视化结果。

29
00:02:07,190 --> 00:02:11,750
这正是我们在本节以下教程中将要执行的操作

30
00:02:11,960 --> 00:02:17,210
当我们介绍其他降维技术（例如LDA）以及

31
00:02:17,210 --> 00:02:18,450
当前的PCA。

32
00:02:18,680 --> 00:02:23,420
好吧，我们一开始会有很多功能，因此将无法可视化

33
00:02:23,420 --> 00:02:24,020
结果。

34
00:02:24,080 --> 00:02:30,230
但是，当我们应用PCa或LDA时，我们会将功能数量减少到2，因此我们将

35
00:02:30,260 --> 00:02:32,220
能够可视化结果。

36
00:02:32,420 --> 00:02:38,810
现在开始吧，进入文件浏览器窗口，找到我们的工作目录文件夹。

37
00:02:38,810 --> 00:02:41,550
所以现在我在台式机上很懒。

38
00:02:41,660 --> 00:02:44,220
我们现在在第九部分进行尺寸缩减。

39
00:02:44,260 --> 00:02:50,840
我们开始，并从第一种降维主成分分析技术开始

40
00:02:50,840 --> 00:02:51,630
PCA。

41
00:02:51,890 --> 00:02:54,710
这就是我们要设置为工作目录的四个目​​录。

42
00:02:54,890 --> 00:03:00,320
因此，请确保该文件夹包含wine文件，如果是这样的话，就可以开始使用了。

43
00:03:00,620 --> 00:03:00,920
好。

44
00:03:00,920 --> 00:03:08,780
因此，我也打开了我们在第3部分分类中做出的野生禽的逻辑回归，因为

45
00:03:08,780 --> 00:03:12,680
您将看到我们将处理分类问题。

46
00:03:12,680 --> 00:03:18,560
基本上，我们要做的就是使用此逻辑回归模型来解决此分类问题

47
00:03:18,560 --> 00:03:19,250
问题。

48
00:03:19,340 --> 00:03:25,700
但是我们将PCA应用于此逻辑回归模型，以便最终得到两个独立的

49
00:03:25,700 --> 00:03:31,280
解释最大差异的变量，因此我们将能够在这里使用这些代码部分

50
00:03:31,280 --> 00:03:33,320
可视化结果。

51
00:03:33,320 --> 00:03:43,400
所以我现在要做的是将所有内容从下到上复制并粘贴到此处

52
00:03:43,970 --> 00:03:45,640
还有我的电脑

53
00:03:46,160 --> 00:03:48,030
现在将改变一些事情。

54
00:03:48,110 --> 00:03:51,000
大多数情况下，我们将实施PCA。

55
00:03:51,270 --> 00:03:55,940
好的，所以我们需要更改的第一件事是不再使用的数据集的名称

56
00:03:55,940 --> 00:04:02,590
社交网络会将我们正在使用的文件与正在使用的葡萄酒进行广告。

57
00:04:02,630 --> 00:04:04,620
葡萄酒也一样。

58
00:04:04,820 --> 00:04:05,960
好的，完美。

59
00:04:05,960 --> 00:04:08,620
因此，我将在以后解释业务问题。

60
00:04:08,840 --> 00:04:15,020
因此，现在我们将导入此数据集，而我将解释背后的业务问题

61
00:04:15,020 --> 00:04:15,630
这个。

62
00:04:15,890 --> 00:04:20,190
因此，让我们首先导入库。

63
00:04:20,240 --> 00:04:21,010
开始了。

64
00:04:21,080 --> 00:04:21,680
做完了

65
00:04:21,890 --> 00:04:23,800
现在让我们进口沙漠。

66
00:04:23,900 --> 00:04:26,540
所以我选择这行并执行。

67
00:04:26,600 --> 00:04:27,300
开始了。

68
00:04:27,350 --> 00:04:29,340
现在让我们去浏览器。

69
00:04:29,510 --> 00:04:31,130
真正重要的数据。

70
00:04:31,130 --> 00:04:32,540
我们来看一下。

71
00:04:32,540 --> 00:04:32,830
好。

72
00:04:32,870 --> 00:04:37,760
首先，这是机器学习文献中非常著名的非常著名的数据集，

73
00:04:37,760 --> 00:04:44,810
您可以在UCI机器上找到代表它的信息，您可以在这里看到，也可以在此链接找到此页面。

74
00:04:44,820 --> 00:04:49,480
所以基本上首先是什么是自变量，什么是因变量。

75
00:04:49,830 --> 00:04:56,390
那么自变量是从一种酒精到这一脯氨酸的所有变量。

76
00:04:56,640 --> 00:05:00,970
而最后一个可变的客户群是因变量。

77
00:05:01,170 --> 00:05:06,240
因此，在原始数据集中，此因变量不称为客户群，实际上

78
00:05:06,330 --> 00:05:07,840
葡萄酒的起源。

79
00:05:07,950 --> 00:05:13,980
但是，让我们想象一下，作为数据科学家，我们正在为一家企业主工作，而这一企业

80
00:05:13,980 --> 00:05:16,950
所有者在此数据集中收集了所有这些信息。

81
00:05:17,010 --> 00:05:21,780
首先，这个企业主所做的是，它收集了这些独立机构的所有信息。

82
00:05:21,780 --> 00:05:26,450
这里的变量是几种葡萄酒的化学信息。

83
00:05:26,730 --> 00:05:33,000
这位企业主运用了一些聚类技术来找到喜欢

84
00:05:33,060 --> 00:05:39,240
根据葡萄酒的信息并应用这些聚类技术来确定特定的葡萄酒。

85
00:05:39,240 --> 00:05:42,520
该企业主确定了三部分客户。

86
00:05:42,630 --> 00:05:44,020
那是这里的第一个。

87
00:05:44,100 --> 00:05:45,410
然后是第二个。

88
00:05:45,630 --> 00:05:51,860
最后，第三种基于这些信息，并归功于其聚类技术

89
00:05:52,070 --> 00:05:57,800
好吧，这家公司或一家公司设法找到了一些客户群，每个客户群都有特定的

90
00:05:57,800 --> 00:06:00,230
对特定葡萄酒的偏爱。

91
00:06:00,230 --> 00:06:05,630
因此，基本上，该业务只发现了三种葡萄酒，每种葡萄酒对应一个细分市场

92
00:06:05,720 --> 00:06:09,110
客户，因此分为三部分。

93
00:06:09,170 --> 00:06:11,640
为什么要为其业务创造附加值。

94
00:06:11,780 --> 00:06:16,670
好吧，因为现在这个企业主可以做的就是获取所有这些风的信息

95
00:06:16,910 --> 00:06:22,580
以及有关客户群的信息，并建立类似物流的分类模型

96
00:06:22,580 --> 00:06:27,800
回归，其中自变量是所有这些变量，而不同的变量是

97
00:06:27,800 --> 00:06:34,430
客户群，因此对于每种新酒，它都可以预测应将其细分到哪个客户群

98
00:06:34,430 --> 00:06:35,830
推荐这个。

99
00:06:35,840 --> 00:06:41,450
因此，我们将要建立的逻辑回归模型将返回

100
00:06:41,510 --> 00:06:43,990
每个新的建议。

101
00:06:44,000 --> 00:06:46,530
因此，这为该企业主增加了很多价值。

102
00:06:46,610 --> 00:06:51,350
但是，如果该企业主希望对预测区域和

103
00:06:51,350 --> 00:06:56,510
我们将要建立的分类模型的预测边界，以便能够查看预测是否

104
00:06:56,540 --> 00:06:59,230
在客户群的正确位置。

105
00:06:59,330 --> 00:07:04,940
好吧，这不可能用所有这些自变量来完成，因为我们当然不能代表这些

106
00:07:04,940 --> 00:07:07,350
一个图中有许多自变量。

107
00:07:07,400 --> 00:07:14,350
我们将需要一二三四五五六七八九10 11 12 13尺寸。

108
00:07:14,480 --> 00:07:15,540
那是不可能的。

109
00:07:15,560 --> 00:07:21,620
所以我们需要做的是应用一些降维技术来提取两个自变量

110
00:07:21,620 --> 00:07:27,080
解释最多的方差，然后我们将能够看到预测区域和预测

111
00:07:27,080 --> 00:07:32,480
边界，因此可以清楚地看到客户群在哪里以及这些客户在哪里

112
00:07:32,480 --> 00:07:34,630
客户细分的预测。

113
00:07:34,880 --> 00:07:39,790
根据提取的特征我们所有的自变量信息。

114
00:07:39,920 --> 00:07:44,330
请记住，这些提取的功能称为主要组件。

115
00:07:44,330 --> 00:07:50,660
因此，让我们这样做，建立逻辑回归分类模型，并应用PCA来减少

116
00:07:50,660 --> 00:07:54,890
问题的维度，并最终将结果可视化。

117
00:07:54,890 --> 00:08:01,490
因此，在关闭这些数据集之前，请注意，我们需要在矩阵中更改索引

118
00:08:01,490 --> 00:08:04,540
的特征以及因变量的索引。

119
00:08:04,670 --> 00:08:08,420
因此，让我们看看我们需要使用此数据集更改哪些索引。

120
00:08:08,450 --> 00:08:14,930
好的，所以这是功能矩阵，我们需要在此处包括各列的索引是独立的

121
00:08:14,950 --> 00:08:20,450
变量，所以这些是从此处到此处所有变量的索引。

122
00:08:20,630 --> 00:08:29,610
而且由于索引和Python开始于0井，因此分别为0 1 2 3 4 5 6 7 8 9 10 11和12。

123
00:08:29,630 --> 00:08:34,280
所以这将是从0到12的范围。

124
00:08:34,300 --> 00:08:37,750
让我们记住，不包括范围的上限。

125
00:08:37,820 --> 00:08:43,980
因此它将是0到13的范围，我们将在13中将其键入零调用。

126
00:08:44,180 --> 00:08:45,800
然后为因变量。

127
00:08:45,920 --> 00:08:48,980
那就是该数据集最后一列的索引。

128
00:08:49,160 --> 00:08:54,680
因此是13，因为这是索引12，因此该索引是13。

129
00:08:54,710 --> 00:09:00,300
现在，我们可以单击“确定”并关闭它，然后在此处替换正确的索引。

130
00:09:00,680 --> 00:09:04,570
所以我们在这里说0叫13。

131
00:09:04,790 --> 00:09:13,970
我们在这里说了13，现在请小心，我们需要卸下这些括号，然后我们将要继续。

132
00:09:14,210 --> 00:09:19,690
因此，我将选择此行来创建要素矩阵。

133
00:09:19,700 --> 00:09:27,220
在这里，我们也选择该行以创建非常有价值的性能，我们可以快速查看一下。

134
00:09:27,230 --> 00:09:31,870
这是包含我们13个独立变量的功能矩阵。

135
00:09:31,910 --> 00:09:32,540
完善。

136
00:09:32,600 --> 00:09:38,720
现在这对于三类葡萄酒来说是无价的，每类葡萄酒对应一个细分市场

137
00:09:38,840 --> 00:09:40,420
客户。

138
00:09:40,430 --> 00:09:41,110
完善。

139
00:09:41,270 --> 00:09:43,950
现在，我们准备继续进行下一部分。

140
00:09:44,060 --> 00:09:49,070
因此，下一部分是将数据集分为训练集和测试集，我们需要做的是

141
00:09:49,070 --> 00:09:54,410
这是因为我们想训练我们的模型和训练集并在单独的位置上测试其性能

142
00:09:54,410 --> 00:09:55,080
测试集。

143
00:09:55,340 --> 00:10:01,050
在这里，让我们选择20％的观测值用于测试集，这样我们就有80％

144
00:10:01,060 --> 00:10:04,250
训练集和20％的测试是完美的。

145
00:10:04,250 --> 00:10:10,760
准备就绪，我们可以选择该部分并执行以将数据集拆分为训练集

146
00:10:10,850 --> 00:10:11,890
然后进行测试。

147
00:10:11,900 --> 00:10:17,540
这里我们有此弃用警告，因为我刚刚更新了Spider的最新版本，

148
00:10:17,540 --> 00:10:23,740
版本交叉验证被Mario选择所取代。

149
00:10:24,120 --> 00:10:26,400
因此，基本上包的名称刚刚更改。

150
00:10:26,550 --> 00:10:31,620
因此，如果我收到这样的通知，则弃用警告应该消失。

151
00:10:31,680 --> 00:10:32,650
现在我们开始。

152
00:10:32,880 --> 00:10:34,840
现在让我们继续下一部分。

153
00:10:35,040 --> 00:10:40,500
所以下一部分是要素缩放，当我们应用维数时必须应用要素缩放

154
00:10:40,500 --> 00:10:43,250
减少技术，例如BCA或LDH。

155
00:10:43,410 --> 00:10:46,380
因此，我们也将选择此部分。

156
00:10:46,380 --> 00:10:47,840
我们在这里没有任何更改。

157
00:10:47,850 --> 00:10:49,050
很好

158
00:10:49,290 --> 00:10:52,540
那就是模板的美，因此可以执行。

159
00:10:52,560 --> 00:10:53,130
开始了。

160
00:10:53,130 --> 00:10:53,860
完善。

161
00:10:53,880 --> 00:10:59,790
现在我们的特征矩阵被杀死，然后下一部分是适合逻辑的部分

162
00:10:59,790 --> 00:11:01,550
回归训练集的模型。

163
00:11:01,710 --> 00:11:06,780
但这实际上不是我们需要运行的下一部分，因为现在是我们必须申请的时间

164
00:11:06,960 --> 00:11:09,960
我们的第一个降维技术PCA。

165
00:11:10,140 --> 00:11:14,370
请记住，您需要在数据处理阶段之后立即播放它。

166
00:11:14,370 --> 00:11:19,590
因此，在功能逐步发展之后，就在拟合逻辑回归模型之前或

167
00:11:19,650 --> 00:11:23,460
训练集的其他任何分类模型。

168
00:11:23,460 --> 00:11:32,510
因此，在这里我将添加一个新部分，这将完美地应用PCA。

169
00:11:32,770 --> 00:11:37,420
因此，我们在这里停止本教程的学习，然后在下一个教程中实现PCa。

170
00:11:37,690 --> 00:11:42,250
并提取出可以解释最多方差的新自变量。

171
00:11:42,250 --> 00:11:44,510
因此，我期待在下一个教程中做到这一点。

172
00:11:44,530 --> 00:11:46,200
直到那时我才真正开始学习。

