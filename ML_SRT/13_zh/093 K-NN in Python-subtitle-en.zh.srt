1
00:00:00,180 --> 00:00:05,820
您好，欢迎今天访问此Python教程，我们将先实现K，然后实现算法K

2
00:00:05,880 --> 00:00:07,240
最近的邻居。

3
00:00:07,470 --> 00:00:12,930
因此，让我们从第一步开始，即设置目录或工作目录，然后转到文件资源管理器

4
00:00:12,930 --> 00:00:20,550
在这里，我们进行机械的Azy Part 3分类，然后在该隐那里有邻居，那就是

5
00:00:20,600 --> 00:00:25,340
希望将其设置为工作目录，因此请确保尽快拥有社交网络

6
00:00:25,360 --> 00:00:26,050
你归档。

7
00:00:26,130 --> 00:00:27,990
如果是这样的话，您就准备好了。

8
00:00:28,020 --> 00:00:31,980
您只需要在这里单击此按钮，我们便已准备就绪。

9
00:00:31,980 --> 00:00:35,090
现在让我们开始实现密钥和算法。

10
00:00:35,310 --> 00:00:39,750
因此，我们当然要使用在逻辑回归中制作的分类模板

11
00:00:39,750 --> 00:00:40,400
部分。

12
00:00:40,620 --> 00:00:47,070
我将从这里到这里带走所有东西。

13
00:00:47,070 --> 00:00:52,780
复制并粘贴到此处。

14
00:00:52,830 --> 00:00:55,680
好的，现在我们只需要更改一些内容。

15
00:00:55,740 --> 00:00:58,610
我们的模型将在手电筒中准备就绪。

16
00:00:58,980 --> 00:01:05,430
因此，我们需要更改的几件事实际上是在此部分中必须创建或分类的

17
00:01:05,440 --> 00:01:05,660
。

18
00:01:05,730 --> 00:01:11,100
但是只需三行，您就知道我们需要导入库，然后我们需要创建

19
00:01:11,100 --> 00:01:11,880
物体。

20
00:01:12,000 --> 00:01:14,360
然后我们需要将对象适合我们的训练集。

21
00:01:14,610 --> 00:01:16,130
因此，它将非常快。

22
00:01:16,380 --> 00:01:23,900
首先选择所有数据进行预处理，然后按命令控制百分比执行

23
00:01:23,900 --> 00:01:24,520
。

24
00:01:24,560 --> 00:01:27,210
在这里，我们正确地进行了所有预处理。

25
00:01:27,420 --> 00:01:30,990
现在让我们进入变量x来检查一切是否正常。

26
00:01:31,200 --> 00:01:38,190
因此，我们将数据集作为提醒仅包含社交网络中用户的信息。

27
00:01:38,190 --> 00:01:44,700
网络有一个商业计划，这是一个汽车公司，汽车公司在社交网络上投放广告，

28
00:01:44,700 --> 00:01:50,690
社交网络收集有关用户是否点击并购买了汽车产品的信息

29
00:01:50,690 --> 00:01:51,190
。

30
00:01:51,210 --> 00:01:56,330
因此，如果用户不购买产品，则为零；如果用户使用产品，则为零。

31
00:01:56,340 --> 00:02:02,340
因此，这里的目标是要创建一个分类器，以对合适的用户进行分类并将其放入合适的

32
00:02:02,340 --> 00:02:08,260
通过预测每个用户是否购买SUV进行分类。

33
00:02:08,430 --> 00:02:14,730
因此，让我们按OK，我们得到了训练集X训练y训练，并经过了X测试和最重的测试

34
00:02:15,300 --> 00:02:15,750
完善。

35
00:02:15,750 --> 00:02:18,550
现在，让我们创建分类器。

36
00:02:19,410 --> 00:02:21,850
好的，所以我们需要导入哪个库。

37
00:02:21,980 --> 00:02:25,930
是西卡特转邻居的图书馆。

38
00:02:26,160 --> 00:02:30,070
从该库中，我们将导入凯特邻居分类器。

39
00:02:30,210 --> 00:02:34,920
因此，让我们从Eskay开始。

40
00:02:34,960 --> 00:02:39,180
了解邻居。

41
00:02:39,270 --> 00:02:40,250
这里是。

42
00:02:40,420 --> 00:02:41,080
进口

43
00:02:43,580 --> 00:02:45,750
Caye邻居

44
00:02:48,850 --> 00:02:50,200
分类器。

45
00:02:50,850 --> 00:02:51,240
好吧。

46
00:02:51,240 --> 00:02:56,670
导入了该类，现在我们将创建该类的新对象。

47
00:02:56,700 --> 00:03:01,080
因此，像往常一样，我们将其称为分类器分类器。

48
00:03:01,410 --> 00:03:02,160
好。

49
00:03:02,280 --> 00:03:03,180
等于。

50
00:03:03,310 --> 00:03:07,330
我们将复制此内容，因为我们正在调用该类。

51
00:03:07,430 --> 00:03:14,140
因此，将其复制粘贴到此处，然后我们将选择参数。

52
00:03:14,340 --> 00:03:19,920
因此，让我们按命令我需要检查此类，这里是。

53
00:03:19,920 --> 00:03:26,850
所以有一些参数，所以第一个参数是，而Nehra没问题，那么哪个邻居会做

54
00:03:26,850 --> 00:03:29,730
我们要在算法中选择K。

55
00:03:30,000 --> 00:03:37,860
因此，我们将选择默认数字，即五个邻居，这足以使一个

56
00:03:37,860 --> 00:03:39,030
好的气化炉。

57
00:03:39,030 --> 00:03:43,590
我们将在本部分的最后一部分中看到如何改进甘蔗和分类器。

58
00:03:43,710 --> 00:03:46,180
但是现在，让我们使用该默认数字。

59
00:03:46,350 --> 00:03:55,850
因此，在这里我们将相加，并且邻居等于5。

60
00:03:56,130 --> 00:04:02,370
然后第二个参数是wait，这里我们不在乎我们不需要这个参数

61
00:04:02,370 --> 00:04:10,420
都不携带任何“叶大小”，我们不在乎，但我们关注度量和P，因为正如Curiel解释的那样

62
00:04:10,860 --> 00:04:17,850
算法中的K是基于欧几里得距离的，我们实际上要选择欧几里德距离

63
00:04:18,180 --> 00:04:24,810
作为基于我们算法的距离，并选择欧几里得距离，如您所见，我们需要

64
00:04:25,080 --> 00:04:28,440
首先选择思维Koski指标。

65
00:04:28,620 --> 00:04:35,310
然后在这里我们需要选择p等于2，因为您可以看到Minkowski的幂参数

66
00:04:35,310 --> 00:04:35,760
指标。

67
00:04:35,760 --> 00:04:41,580
这意味着当我们选择Minkowski度量标准时，当人们选择它时，这等同于使用

68
00:04:41,580 --> 00:04:47,220
曼哈顿距离，但这不是我们想要的距离，我们想要的是欧几里得距离，实际上是

69
00:04:47,220 --> 00:04:49,390
此参数也是picos。

70
00:04:49,650 --> 00:04:55,830
所以我们要做的就是将这两个参数metric等于Minkowski和

71
00:04:55,910 --> 00:04:56,690
也要关闭

72
00:04:56,940 --> 00:05:09,380
因此，我们在此处添加公制等于我的Koski，p等于并设置分类

73
00:05:09,420 --> 00:05:12,460
您只需准备好这三个参数。

74
00:05:12,830 --> 00:05:19,880
好吧，现在准备就绪时，让我们获得这个分类的对象，因为这是甘蔗的对象

75
00:05:20,120 --> 00:05:28,800
非常好的上课让该主题适合我们的培训，因此我们按照通常的分类器思想进行操作

76
00:05:28,980 --> 00:05:30,530
适合。

77
00:05:30,530 --> 00:05:37,760
因此，我们在这里使用了fit方法，这是K类邻居气化炉的方法，然后对其进行拟合

78
00:05:37,760 --> 00:05:44,780
到X火车和y火车。

79
00:05:45,440 --> 00:05:46,040
完善。

80
00:05:46,110 --> 00:05:51,150
现在，实际上是让我们在这里选择这三行，然后开始。

81
00:05:51,140 --> 00:05:52,510
一切都应该没问题。

82
00:05:53,060 --> 00:05:54,320
很好。

83
00:05:54,320 --> 00:05:54,720
完善。

84
00:05:54,710 --> 00:05:59,510
因此，这些都是默认情况下选择的所有不同参数，因为我们没有指定它们。

85
00:05:59,880 --> 00:06:03,740
现在已经准备就绪，可以预测测试结果了。

86
00:06:03,740 --> 00:06:05,180
我们开工吧。

87
00:06:05,270 --> 00:06:07,530
为什么普拉特在这里是白面包。

88
00:06:07,560 --> 00:06:15,120
现在，如果我们要进行可行的实验，我们会发现白面包，可以将其与白测试进行比较以了解

89
00:06:15,120 --> 00:06:15,950
它在做。

90
00:06:16,190 --> 00:06:18,250
好，所以这就是为什么这是白面包。

91
00:06:18,300 --> 00:06:19,980
因此，这就是事实。

92
00:06:20,120 --> 00:06:24,860
这就是预测，因此让我们将其从零到6进行比较。

93
00:06:24,870 --> 00:06:26,540
事实是零。

94
00:06:26,550 --> 00:06:28,250
那是用户没有购买。

95
00:06:28,310 --> 00:06:31,840
预测也为零。

96
00:06:31,910 --> 00:06:37,230
这意味着预测用户从零到六的数量没有购买SUV，因此

97
00:06:37,230 --> 00:06:37,670
好。

98
00:06:37,680 --> 00:06:38,720
好的预测。

99
00:06:38,780 --> 00:06:43,370
七分之一的用户购买了SUV，因为它是一辆，而预测也是。

100
00:06:43,440 --> 00:06:48,020
出色的用户也没有购买SUV零零。

101
00:06:48,060 --> 00:06:50,950
大内恩斯在这里为零。

102
00:06:50,970 --> 00:06:57,530
实际上，第九名用户没有购买SUV，但我们的犬科客户在这里犯了一个小错误。

103
00:06:57,530 --> 00:06:59,800
他预言了第九种用法。

104
00:07:00,240 --> 00:07:01,850
但这是正常的。

105
00:07:01,860 --> 00:07:08,430
我们总是会有一些错误的预测，因为所有模型都是错误的，但有些模型是有用的。

106
00:07:08,430 --> 00:07:13,090
这个绝对有用，但不是100％正确。

107
00:07:13,160 --> 00:07:20,430
因此，让我们按OK，现在让我们看一下混淆矩阵，一次查看

108
00:07:20,490 --> 00:07:21,790
错误的预测。

109
00:07:22,060 --> 00:07:27,210
因此，让我们执行此操作，因为模板是以这种方式制作的，因此我们无需在此处做任何事情

110
00:07:27,200 --> 00:07:32,150
除此处的分类器外，我们无需更改任何其他内容。

111
00:07:32,200 --> 00:07:35,610
因此，让我们按命令并按Enter来执行。

112
00:07:35,750 --> 00:07:38,200
这是计算机矩阵。

113
00:07:38,240 --> 00:07:39,220
看，它在这里。

114
00:07:39,240 --> 00:07:40,330
我们来看一下。

115
00:07:40,820 --> 00:07:41,360
哇。

116
00:07:41,370 --> 00:07:42,280
一点也不差。

117
00:07:42,290 --> 00:07:47,160
如您所见，记得逻辑回归有11个错误的预测。

118
00:07:47,150 --> 00:07:50,450
现在我们有四个加三个等于七个。

119
00:07:50,450 --> 00:07:51,390
错误的预测。

120
00:07:51,380 --> 00:07:52,640
真是了不起。

121
00:07:52,650 --> 00:07:57,460
他们来到这里，这些都是正确的预测的数目。

122
00:07:57,450 --> 00:07:58,660
好吧，那太好了。

123
00:07:58,650 --> 00:08:00,800
现在是最好的部分。

124
00:08:00,840 --> 00:08:01,890
让我们看一下结果。

125
00:08:01,880 --> 00:08:07,370
让我们看一下预测边界，看看它是否为线性分类。

126
00:08:07,370 --> 00:08:09,670
您猜它是线性的还是非线性的。

127
00:08:09,690 --> 00:08:10,040
是的

128
00:08:10,030 --> 00:08:14,180
这意味着您知道如果它是线性分隔符，它将是一条直线。

129
00:08:14,340 --> 00:08:18,910
如果它是非线性的Caspari，则分离器将成为某种曲线。

130
00:08:19,160 --> 00:08:23,020
因此，让我们看看它是什么，让我们在这里更改标题。

131
00:08:23,030 --> 00:08:27,200
您知道我们在这里放食火鸡，因为它是模板。

132
00:08:27,410 --> 00:08:30,670
但是这里我们将选择k和。

133
00:08:31,040 --> 00:08:31,750
和。

134
00:08:32,000 --> 00:08:32,690
好吧。

135
00:08:32,880 --> 00:08:43,080
然后让我们也做一下，我们不会忘记分类器K K，现在它已经准备好了

136
00:08:43,320 --> 00:08:44,230
一切都很好。

137
00:08:44,380 --> 00:08:44,670
是的

138
00:08:44,650 --> 00:08:48,120
因此，基本上我们唯一需要更改的就是此处的转移。

139
00:08:48,170 --> 00:08:54,150
我们需要创建一个分类器，然后仅在此处更改标题，因为我们要指定

140
00:08:54,620 --> 00:08:56,110
我们正在使用的模型。

141
00:08:56,220 --> 00:09:03,550
因此，让我们选择从此处到此处的所有内容，然后按Command Control按Enter执行。

142
00:09:03,950 --> 00:09:05,860
现在我们开始。

143
00:09:06,410 --> 00:09:07,820
哇，哇。

144
00:09:07,830 --> 00:09:08,590
好。

145
00:09:08,970 --> 00:09:09,290
是。

146
00:09:09,290 --> 00:09:09,750
完善。

147
00:09:09,750 --> 00:09:12,200
那是一个很好的预测界限，不是吗。

148
00:09:12,200 --> 00:09:16,940
我的意思是，它看起来与我们在逻辑回归中看到的有很大不同。

149
00:09:16,980 --> 00:09:20,190
只是为了解释结果而已。

150
00:09:20,180 --> 00:09:21,860
好的，这是结果。

151
00:09:22,110 --> 00:09:28,430
好，因此预测边界非常特殊，您可以看到它看起来像一个边界

152
00:09:28,430 --> 00:09:32,950
美国某州或某个国家/地区的边界。

153
00:09:33,060 --> 00:09:36,780
实际上，它甚至不是直线或漂亮的曲线。

154
00:09:36,770 --> 00:09:40,900
您知道它到处都是那些不合规定的东西。

155
00:09:40,890 --> 00:09:48,140
但这很好，这是因为它是基于每次找到K个最近的邻居，所以

156
00:09:48,140 --> 00:09:52,690
大多数情况下，甘蔗的预测边界看起来像这样。

157
00:09:52,910 --> 00:09:59,150
与logistic回归相比，我们可以看到此类确实对工作进行了出色的分类

158
00:09:59,150 --> 00:10:06,020
将用户归为正确的类别，我们可以在此处看到所有红色用户，即没有

159
00:10:06,020 --> 00:10:06,540
买这个。

160
00:10:06,520 --> 00:10:11,760
我们位于红色区域，这表示最常预测用户不购买的区域

161
00:10:11,750 --> 00:10:18,240
SUV，我们看到这里的大多数绿点都在该班级非常预测的绿色区域

162
00:10:18,240 --> 00:10:20,510
该用户购买了SUV。

163
00:10:20,510 --> 00:10:23,160
记住，对于逻辑回归，我们有一条直线。

164
00:10:23,370 --> 00:10:29,820
因此，它无法正确分类所有与您知道的用户相对应的绿色用途

165
00:10:29,960 --> 00:10:36,920
年龄大约是平均水平，而工资却低于平均水平，但还是因为SUV而购买了SUV

166
00:10:36,920 --> 00:10:42,240
真的很酷而且便宜，逻辑回归无法在正确的类别中抓住这些人

167
00:10:42,240 --> 00:10:43,290
因为它是线性的。

168
00:10:43,460 --> 00:10:51,350
您知道这条线下面是红色区域，其中包含那些绿色用户。

169
00:10:51,360 --> 00:10:56,400
所以这里的预测不正确，因为边界不是直线，而是您

170
00:10:56,390 --> 00:10:59,240
知道在正确的类别中捕获所有正确的用法。

171
00:10:59,420 --> 00:11:05,120
好吧，这次甘蔗和分类器可以正确捕获那些用途，并将它们放在绿色区域

172
00:11:05,120 --> 00:11:05,370
。

173
00:11:05,420 --> 00:11:07,970
因此，这绝对是一项了不起的工作。

174
00:11:08,000 --> 00:11:11,900
然后您当然会有一些不正确的预测，例如绿色用户，在这里我们可以看到一些绿色

175
00:11:11,900 --> 00:11:13,870
红木用户。

176
00:11:13,880 --> 00:11:21,690
例如，这个用户这个用户这个用户这个用户这样就可以了，因为默认参数

177
00:11:21,680 --> 00:11:27,260
选择KNM的方式是为了防止模型过度拟合。

178
00:11:27,410 --> 00:11:32,440
选择它们的方式使佳能算法不太适合数据集。

179
00:11:32,510 --> 00:11:35,330
因此，实际上很好，这是一个很好的模型。

180
00:11:35,340 --> 00:11:37,350
这是一个很好的分类。

181
00:11:37,810 --> 00:11:42,870
可以，但是这就是训练方法，这是我们的道德者学习如何对数据进行分类的地方。

182
00:11:42,880 --> 00:11:47,970
因此，让我们看看它如何对测试集中的新集中的数据进行分类。

183
00:11:48,050 --> 00:11:49,500
标题很好。

184
00:11:49,700 --> 00:11:50,530
就是这样。

185
00:11:50,570 --> 00:11:54,710
然后按命令并控制我们并执行。

186
00:11:54,870 --> 00:11:56,160
这是测试集。

187
00:11:56,160 --> 00:11:58,100
因此，让我们看一下。

188
00:11:58,190 --> 00:11:59,370
这是测试集。

189
00:11:59,370 --> 00:12:03,350
正如我们所看到的，这些也是出色的结果，也很棒。

190
00:12:03,360 --> 00:12:09,140
我们当然将区域与我们在训练中观察到的区域相同，因为这些区域

191
00:12:09,130 --> 00:12:14,650
您所知道的区域是通过训练火车上的算法生成的。

192
00:12:14,660 --> 00:12:17,900
在这里，我们只是在看一些新的观察结果。

193
00:12:17,900 --> 00:12:20,670
但是区域保持不变。

194
00:12:20,660 --> 00:12:27,920
因此，我们可以看到这些区域完全适合作为新观测值的测试观测值

195
00:12:27,920 --> 00:12:33,000
因为我们可以看到大多数作为真实观察点的点都在正确的区域

196
00:12:33,020 --> 00:12:37,730
正确的预测区域，即红色区域，大多数绿点位于正确的区域

197
00:12:37,740 --> 00:12:38,980
这是绿色区域。

198
00:12:39,200 --> 00:12:46,800
所以事实与预测是一致的，当然我们有一些不正确的预测，但是

199
00:12:46,860 --> 00:12:48,890
那是因为我们防止过拟合。

200
00:12:48,890 --> 00:12:53,580
因此，例如，我们有一个这个，我们实际上可以计算出错误预测的数量

201
00:12:53,580 --> 00:12:55,250
计算在混淆矩阵中。

202
00:12:55,400 --> 00:12:57,090
记住我们有七个。

203
00:12:57,170 --> 00:12:58,900
因此，让我们快速计算一下。

204
00:12:59,150 --> 00:13:06,980
一二三四五六和七。

205
00:13:06,990 --> 00:13:09,070
好的，这是正确的预测。

206
00:13:09,270 --> 00:13:10,550
很好

207
00:13:10,910 --> 00:13:11,210
好。

208
00:13:11,210 --> 00:13:17,750
因此，尽管我们在这里发现了一个新的分类，但与我们在逻辑回归中看到的却大不相同

209
00:13:17,750 --> 00:13:18,560
。

210
00:13:18,600 --> 00:13:24,900
因此，您现在必须记住的是必须要了解的是，逻辑回归是线性的

211
00:13:24,900 --> 00:13:29,520
或分类器，因此这是分隔两个类别的直线。

212
00:13:29,570 --> 00:13:32,480
现在，k n n是非线性增强器。

213
00:13:32,610 --> 00:13:41,330
这样即使在数据不可线性分离的情况下，这种方式也可以分离数据

214
00:13:41,340 --> 00:13:42,060
。

215
00:13:42,060 --> 00:13:43,900
因此，感谢您观看本教程。

216
00:13:43,910 --> 00:13:49,430
我希望您喜欢发现罐头模型，也希望您喜欢这张图。

217
00:13:49,430 --> 00:13:54,390
我迫不及待想向您展示下一个分类器，我们将有其他惊喜，因此我们将

218
00:13:54,380 --> 00:13:59,780
当我们特别看到决策树并为美国分类器运行时会感到惊讶

219
00:13:59,790 --> 00:14:05,960
拥有更漂亮的决策边界，尤其是当我们将内核视为V.M. 分类器

220
00:14:05,960 --> 00:14:06,170
。

221
00:14:06,270 --> 00:14:09,990
所以我迫不及待地要发布这个，直到享受机器学习

