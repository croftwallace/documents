1
00:00:00,300 --> 00:00:02,670
您好，欢迎来到本美术教程。

2
00:00:02,670 --> 00:00:09,480
因此，在上一节中，这种PCA特征提取技术降低了我们问题的维度

3
00:00:09,480 --> 00:00:13,500
通过提取解释方差最大的变量。

4
00:00:13,640 --> 00:00:15,720
现在在LDH中，这是完全不同的。

5
00:00:15,750 --> 00:00:22,020
我们正在提取一些新的自变量，这些变量将分离大多数因变量

6
00:00:22,020 --> 00:00:22,700
变量。

7
00:00:22,980 --> 00:00:27,810
因此，自从这次以来，它考虑了因变量的类。

8
00:00:27,960 --> 00:00:33,090
好吧，这意味着它考虑了因变量来进行此特征提取技术

9
00:00:33,300 --> 00:00:38,760
因此，这使LDA成为有监督的降维模型。

10
00:00:38,760 --> 00:00:41,780
好的，现在让我们在LDA上应用LDA。

11
00:00:42,050 --> 00:00:45,190
因此，首先非常迅速，让我们在该目录中添加步枪。

12
00:00:45,210 --> 00:00:50,700
我们要实现的任务是将指针指向九维缩小，而我们现在处于

13
00:00:50,700 --> 00:00:54,130
第44条线性判别分析。

14
00:00:54,150 --> 00:00:56,130
因此，让我们进入内部，然后继续。

15
00:00:56,130 --> 00:00:58,030
您想要设置为工作目录。

16
00:00:58,060 --> 00:01:00,340
我们仍在研究说明文件的文件。

17
00:01:00,340 --> 00:01:05,840
因此，这与我们实施PCA时遇到的业务问题完全相同。

18
00:01:05,880 --> 00:01:11,610
因此，这是将这种降维技术LDA与

19
00:01:11,610 --> 00:01:13,260
前一个PCA。

20
00:01:13,620 --> 00:01:18,390
现在，我们不要忘记单击此处的更多按钮并将其设置为工作目录。

21
00:01:18,480 --> 00:01:19,100
完善。

22
00:01:19,200 --> 00:01:22,770
因此，由于我们正在处理与以前相同的业务问题。

23
00:01:22,770 --> 00:01:27,210
对于PCa而言，很好地实施LDA将会非常容易。

24
00:01:27,330 --> 00:01:35,750
我们将采用PCA代码，将所有内容从这里下移到底层副本，然后返回

25
00:01:35,760 --> 00:01:43,200
LGA并将整个内容粘贴到此处和此代码的内部，我们基本上只需要替换它即可

26
00:01:43,380 --> 00:01:48,490
通过专门用于应用LDA的新部分来应用PCA部分。

27
00:01:48,510 --> 00:01:51,650
因此，我将删除所有这部分。

28
00:01:51,780 --> 00:02:00,490
因此，让我们删除它，在这里用LDA替换PCA，现在是实现LDA的时候了。

29
00:02:00,690 --> 00:02:07,230
因此，我们将用于应用LDA的程序包是mass程序包和s s，它实际上是

30
00:02:07,230 --> 00:02:10,950
默认情况下，您的软件包列表中的软件包。

31
00:02:11,040 --> 00:02:17,550
它是此处的软件包，具有批量支持功能和数据集，用于启用和替换。

32
00:02:17,550 --> 00:02:17,820
好吧。

33
00:02:17,820 --> 00:02:20,080
因此，如您所见，这并不重要。

34
00:02:20,130 --> 00:02:23,700
因此，让我们使用library命令导入。

35
00:02:23,700 --> 00:02:28,050
在这里，我们在括号中以这种方式进行汇总。

36
00:02:28,050 --> 00:02:28,710
好吧。

37
00:02:28,710 --> 00:02:33,180
我们已经可以选择导入包了。

38
00:02:33,180 --> 00:02:35,420
现在让我们实施LDA。

39
00:02:35,550 --> 00:02:42,420
因此，我们要做的第一件事是要求BCA创建一个LDA变量，该变量将用于转换

40
00:02:42,480 --> 00:02:47,700
我们将原始数据集放入由Lynnie或判别式组成的新数据集中。

41
00:02:47,700 --> 00:02:51,240
因此，我们将这个变量称为LDA等于。

42
00:02:51,510 --> 00:02:55,200
现在，我们将像这样简单地使用LDA函数。

43
00:02:55,320 --> 00:02:57,690
然后添加一些括号。

44
00:02:57,690 --> 00:03:01,680
现在让我们按f一看一下参数。

45
00:03:01,680 --> 00:03:03,480
好吧，论点在这里。

46
00:03:03,480 --> 00:03:05,750
第一个参数是公式。

47
00:03:05,940 --> 00:03:12,050
因此，这正是因变量相对于自变量的公式。

48
00:03:12,060 --> 00:03:13,950
到目前为止，原始的。

49
00:03:14,010 --> 00:03:16,850
因此，这里作为第一个参数，我们将输入。

50
00:03:16,920 --> 00:03:20,370
公式等于客户群。

51
00:03:21,690 --> 00:03:26,190
请记住，这是因变量的名称并被填充。

52
00:03:26,310 --> 00:03:31,890
而且我们可以在不需要写所有自变量名称的地方添加一个点。

53
00:03:31,890 --> 00:03:33,740
点在这里给我们。

54
00:03:33,780 --> 00:03:38,770
因此，先出现下一个参数，然后下一个参数是数据。

55
00:03:38,970 --> 00:03:43,730
至于Pca，这里的数据将成为训练集。

56
00:03:43,740 --> 00:03:44,760
所以我们在这里添加。

57
00:03:44,820 --> 00:03:46,720
训练训练套。

58
00:03:46,740 --> 00:03:47,570
开始了。

59
00:03:47,940 --> 00:03:48,530
好吧。

60
00:03:48,690 --> 00:03:54,630
实际上，仅此而已，并且由于特定原因，这是直接相关的非常重要的原因

61
00:03:54,840 --> 00:03:55,770
到LDA。

62
00:03:56,040 --> 00:04:00,990
这是由于LDA是一种有监督的降维技术。

63
00:04:01,140 --> 00:04:06,900
记住，监督是指LDA模型考虑了因变量，因为

64
00:04:06,900 --> 00:04:08,870
考虑了因变量。

65
00:04:09,000 --> 00:04:15,540
嗯，很容易理解Lynnie或判别式的数量与

66
00:04:15,540 --> 00:04:20,610
依赖生存者的信息，而该信息实际上是

67
00:04:20,610 --> 00:04:21,700
因变量。

68
00:04:21,810 --> 00:04:27,510
线性或判别式的数量与信息之间存在明显的相关性

69
00:04:27,510 --> 00:04:28,870
的因变量。

70
00:04:28,880 --> 00:04:35,200
它是k减去一个线性判别式，其中k是类别数。

71
00:04:35,220 --> 00:04:40,960
所以在这里，因为我们有三个类，这意味着我们最多得到三个减一等于两个线性

72
00:04:40,960 --> 00:04:42,000
或判别式。

73
00:04:42,150 --> 00:04:48,690
因此，在这里，如果不指定线性判别式的数量等于2，我们将自动获得

74
00:04:48,740 --> 00:04:51,710
线性或判别式，因此仅此而已。

75
00:04:51,710 --> 00:04:54,160
我们不需要添加任何其他参数。

76
00:04:54,420 --> 00:05:02,640
因此，可以使用LDA对象将原始数据集转换为由

77
00:05:02,690 --> 00:05:03,910
判别式。

78
00:05:04,110 --> 00:05:11,160
我们将要进行判别，这正是我们要提取的PCa的要求

79
00:05:11,160 --> 00:05:16,530
功能，这次这两个新提取的功能将把最多的两个类别分开。

80
00:05:16,620 --> 00:05:19,560
因此，我们也应该获得非常好的结果。

81
00:05:19,560 --> 00:05:22,640
然后，这与PCa相同。

82
00:05:22,800 --> 00:05:28,830
我们需要转换训练集和测试集，以便我们可以在下一部分中使用它们

83
00:05:28,950 --> 00:05:35,430
因此我们将使用训练集使SVM很好地适应训练集，以建立分类器并

84
00:05:35,430 --> 00:05:40,920
然后我们将使用此测试集进行预测，以预测测试结果

85
00:05:40,920 --> 00:05:46,740
以及得出结论矩阵，最重要的是，我们将可视化训练集和

86
00:05:46,740 --> 00:05:52,200
测试结果之所以能够执行，是因为我们现在具有两个功能。

87
00:05:52,200 --> 00:05:52,530
好吧。

88
00:05:52,530 --> 00:05:53,360
因此，让我们开始吧。

89
00:05:53,400 --> 00:05:57,690
让我们做同样的事情将LDA应用于训练集和测试。

90
00:05:57,690 --> 00:06:00,930
因此，首先让我们从训练集训练集开始。

91
00:06:01,200 --> 00:06:05,730
因此请记住，我们将其保留为训练集，这样我们就不必在其余时间对其进行更改。

92
00:06:05,730 --> 00:06:06,490
部分。

93
00:06:06,540 --> 00:06:08,540
所以火车等于。

94
00:06:08,880 --> 00:06:14,340
然后记住，我们需要用来预测实际上与PCa完全相同的函数。

95
00:06:14,340 --> 00:06:18,210
但是，我们将需要添加一些东西使其工作。

96
00:06:18,270 --> 00:06:23,820
我将解释它是什么，但绝对可以使用漂亮的函数进行此转换。

97
00:06:23,970 --> 00:06:29,160
因此，我们现在需要在这个漂亮的函数中指定括号。

98
00:06:29,160 --> 00:06:31,510
您知道这里的第一个参数是对象。

99
00:06:31,650 --> 00:06:38,070
因此，对象是LPA，然后出现，然后是第二个参数，第二个参数是数据集

100
00:06:38,160 --> 00:06:42,200
我们要对其进行转换，以提取新功能。

101
00:06:42,300 --> 00:06:43,750
那就是火车。

102
00:06:43,860 --> 00:06:44,700
这里是。

103
00:06:44,700 --> 00:06:50,430
提醒一下，这是由13个独立变量组成的原始数据集，

104
00:06:50,430 --> 00:06:57,550
新的训练集由两个Lindhardt判别式的两个提取特征组成。

105
00:06:57,570 --> 00:06:58,150
好吧。

106
00:06:58,200 --> 00:07:03,660
因此，我们已经可以这样做，以查看是否获得了我们期望的正确培训。

107
00:07:03,690 --> 00:07:09,900
因此，在执行此行之前，我们需要执行前面的部分，因为我们需要

108
00:07:09,900 --> 00:07:12,690
导入数据集并应用数据预处理。

109
00:07:12,780 --> 00:07:13,890
因此，让我们开始吧。

110
00:07:13,890 --> 00:07:15,480
我们在这里没有任何更改。

111
00:07:15,480 --> 00:07:17,260
一切都已经做好了准备。

112
00:07:17,340 --> 00:07:20,560
感谢我们在PCa的预览部分所做的工作。

113
00:07:20,670 --> 00:07:22,170
因此，让我们执行一下。

114
00:07:22,170 --> 00:07:22,860
开始了。

115
00:07:22,920 --> 00:07:23,960
执行得好。

116
00:07:24,210 --> 00:07:32,530
现在让我们应用LDH创建LDA对象，然后使用该对象来转换我们的原始训练

117
00:07:32,540 --> 00:07:36,760
这套新训练集由我们的两个判别力Linnea组成。

118
00:07:36,810 --> 00:07:39,380
因此，我们已经导入了大包装。

119
00:07:39,390 --> 00:07:42,190
因此，我们只需要执行这一行代码即可。

120
00:07:42,240 --> 00:07:43,280
我们开始做吧。

121
00:07:43,350 --> 00:07:44,060
开始了。

122
00:07:44,070 --> 00:07:45,800
LDA对象创建良好。

123
00:07:45,990 --> 00:07:48,740
现在我们准备改变训练集。

124
00:07:49,080 --> 00:07:54,750
但是在选择这一行并执行之前，我们需要添加我刚刚提到的另一件事

125
00:07:55,170 --> 00:08:02,850
此功能将应用于此处的整个产品LDA培训集，并将对此进行设置

126
00:08:02,850 --> 00:08:11,250
训练集将被转换为数据帧，因为对于PCa，当我们进行转换时

127
00:08:11,250 --> 00:08:15,520
这里的火车是由主要部分组成的新火车。

128
00:08:15,540 --> 00:08:20,320
好吧，我们得到了一个数据帧，但LDA却不一样。

129
00:08:20,430 --> 00:08:25,890
我们将得到一个矩阵，并且需要一个数据框，因为这样您便知道我们有下一个代码节。

130
00:08:25,990 --> 00:08:31,420
在这些代码段中，我们使用的功能期望此处的火车组具有数据帧

131
00:08:31,420 --> 00:08:33,510
例如，它期望一个数据帧。

132
00:08:33,790 --> 00:08:39,580
因此，我们绝对需要转换此变换训练集，如果我们

133
00:08:39,580 --> 00:08:41,930
以这种方式执行它，但是是一个矩阵。

134
00:08:42,160 --> 00:08:49,060
因此，要将其简单地转换为数据框，我认为我们已经需要使用

135
00:08:49,060 --> 00:08:59,830
用作点数据点框架和圆括号，我们在此处关闭圆括号并将其设置为

136
00:08:59,830 --> 00:09:02,520
将训练集转换为数据框。

137
00:09:02,620 --> 00:09:08,020
现在我们准备执行此行代码，以获取由提取的代码组成的新训练集

138
00:09:08,020 --> 00:09:10,390
区别特征。

139
00:09:10,420 --> 00:09:14,140
因此，让我们开始吧，就像这行代码一样执行。

140
00:09:14,200 --> 00:09:17,910
如您所见，训练集仍然是训练集。

141
00:09:18,040 --> 00:09:19,740
否则它将是值。

142
00:09:19,960 --> 00:09:24,210
当我单击它时，让我们看一下我们刚刚创建的内容。

143
00:09:24,460 --> 00:09:28,540
首先，我们首先看到的是这首先来到这里。

144
00:09:28,630 --> 00:09:30,420
那就是因变量本身。

145
00:09:30,550 --> 00:09:35,640
我知道这不再称为客户群，但实际上是称为他们的客户群

146
00:09:35,650 --> 00:09:41,300
对于相同的观察结果和相同的标签1 2和3来说，这是完全相同的，但是魔术

147
00:09:41,380 --> 00:09:43,810
它被按功能称为类。

148
00:09:43,900 --> 00:09:45,100
因此，不必为此担心。

149
00:09:45,160 --> 00:09:46,570
那就是因变量。

150
00:09:46,690 --> 00:09:52,980
然后我们看到的下一个有趣的事情是两个线性判别式LG 1和LG 2。

151
00:09:53,200 --> 00:09:58,420
正如我告诉您的那样，我们得到的是线性判别式，因为我们有三个

152
00:09:58,420 --> 00:10:00,540
第四类非常宝贵。

153
00:10:00,670 --> 00:10:02,400
所以这对我们现在很重要。

154
00:10:02,500 --> 00:10:07,180
这就是我们将要使用的变量这是我们将用来训练

155
00:10:07,180 --> 00:10:12,840
SVM模型可以做出预测，以制作计算机矩阵并最终可视化结果。

156
00:10:13,210 --> 00:10:17,600
然后在一个过程中将其他三个变量改进为3。

157
00:10:17,680 --> 00:10:21,100
那只是从LDA模型方程式导出的变量。

158
00:10:21,100 --> 00:10:22,630
所以这在这里不是很重要。

159
00:10:22,630 --> 00:10:28,180
重要的是我们有我们的因变量类和两个新提取的功能。

160
00:10:28,180 --> 00:10:30,430
Linnie是判别式一和二。

161
00:10:30,690 --> 00:10:35,110
因此，现在我们要做的是将我们的培训设置为正确的格式。

162
00:10:35,170 --> 00:10:40,810
那就是我们要训练和设置由第一到两个提取特征组成的两个新独立

163
00:10:40,810 --> 00:10:44,800
变量，然后减少因变量类的位置。

164
00:10:44,830 --> 00:10:46,820
除了客户群，别无其他。

165
00:10:46,930 --> 00:10:52,920
因此，基本上，我们在这里需要做的与我们在索引中使用的PCa所做的一样

166
00:10:53,190 --> 00:10:58,580
不仅可以为列设置正确的顺序，还可以在此处不包括三列。

167
00:10:58,580 --> 00:11:01,140
所以你想追求三个。

168
00:11:01,180 --> 00:11:10,630
因此，为了提高效率，我们将采用PCA模型，并将此行复制并

169
00:11:11,350 --> 00:11:14,530
回到LGA并在此处进行分页。

170
00:11:14,710 --> 00:11:15,150
好吧。

171
00:11:15,170 --> 00:11:22,160
现在，对于PCa，我们需要在今年和明年将三个指标包括在内。

172
00:11:22,160 --> 00:11:22,650
1。

173
00:11:22,660 --> 00:11:29,110
因此，该列的索引为1 2 3 4，而5为索引5。

174
00:11:29,320 --> 00:11:30,720
所以我们在这里添加它。

175
00:11:30,790 --> 00:11:32,960
将2替换为5。

176
00:11:33,010 --> 00:11:39,010
然后，这里的第二个索引应该是您提取的特征Elda 2的第二个索引。

177
00:11:39,010 --> 00:11:40,610
索引为6。

178
00:11:40,630 --> 00:11:42,250
该列的索引为6。

179
00:11:42,430 --> 00:11:45,670
因此，让我们用3替换您的3。

180
00:11:45,670 --> 00:11:46,550
好吧。

181
00:11:46,600 --> 00:11:53,380
最终这应该是因变量的索引，而该索引当然是一个，因为

182
00:11:53,380 --> 00:11:56,040
这是这里没有x1的第一列。

183
00:11:56,380 --> 00:12:01,930
所以现在当我在这里执行此行时，我们来看一下新的交易集。

184
00:12:02,200 --> 00:12:04,770
好吧，这正是我们想要的。

185
00:12:04,930 --> 00:12:09,640
前两列是新提取的功能，最后一列具有不可估量的价值。

186
00:12:09,640 --> 00:12:14,040
完全按照其余代码节中的预期进行向量处理。

187
00:12:14,410 --> 00:12:15,290
那么完美。

188
00:12:15,410 --> 00:12:21,430
我们的火车经过了很好的改造，可以用于训练SBA模型。

189
00:12:21,490 --> 00:12:21,880
好吧。

190
00:12:21,880 --> 00:12:23,780
现在，我们需要对测试集执行相同的操作。

191
00:12:23,800 --> 00:12:25,630
因此，这将是非常快速和容易的。

192
00:12:25,690 --> 00:12:28,390
我们将在这里选择这两只手。

193
00:12:28,390 --> 00:12:36,510
复制粘贴，现在我们只需要在这里用测试集替换训练集即可。

194
00:12:37,660 --> 00:12:45,450
在这里，最终在这里和现在，我们可以只问您这两行，但让我们逐一

195
00:12:45,450 --> 00:12:46,240
一。

196
00:12:46,260 --> 00:12:50,510
到目前为止，这是由13个组成的测试集，它们非常宝贵。

197
00:12:50,520 --> 00:12:51,820
原来的。

198
00:12:52,080 --> 00:13:00,150
然后，当我们选择这一行并执行得很好时，我们只会得到两个我们想要提取的新特征

199
00:13:00,150 --> 00:13:03,780
知道方程的两个和三个变量。

200
00:13:03,870 --> 00:13:05,850
当然，这是在圣经课上发生的。

201
00:13:06,090 --> 00:13:12,660
然后，当我们再次执行此操作以正确的顺序获取正确的索引时，我们将执行此操作。

202
00:13:12,660 --> 00:13:15,410
现在，我们得到了包含两者的测试。

203
00:13:15,450 --> 00:13:20,610
您可以提取第一位置的老师Elodie一个和另外两个，以及因变量

204
00:13:20,660 --> 00:13:21,670
在最后位置。

205
00:13:21,930 --> 00:13:23,070
太完美了。

206
00:13:23,070 --> 00:13:28,820
现在，我们准备执行其余部分以构建我们的SVM模型。

207
00:13:28,830 --> 00:13:29,990
好吧，让我们做吧。

208
00:13:30,030 --> 00:13:35,080
实际上，在本节中，我们没有太多需要更改的内容，您认为我们需要更改某些内容吗？

209
00:13:35,220 --> 00:13:42,150
答案是肯定的，因为请记住因变量不再称为客户群。

210
00:13:42,150 --> 00:13:47,790
即使这次客户说的是一个变量，它也有不同的名称，即class和

211
00:13:47,790 --> 00:13:53,010
实际上，这是我们唯一需要更改的事情，因为训练集仍然具有相同的名称。

212
00:13:53,070 --> 00:13:55,110
这是火车，我们只是这里的朋友。

213
00:13:55,230 --> 00:13:56,080
这样很好。

214
00:13:56,100 --> 00:14:01,700
然后它是相同的类型和相同的内核，因为我们正在构建线性模型或作为VM模型。

215
00:14:01,710 --> 00:14:06,030
好吧，太完美了，让我们执行此部分吧。

216
00:14:06,120 --> 00:14:11,660
创建完成的模型，现在我们可以预测测试结果了。

217
00:14:11,670 --> 00:14:14,960
因此，测试结果是否需要在此处进行一些更改。

218
00:14:15,180 --> 00:14:20,480
那么这次的答案是否定的，因为我们有了测试转换。

219
00:14:20,490 --> 00:14:23,230
它具有与此名称相同的名称和分类器。

220
00:14:23,370 --> 00:14:24,630
因此，一切都是完美的。

221
00:14:24,690 --> 00:14:29,430
我们准备好执行这行完美的代码。

222
00:14:29,610 --> 00:14:30,860
同样在这里。

223
00:14:30,960 --> 00:14:32,360
我们不需要更改任何内容。

224
00:14:32,370 --> 00:14:36,960
我们可以通过执行此行来制作混淆矩阵。

225
00:14:36,960 --> 00:14:37,620
开始了。

226
00:14:37,610 --> 00:14:39,170
创建混乱矩阵。

227
00:14:39,300 --> 00:14:43,200
让我们看看我们是否还能获得100％的准确性。

228
00:14:43,200 --> 00:14:50,310
我们将能够在手电筒中看到它，因为如果有一个错误的预测，那么这意味着

229
00:14:50,310 --> 00:14:54,590
我们将无法获得与PCa一样的100％准确性。

230
00:14:54,600 --> 00:14:57,430
因此，这将不会像PCA那样完美。

231
00:14:57,600 --> 00:15:01,460
因此，让我们输入C.M。 在这里，然后按Enter。

232
00:15:01,770 --> 00:15:05,880
不幸的是，我们在这里得到了一个错误的预测。

233
00:15:06,180 --> 00:15:12,570
但这没什么大不了的，因为不仅一个错误的预测仍然很好，而且

234
00:15:12,750 --> 00:15:15,830
请记住，当我们可视化PCa产生的趋势时。

235
00:15:16,020 --> 00:15:17,760
好吧，我们有错误的预测。

236
00:15:17,760 --> 00:15:22,620
因此，我们很幸运今年得到了零个错误的预测。

237
00:15:22,740 --> 00:15:23,390
好吧

238
00:15:23,410 --> 00:15:25,030
这样仍然效果极佳。

239
00:15:25,050 --> 00:15:27,640
现在，让我们可视化所产生的趋势。

240
00:15:27,690 --> 00:15:30,080
现在我们需要在此部分中进行一些更改。

241
00:15:30,240 --> 00:15:36,600
好吧，尝试弄清楚是否可以，因为如果您使用本节，了解这一点很重要

242
00:15:36,600 --> 00:15:40,290
在此处可以在数据集上可视化问题的结果。

243
00:15:40,560 --> 00:15:48,570
好吧，这次的答案是肯定的，因为这里的这行代码调用名称应该具有真实的

244
00:15:48,570 --> 00:15:50,610
您的自变量的名称。

245
00:15:50,600 --> 00:15:53,540
新提取的特征L.D. 你想要一个替代..

246
00:15:53,700 --> 00:16:01,530
因此，在这里我们需要更换一台PC，并确保分别购买L.D.的X。 一个，因为那是

247
00:16:01,530 --> 00:16:08,910
第一个提取特征的名称，第一个新自变量和X表示L.D. 到第二

248
00:16:09,180 --> 00:16:10,300
您将其提取。

249
00:16:10,330 --> 00:16:12,200
第二个自变量。

250
00:16:12,270 --> 00:16:22,090
因此，让我们将它们替换为X，再替换为一个，然后决定购买L.D.的X。  2。

251
00:16:22,350 --> 00:16:27,000
因此，这非常重要，这是您唯一想更改的呼叫名称的唯一方法

252
00:16:27,300 --> 00:16:29,500
自变量的真实名称。

253
00:16:29,550 --> 00:16:33,450
当可视化结果时，您将需要更改其他内容。

254
00:16:33,450 --> 00:16:39,750
好吧，我们也可以更改，但这不是强制性的，仅用于x轴和

255
00:16:39,750 --> 00:16:40,660
y轴。

256
00:16:40,680 --> 00:16:41,870
所以我们还是去做吧。

257
00:16:41,910 --> 00:16:45,570
而且这其中一些我们不需要指定自变量的真实名称。

258
00:16:45,570 --> 00:16:47,400
我们可以替换一个。

259
00:16:47,390 --> 00:16:52,900
另外，您想指定它是线性判别式而不是主要成分。

260
00:16:52,980 --> 00:16:53,950
和这里一样。

261
00:16:54,000 --> 00:16:56,710
我们可以用L.D.代替两个 二。

262
00:16:56,730 --> 00:16:57,100
好吧。

263
00:16:57,120 --> 00:16:58,110
现在完成了。

264
00:16:58,140 --> 00:17:00,260
现在，该代码可以执行了。

265
00:17:00,300 --> 00:17:05,350
因此，让我们对测试结果的可视化进行快速相同的更改。

266
00:17:05,520 --> 00:17:17,790
因此，让我们在这里用x替换一个，它将执行一个，然后用X LG 2替换pc 2，然后用一个替换一个，再替换两个PC

267
00:17:18,270 --> 00:17:19,700
由两个。

268
00:17:19,920 --> 00:17:23,340
现在一切准备就绪，我们无需再进行任何更改。

269
00:17:23,340 --> 00:17:28,720
我们可以喝杯咖啡，然后可视化训练结果和测试结果。

270
00:17:28,770 --> 00:17:29,840
所以做吧。

271
00:17:30,060 --> 00:17:31,930
希望一切都很好。

272
00:17:32,010 --> 00:17:35,510
因此，我将选择此处的所有部分。

273
00:17:35,720 --> 00:17:38,650
如此可视化整理结果，让我们开始吧。

274
00:17:38,680 --> 00:17:40,180
现在我们开始。

275
00:17:40,540 --> 00:17:40,890
好吧。

276
00:17:40,900 --> 00:17:47,650
所以它执行起来总是需要一点时间，但是我们会到达那里，我们已经可以点击绘图了

277
00:17:47,990 --> 00:17:56,190
可能是车站即将完工，这是美丽的结果。

278
00:17:56,350 --> 00:18:00,250
这三个类别几乎完全分开，完全分开。

279
00:18:00,250 --> 00:18:05,420
我们可以看到错误的预测，但请注意，这与我们看到的错误预测不一样

280
00:18:05,420 --> 00:18:08,710
令人困惑的矩阵，因为它涉及测试集。

281
00:18:08,740 --> 00:18:13,800
这是训练的结果，因此我们对趋势集也有一个不正确的预测。

282
00:18:13,800 --> 00:18:16,180
就是这个，但这几乎是完美的。

283
00:18:16,180 --> 00:18:22,570
理解起来很直观，因为您记得LDH尝试将大多数类分开

284
00:18:22,660 --> 00:18:28,720
的因变量，所以这就是为什么我们在这里可以看到预测边界是等距的

285
00:18:28,720 --> 00:18:33,200
到这里的大多数绿色点和这里的大多数点。

286
00:18:33,370 --> 00:18:34,840
太完美了。

287
00:18:34,840 --> 00:18:40,720
每种葡萄酒都属于其正确的客户群，因此，这种葡萄酒企业主可以感觉很漂亮

288
00:18:40,720 --> 00:18:47,710
有信心预测每种新酒的客户群，因此他应该推荐而不是

289
00:18:47,710 --> 00:18:52,930
只有他可以非常有信心向合适的顾客推荐新酒，同时还要感谢

290
00:18:52,930 --> 00:18:57,960
特征提取技术使他可以在两个维度上可视化结果。

291
00:18:58,020 --> 00:18:58,540
谢谢。

292
00:18:58,570 --> 00:19:02,560
判别式中的第二个新自变量。

293
00:19:02,710 --> 00:19:08,890
现在，这位葡萄酒企业主可以清楚地说明其不同的客户群，

294
00:19:08,890 --> 00:19:14,680
在每个客户群中都有不同的客户，因此最终可以非常方便。

295
00:19:15,040 --> 00:19:16,330
好吧，太完美了。

296
00:19:16,390 --> 00:19:21,890
我们设法建立了一个出色的LDA模型，因此我们会在此方面做得很好，因此我们将继续

297
00:19:21,890 --> 00:19:26,660
到本课程的下一部分，这将是另一种特征提取技术。

298
00:19:26,740 --> 00:19:29,790
但这一次适用于非线性问题。

299
00:19:29,810 --> 00:19:37,120
因此，由于此问题显然是线性问题，因为我们设法非常成功地应用了Lynnie

300
00:19:37,120 --> 00:19:44,050
我们的模型SPCA和LDK会不会适用，将来在此模型上的提取模型

301
00:19:44,050 --> 00:19:44,820
数据集。

302
00:19:45,010 --> 00:19:46,900
因此，我们将处理另一个数据集。

303
00:19:46,960 --> 00:19:52,300
当然会有非线性，而我们将要使用的下一个新的特征提取技术

304
00:19:52,300 --> 00:19:55,170
看到将是内核PCA。

305
00:19:55,360 --> 00:19:57,700
因此，我期待在下一节中开始。

306
00:19:57,850 --> 00:19:59,620
在那之前，享受机器学习。

