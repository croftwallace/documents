1
00:00:00,330 --> 00:00:02,550
您好，欢迎来到本Python教程。

2
00:00:02,820 --> 00:00:06,990
好了，我们完成了添加人工神经网络的各层的工作。

3
00:00:06,990 --> 00:00:12,810
现在我们要做的是编译基本上在应用中的整个人工神经网络

4
00:00:12,810 --> 00:00:16,650
在整个人工神经网络上，随机意义上的。

5
00:00:16,740 --> 00:00:18,120
那就是编译的意思。

6
00:00:18,150 --> 00:00:22,890
这就是我们将要添加的单行代码的地方。

7
00:00:22,890 --> 00:00:24,070
好吧，让我们开始吧。

8
00:00:24,090 --> 00:00:30,090
我们将以在类中添加对象作为开始，就像我们在

9
00:00:30,090 --> 00:00:35,640
然后我们现在将完全使用与add方法相同的另一种方法。

10
00:00:35,730 --> 00:00:37,950
但是这一次它不是方法。

11
00:00:38,040 --> 00:00:39,520
您可以猜测会是什么。

12
00:00:39,630 --> 00:00:42,050
这将是编译方法。

13
00:00:42,300 --> 00:00:43,550
就如此容易。

14
00:00:43,770 --> 00:00:47,190
因此，我们将在此处添加一个点，然后进行编译。

15
00:00:47,190 --> 00:00:48,130
就在这里。

16
00:00:48,180 --> 00:00:49,150
按回车。

17
00:00:49,380 --> 00:00:50,320
好吧。

18
00:00:50,430 --> 00:00:54,130
现在，当然，此编译方法包含几个参数。

19
00:00:54,180 --> 00:00:56,880
因此，我们在此处添加一些括号。

20
00:00:56,880 --> 00:01:01,980
因此，我将向您解释这些参数是什么，我们将一一添加。

21
00:01:01,980 --> 00:01:02,790
好吧。

22
00:01:02,790 --> 00:01:05,550
因此，让我们按一下这里来检查参数。

23
00:01:05,730 --> 00:01:06,800
和我。

24
00:01:07,200 --> 00:01:12,660
在这里，我们获得了有关此编译方法的一些信息以及我们感兴趣的信息

25
00:01:12,780 --> 00:01:14,460
当然是争论。

26
00:01:14,460 --> 00:01:14,760
好吧。

27
00:01:14,760 --> 00:01:19,980
因此，让我们看一下第一个参数，第一个参数是优化器。

28
00:01:19,980 --> 00:01:26,520
因此，优化器优化器就是您要用来查找最佳权重集的算法

29
00:01:26,820 --> 00:01:31,860
在神经网络中，因为您知道我们定义了我们的神经网络，它是由不同的

30
00:01:31,860 --> 00:01:36,230
层，但权重仍仅初始化。

31
00:01:36,390 --> 00:01:43,080
因此，现在我们必须对您应用某种算法，以便找出可以使我们

32
00:01:43,080 --> 00:01:45,070
神经网络功能最强大。

33
00:01:45,210 --> 00:01:50,910
所以这就是为什么这里的编译方法要添加一个算法，而该算法将是

34
00:01:50,910 --> 00:01:56,700
没什么别的，只是要对感知算法进行谴责，并且有几种类型的随机

35
00:01:56,700 --> 00:02:02,160
出色的血统算法和非常高效的算法称为Adam，这正是正在发生的事情

36
00:02:02,160 --> 00:02:05,490
作为此优化参数的输入。

37
00:02:05,490 --> 00:02:10,150
好的，现在让我们开始吧，优化器等于。

38
00:02:10,230 --> 00:02:13,360
因此，现在我们需要添加引号亚当。

39
00:02:13,470 --> 00:02:14,690
开始了。

40
00:02:14,760 --> 00:02:16,170
伟大然后出现。

41
00:02:16,180 --> 00:02:18,070
现在，我们添加第二个参数。

42
00:02:18,180 --> 00:02:20,130
所以第二个参数是损失。

43
00:02:20,190 --> 00:02:26,130
这对应于随机函数内的最后一个函数，这是一种算法

44
00:02:26,130 --> 00:02:31,770
之所以在原子算法之内，是因为如果您深入研究随机数的数学细节，

45
00:02:31,770 --> 00:02:37,260
从某种意义上讲，您将看到它基于丢失的功能，您需要对其进行优化以找到

46
00:02:37,350 --> 00:02:38,420
最佳权重。

47
00:02:38,640 --> 00:02:42,420
这就是我们现在必须指定的最后一个功能。

48
00:02:42,570 --> 00:02:45,100
因此，您知道我可以举一些例子。

49
00:02:45,120 --> 00:02:47,840
在研究线性回归时，您看到了最后一个函数。

50
00:02:47,880 --> 00:02:50,330
记住不可治愈的直觉教程。

51
00:02:50,430 --> 00:02:53,200
最后一个函数是方形箭头的总和。

52
00:02:53,310 --> 00:02:57,820
那就是实际值和预测值之间的平方差之和。

53
00:02:57,990 --> 00:03:03,280
好吧，这是我们用来优化回归模型参数的最后一个函数，

54
00:03:03,280 --> 00:03:04,500
完全相同的年份。

55
00:03:04,500 --> 00:03:09,960
我们有一些参数，这些参数是神经网络中的权重，因此我们需要指定

56
00:03:09,960 --> 00:03:15,240
最后一个函数，从某种意义上说，我们将根据自己的随机率最终找到最优

57
00:03:15,240 --> 00:03:15,940
重量。

58
00:03:16,190 --> 00:03:19,500
因此，它将成为神经网络的最后一个功能。

59
00:03:19,650 --> 00:03:24,840
再一次，如果您想更深入地研究随机伟大的数学细节，

60
00:03:24,840 --> 00:03:29,550
最后一个函数将类似于逻辑回归，因为当您采用

61
00:03:29,550 --> 00:03:35,700
一个只有一个神经元的简单神经网络，使其成为一种感知模型，如果您使用S型

62
00:03:35,730 --> 00:03:41,850
这个感知的激活函数嗯，你说一个逻辑回归模型，然后

63
00:03:41,850 --> 00:03:47,580
在逻辑回归模型的意义上更深入地探讨了随机数的数学细节

64
00:03:47,880 --> 00:03:52,560
好吧，您会发现最后一个函数不是平方误差的平方和。

65
00:03:52,560 --> 00:03:59,930
回归，但这将是较低的Gareth的MC函数，称为对数损失。

66
00:04:00,100 --> 00:04:05,250
因此，由于输出层的激活函数就是S型函数。

67
00:04:05,340 --> 00:04:10,710
好，我们将在此处使用的最后一个函数是，然后在其上是随机的

68
00:04:10,710 --> 00:04:16,410
从某种意义上说，算法很好地基于最后一个用于编译或A-N的函数

69
00:04:16,530 --> 00:04:19,220
也将成为对数类。

70
00:04:19,320 --> 00:04:20,400
这叫什么名字。

71
00:04:20,420 --> 00:04:22,020
Garrett McLaws在这里起作用。

72
00:04:22,170 --> 00:04:28,050
好吧，如果您的因变量具有二进制结果，那么忽略McLaws函数就称为二进制

73
00:04:28,110 --> 00:04:29,820
强调Krus熵。

74
00:04:30,000 --> 00:04:35,120
如果您的因变量具有两个以上的结果（如三个类别），则此对数

75
00:04:35,310 --> 00:04:39,360
该函数称为分类下划线交叉熵。

76
00:04:39,360 --> 00:04:40,500
好的，这是个好消息。

77
00:04:40,500 --> 00:04:43,870
现在您知道了在这里为最后一个功能使用什么输入。

78
00:04:43,950 --> 00:04:47,840
如果您正在使用具有两个以上类别的因变量。

79
00:04:48,090 --> 00:04:56,250
但是这里我们有一个二进制结果单输入，这里引号中的最后一个参数是二进制下划线

80
00:04:56,370 --> 00:04:58,200
交叉熵。

81
00:04:58,380 --> 00:04:59,720
好的，然后引用。

82
00:04:59,810 --> 00:05:00,820
大。

83
00:05:01,200 --> 00:05:01,830
完善。

84
00:05:01,830 --> 00:05:05,180
现在我们需要添加第三个也是最后一个参数。

85
00:05:05,400 --> 00:05:07,890
这是此指标参数。

86
00:05:07,890 --> 00:05:10,050
那么什么是度量参数。

87
00:05:10,140 --> 00:05:14,390
好吧，这很简单，这只是您选择评估模型的标准。

88
00:05:14,550 --> 00:05:19,740
通常，我们使用在本课程中已经多次练习的准确性标准。

89
00:05:19,740 --> 00:05:24,750
所以基本上发生的是，在每次观察后或每批之后对重量进行日期确定

90
00:05:24,810 --> 00:05:31,310
在许多观察结果中，算法使用此准确性标准来改善模型性能。

91
00:05:31,410 --> 00:05:37,470
您将看到，当我们将辅助工具安装到我们的训练集中时，准确性将会提高

92
00:05:37,710 --> 00:05:43,800
直到达到最高精度为止，这会逐渐发生，因为我们在这里选择精度

93
00:05:43,800 --> 00:05:44,850
指标。

94
00:05:44,850 --> 00:05:46,850
好的，让我们在此处添加此参数。

95
00:05:46,980 --> 00:05:53,420
指标等于，并且因为实际上此指标参数期望使用指标列表。

96
00:05:53,540 --> 00:05:59,490
但是在这里，我们仅使用一个指标，即我们需要在其中添加此精度指标的精度指标

97
00:05:59,880 --> 00:06:05,850
方括号，因为通过添加这样的方括号，我们正在创建一个列表，但是此列表将仅包含

98
00:06:05,850 --> 00:06:10,440
一个要素，即精度指标。

99
00:06:10,440 --> 00:06:10,760
好吧。

100
00:06:10,760 --> 00:06:11,440
完善。

101
00:06:11,550 --> 00:06:14,910
现在我们准备编译阿们了。

102
00:06:14,910 --> 00:06:15,900
因此，让我们开始吧。

103
00:06:15,900 --> 00:06:19,030
我们将选择此行并执行。

104
00:06:19,170 --> 00:06:22,490
没事，好编译好。

105
00:06:22,500 --> 00:06:23,690
这样就完成了。

106
00:06:23,910 --> 00:06:30,270
实际上，下一步是本节中最激动人心的步骤，因为这是这一步

107
00:06:30,390 --> 00:06:32,550
我们将看到该算法的实际效果。

108
00:06:32,550 --> 00:06:37,380
我们将选择一些书籍，您知道我们训练或注视的次数，并且

109
00:06:37,380 --> 00:06:38,680
在整个训练集上。

110
00:06:38,760 --> 00:06:42,900
因此，从实际意义上讲，我们将看到这一点变得更好，即我们将看到如何

111
00:06:42,900 --> 00:06:49,620
我们训练有素的古老模型，以及如何提高其准确性以及每个时期的每一轮比赛。

112
00:06:49,620 --> 00:06:53,850
所以这将是非常令人兴奋的，这将是下一个教程的内容。

113
00:06:53,850 --> 00:06:55,440
在那之前，您正在学习。

