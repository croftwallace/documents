1
00:00:00,300 --> 00:00:02,290
您好，欢迎回到本课程和深度学习。

2
00:00:02,430 --> 00:00:07,980
今天，我们谈论的是神经元，它是人工神经网络的基本组成部分。

3
00:00:08,010 --> 00:00:09,390
因此，让我们开始吧。

4
00:00:09,390 --> 00:00:11,340
以前，我们看到的图像看起来像这样。

5
00:00:11,340 --> 00:00:18,300
这些是实际的现实生活中的神经元，已被涂抹到光泽和颜色上，

6
00:00:18,360 --> 00:00:19,950
通过显微镜观察它们。

7
00:00:19,950 --> 00:00:22,140
这就是您所看到的样子。

8
00:00:22,140 --> 00:00:29,730
人体非常有趣的结构，然后有许多不同的尾巴状分支

9
00:00:29,730 --> 00:00:30,250
他们。

10
00:00:30,330 --> 00:00:32,350
这很有趣。

11
00:00:32,370 --> 00:00:38,400
但是问题是我们如何在机器中重新创建它，因为我们确实需要重新创建它，并且

12
00:00:38,400 --> 00:00:47,610
机器，因为深度学习的全部目的是模仿人脑的工作方式，希望

13
00:00:47,820 --> 00:00:51,000
通过这样做，我们将创造出惊人的东西。

14
00:00:51,000 --> 00:00:55,200
我们将为机器学习提供一个惊人的基础架构。

15
00:00:55,230 --> 00:00:56,800
以及为什么我们希望如此。

16
00:00:56,820 --> 00:01:03,480
好吧，因为人的大脑很好，恰好是最强大的学习学习工具之一

17
00:01:03,880 --> 00:01:07,300
在地球上或类似的学习机制。

18
00:01:07,320 --> 00:01:11,310
我们只是希望，如果我们重新创建，我们将拥有如此出色的产品。

19
00:01:11,310 --> 00:01:17,670
因此，我们目前面临的挑战是创建人工神经网络的第一步

20
00:01:17,700 --> 00:01:18,380
神经元。

21
00:01:18,390 --> 00:01:19,090
那么我们该怎么做。

22
00:01:19,110 --> 00:01:23,840
好吧，首先让我们仔细看看它到底是什么。

23
00:01:23,880 --> 00:01:33,180
该图像由西班牙神经科学家和Chagga Ramon Yi Kajal于1899年首次创建。

24
00:01:33,180 --> 00:01:37,780
他所做的是，他死于实际脑组织的神经元中。

25
00:01:37,780 --> 00:01:39,850
并在显微镜下观察它们。

26
00:01:39,900 --> 00:01:43,530
当他看着他们时，他实际上画了他所看到的，这就是他所看到的。

27
00:01:43,530 --> 00:01:49,560
他看到它在您的手或顶部那两个大神经元的手中，所有这些分支都在

28
00:01:49,560 --> 00:01:57,930
从他们的顶端部分，然后每个人都有这个Araud或类似线程出来

29
00:01:57,930 --> 00:01:59,410
底部很长的一个。

30
00:01:59,520 --> 00:02:01,510
这就是他所看到的。

31
00:02:01,660 --> 00:02:07,800
现在您知道技术已经取得了很大进步，而且我们已经更详细地看到了神经元

32
00:02:07,800 --> 00:02:11,890
现在我们可以实际绘制它的图表了。

33
00:02:11,910 --> 00:02:13,220
因此，让我们看一下。

34
00:02:13,440 --> 00:02:14,190
这是一个神经元。

35
00:02:14,190 --> 00:02:21,810
这看起来很像圣地亚哥周围到处散布的东西

36
00:02:21,810 --> 00:02:24,310
一年，我们可以看到它有一个身体。

37
00:02:24,570 --> 00:02:29,100
那是神经元的主要部分，然后在顶部有一些分支，称为树突

38
00:02:29,160 --> 00:02:33,200
它也有一个X，代表欧元的长尾巴。

39
00:02:33,300 --> 00:02:38,030
那么，这些树突是什么？当异物成为很好的轴突时。

40
00:02:38,130 --> 00:02:44,040
这里要理解的关键是神经元本身几乎没有用。

41
00:02:44,040 --> 00:02:45,570
就像它就像一只蚂蚁。

42
00:02:45,600 --> 00:02:46,140
对。

43
00:02:46,170 --> 00:02:49,640
靠它自己可以同时投放五个心理广告。

44
00:02:49,830 --> 00:02:51,170
也许他们可以拿起东西。

45
00:02:51,190 --> 00:02:55,830
但是，他们又一次不愿意建立蚁丘，或者称他们建立无法建立的殖民地。

46
00:02:56,430 --> 00:02:59,340
作为一个巨大的有机体一起工作。

47
00:02:59,370 --> 00:03:03,510
但与此同时，当您有很多广告时，就像您拥有一百万个广告一样，它们可以制作

48
00:03:03,510 --> 00:03:05,680
他们可以建立一个整个殖民地的蚁丘。

49
00:03:05,680 --> 00:03:06,600
神经元也是一样。

50
00:03:06,600 --> 00:03:12,320
它本身并没有那么强壮，但是当您有很多神经元在一起时，它们会共同发挥作用。

51
00:03:12,510 --> 00:03:13,820
以及他们如何一起工作。

52
00:03:13,820 --> 00:03:14,430
就是那个问题。

53
00:03:14,440 --> 00:03:19,140
嗯，这就是树枝状晶体和Aksenov的作用，所以树枝状晶体就像是

54
00:03:19,140 --> 00:03:22,980
神经元和轴突的信号是神经元信号的发送器。

55
00:03:23,220 --> 00:03:26,520
这是所有概念如何工作的图像。

56
00:03:26,520 --> 00:03:32,550
因此，在顶部，您可以独自行走，并且可以看到，树突与其他轴突相连

57
00:03:32,550 --> 00:03:35,990
更远的神经元。

58
00:03:36,000 --> 00:03:42,930
然后来自您自己的信号沿其轴突传播，并连接或传递至

59
00:03:42,930 --> 00:03:44,960
下一个神经元，那就是它们如何连接的。

60
00:03:45,030 --> 00:03:53,040
在那张小图上，您可以看到轴突实际上并没有接触到树枝状晶体。

61
00:03:53,310 --> 00:03:59,130
许多机器学习或一些机器学习科学家都非常坚信这一事实

62
00:03:59,130 --> 00:04:03,650
它不会像它不碰的房间那样碰它。

63
00:04:03,660 --> 00:04:06,890
已经证明那里没有物理连接。

64
00:04:06,960 --> 00:04:14,010
但是，我们感兴趣的一点是，它们之间的联系

65
00:04:14,010 --> 00:04:19,590
传递的信号使您回想起签收，您可以在那张小图像中看到

66
00:04:20,300 --> 00:04:22,210
图括号是一个注册。

67
00:04:22,230 --> 00:04:23,820
这就是我们将要做的趋势。

68
00:04:23,820 --> 00:04:29,820
因此，与其称人工神经元为线，不如称其为连接器

69
00:04:29,820 --> 00:04:34,200
人造神经元，我们现在将其称为轴突或树突，因为那时的问题是

70
00:04:34,200 --> 00:04:36,880
这与神经元是神经元有关。

71
00:04:36,990 --> 00:04:39,340
我们只称这种好，称其为细胞迹象是好的。

72
00:04:39,510 --> 00:04:42,680
这只是在回答所有问题。

73
00:04:42,690 --> 00:04:47,610
基本上，传递信号的位置与该元素属于谁无关。

74
00:04:47,610 --> 00:04:51,550
它们只是信号传递的代表，我们现在将看到。

75
00:04:51,960 --> 00:04:55,210
因此，基本上，这就是神经元的工作方式。

76
00:04:55,210 --> 00:05:03,390
是的，让我们继续介绍如何代表神经元在机器中创建神经元

77
00:05:03,390 --> 00:05:04,690
所以我们要离开了。

78
00:05:04,690 --> 00:05:09,420
现在，我们正在从神经科学转向技术。

79
00:05:09,460 --> 00:05:10,260
现在我们开始。

80
00:05:10,360 --> 00:05:17,260
所以这是我们的神经元，有时也称为节点，那么您自己的神经元会获得一些输入信号，并且

81
00:05:17,260 --> 00:05:18,400
输出信号。

82
00:05:18,400 --> 00:05:21,040
所以树突和轴突记得。

83
00:05:21,040 --> 00:05:27,490
但是再次，我们将调用这些注册，然后将这些输入信号呈现给他们

84
00:05:27,490 --> 00:05:29,040
其他神经元。

85
00:05:29,080 --> 00:05:35,500
因此，在这种特定情况下，您可以看到该神经元是绿色的，您所处的状态是从黄色获取信号

86
00:05:35,500 --> 00:05:35,850
神经元。

87
00:05:35,860 --> 00:05:41,800
在本课程中，我们将尝试遵循某种颜色编码方式，其中黄色表示

88
00:05:41,830 --> 00:05:42,540
输入层。

89
00:05:42,540 --> 00:05:50,700
所以基本上所有在第一层外层的神经元都是信号

90
00:05:50,710 --> 00:05:52,300
通过信号进入。

91
00:05:52,300 --> 00:05:59,200
将此信号称为信号基本上只是输入值，可能有点过头了。

92
00:05:59,340 --> 00:06:04,720
因此，您知道，即使在简单的线性回归中，您也有输入值，然后有预测

93
00:06:04,720 --> 00:06:05,620
值这里同样。

94
00:06:05,620 --> 00:06:10,720
所以您有输入值，它们是黄色的，然后在右侧您会看到

95
00:06:10,720 --> 00:06:11,260
它会是红色的。

96
00:06:11,260 --> 00:06:12,690
这将是输出值。

97
00:06:13,570 --> 00:06:17,140
我想在此指出的另一件事是，在此特定示例中，我们正在查看

98
00:06:17,140 --> 00:06:21,320
神经元从输入层神经元获取信号。

99
00:06:21,320 --> 00:06:24,220
也有神经元，但它们输入神经元。

100
00:06:24,520 --> 00:06:31,450
有时您会有神经元从其他隐藏层神经元获得信号，因此从其他绿色

101
00:06:31,450 --> 00:06:35,860
神经元和概念将完全相同，我的意思是在这种情况下，我们为了简单起见

102
00:06:35,860 --> 00:06:42,830
因此，我们在描绘此示例，并且就输入层而言，考虑它的方法是在

103
00:06:42,970 --> 00:06:49,900
类似于人脑，输入层是您的正确感觉，因此无论您看到什么，都能听到

104
00:06:49,900 --> 00:06:52,280
感到触摸或闻到。

105
00:06:52,510 --> 00:06:57,220
当然，就像有很多东西可以看到，有很多信息

106
00:06:57,220 --> 00:06:57,540
在。

107
00:06:57,730 --> 00:07:02,870
但是，这些就是你的。那就是你的大脑所能限制的几乎是一生。

108
00:07:03,010 --> 00:07:09,160
以色列生活在一个用骨头做成的盒子里，这只是思考的想法

109
00:07:09,160 --> 00:07:15,430
关于您的大脑只是被锁在一个黑匣子中，并且它唯一可以看到的就是听到的声音。

110
00:07:15,430 --> 00:07:20,920
唯一得到的是来自这些器官的电脉冲，我们应该称之为

111
00:07:20,920 --> 00:07:28,210
您的耳朵和鼻子，您知道您的触觉，以及您和您的口味。

112
00:07:28,220 --> 00:07:34,150
对，所以它只是在接收信号，但是它基本上生活在这个暗黑盒子中，

113
00:07:34,150 --> 00:07:38,460
通过您的感觉对世界的感知是惊人的。

114
00:07:38,500 --> 00:07:38,930
是的，是的。

115
00:07:38,950 --> 00:07:43,030
因此，您输入的这些都是人脑方面的信息。

116
00:07:43,030 --> 00:07:49,540
这些就是您的五种感官，从机器学习或深度学习的角度来讲，这基本上就是您的

117
00:07:49,900 --> 00:07:55,520
输入值是您的自变量，我们将在一秒钟内得到它，因此您的输入值

118
00:07:56,400 --> 00:08:01,090
信号通过正负窦传递到神经元，然后您自己的输出值

119
00:08:01,090 --> 00:08:03,190
沿着链条进一步走下去。

120
00:08:03,550 --> 00:08:07,990
在这种特定情况下，就颜色编码而言，黄色再次表示输入层，因此我们简化了

121
00:08:07,990 --> 00:08:11,830
我们在这里所说的一切，我们只会像输入层一样，然后我们将拥有

122
00:08:11,830 --> 00:08:16,510
一个带有绿色的隐藏层，绿色是腹地，那么我们将在此处获得输出

123
00:08:16,510 --> 00:08:17,530
远。

124
00:08:17,530 --> 00:08:21,360
因此，我们现在就可以习惯这些调用。

125
00:08:21,580 --> 00:08:24,030
因此，我们要走的就是基本结构。

126
00:08:24,030 --> 00:08:28,390
因此，现在让我们详细介绍一下我们拥有的这些不同元素。

127
00:08:28,390 --> 00:08:31,090
因此，我们有了输入层，在这里有了什么。

128
00:08:31,090 --> 00:08:37,090
好吧，我们有这些输入，它们实际上是独立变量，因此依赖于变量1和位变量

129
00:08:37,090 --> 00:08:42,760
对于自变量，这里要记住的重要一点是，这些自变量

130
00:08:42,790 --> 00:08:44,740
全部用于一个观察。

131
00:08:44,740 --> 00:08:47,620
因此，将其仅视为数据库中的一行。

132
00:08:47,620 --> 00:08:54,790
一个观察结果只是您获取了所有自变量，您可能知道这是一个人的年龄

133
00:08:54,820 --> 00:09:01,270
银行帐户中有多少钱，然后他们如何开车或步行上班

134
00:09:01,270 --> 00:09:03,060
使用洗发水的保护。

135
00:09:03,070 --> 00:09:08,800
因此，这就是一个特定人的全部描述，您要么在训练模型

136
00:09:09,130 --> 00:09:12,520
或您正在执行一些预测。

137
00:09:12,610 --> 00:09:16,900
关于这些变量，您需要了解的另一件事是您需要对其进行标准化，以便

138
00:09:16,900 --> 00:09:21,310
需要标准化它们，这意味着确保它们的平均值为零，方差为1

139
00:09:21,340 --> 00:09:29,080
或者您有时也可以，岬角可能会在地图上更详细地指出这些痕迹

140
00:09:29,080 --> 00:09:33,310
您可能会遇到的实用术语这些有时您可能想知道标准化可能想要

141
00:09:33,310 --> 00:09:34,800
标准化它们。

142
00:09:34,990 --> 00:09:41,250
意思是，您不必确定均值和非常穆瑟的方差就是一个

143
00:09:41,260 --> 00:09:46,480
减去最小值，然后用最大值减去最小值除以值的范围

144
00:09:46,480 --> 00:09:49,210
并且这四个值在0到1之间。

145
00:09:49,510 --> 00:09:53,580
而这取决于这种情况，您可能想要做一个或另一个。

146
00:09:53,590 --> 00:10:00,700
但基本上，您希望所有这些变量在相同的值范围内都非常相似

147
00:10:00,760 --> 00:10:01,670
以及为什么。

148
00:10:01,690 --> 00:10:02,160
这是为什么。

149
00:10:02,180 --> 00:10:06,890
那么所有这些值都将进入神经网络，正如我们现在所看到的那样，

150
00:10:06,890 --> 00:10:13,190
加起来并乘以它，再加起来，依此类推，对于

151
00:10:13,190 --> 00:10:17,140
神经网络来处理它们，如果它们都差不多的话。

152
00:10:17,290 --> 00:10:23,770
而且事实上，您知道那就是它将如何正常工作的方式。

153
00:10:24,230 --> 00:10:29,210
如果您想了解有关标准化规范化的更多信息，以及

154
00:10:29,210 --> 00:10:36,440
您知道年轻的Licken将优质的补充阅读论文称为有效反向概率的哪些变量

155
00:10:37,030 --> 00:10:39,780
1998那里的链接。

156
00:10:39,860 --> 00:10:47,570
因此，我们实际上将在深度学习领域中进一步讨论这个非凡的人

157
00:10:47,680 --> 00:10:52,160
在本课程的一部分中，我们讨论卷积神经网络，您将看到

158
00:10:52,160 --> 00:10:55,240
绝对是一个知道他在说什么的人。

159
00:10:55,280 --> 00:11:00,830
他是杰弗里·欣顿（Jeffrey Hinton）的亲密朋友，我们已经见过他们非常昏暗。

160
00:11:00,860 --> 00:11:07,070
因此，在本文中，您将了解有关归一化的中心化的更多信息，但您也可以学到很多

161
00:11:07,070 --> 00:11:11,510
其他不同的技巧和窍门，随着您的到来，您将成为更多阅读的好来源

162
00:11:11,510 --> 00:11:12,120
经历这个。

163
00:11:12,120 --> 00:11:14,510
所以检查出来。

164
00:11:15,830 --> 00:11:22,090
如果您对此感兴趣，可以查看是否对其他阅读感兴趣。

165
00:11:22,160 --> 00:11:26,840
然后我们去处理变量。

166
00:11:26,840 --> 00:11:29,610
在这里，我们得到了输出值。

167
00:11:29,660 --> 00:11:31,610
那么我们的输出值是多少。

168
00:11:31,610 --> 00:11:34,400
好吧，我们有两个选择。

169
00:11:34,760 --> 00:11:35,900
好吧，我们有两个选择。

170
00:11:35,930 --> 00:11:41,740
输出值可以是连续的，例如价格，可以是二进制，例如一个人

171
00:11:41,740 --> 00:11:48,890
将会退出或停留，也可能是分类言语和物理蠕动分类言语。

172
00:11:48,890 --> 00:11:53,690
这里要记住的重要一点是，在这种情况下，您的输出值将不只是一个

173
00:11:53,690 --> 00:12:00,020
是几个输出值，因为它们将是一个虚拟变量，将代表您的类别

174
00:12:00,990 --> 00:12:07,220
这就是它的工作方式，重要的是要记住，在这种情况下，这就是你要怎么做

175
00:12:07,220 --> 00:12:11,910
使您的类别脱离人工神经网络。

176
00:12:12,440 --> 00:12:15,150
但是，让我们回到一个输出音量的简单情况。

177
00:12:15,380 --> 00:12:21,410
现在，让我们再谈一点，或者说一点类似现成的观点，我只想重申这一点。

178
00:12:21,410 --> 00:12:24,950
在左侧的点上，您只有一个观察结果。

179
00:12:25,100 --> 00:12:29,360
因此，我想知道您是从数据集中获取数据，还是从右侧获取数据呢？

180
00:12:29,390 --> 00:12:31,820
这是相同的观察。

181
00:12:31,820 --> 00:12:37,730
重要的是要记住，就像您输入的任何内容都是一行然后是输出一样

182
00:12:37,730 --> 00:12:39,600
您得到的是相同的确切行。

183
00:12:39,710 --> 00:12:44,080
或者，如果您正在训练您的神经网络，那么您会知道您正在为该角色输入信息

184
00:12:44,090 --> 00:12:46,080
您将输出放在那一行。

185
00:12:46,100 --> 00:12:52,760
因此，就像您要简化复杂性一样，可以将其视为简单回归或多元变量

186
00:12:52,910 --> 00:12:53,630
线性回归。

187
00:12:53,630 --> 00:12:57,730
因此，您输入值即可得到输出。

188
00:12:57,770 --> 00:12:59,330
那里毫无疑问。

189
00:12:59,330 --> 00:13:02,420
当我们谈论诸如回归之类的事情时，因为我们已经习惯了。

190
00:13:02,420 --> 00:13:04,600
同样的事情在这里并没有太复杂。

191
00:13:04,610 --> 00:13:06,370
我们只是输入要获取的值。

192
00:13:06,380 --> 00:13:10,940
但请记住，每次处理的都是一行，因此您不会感到困惑并开始

193
00:13:10,940 --> 00:13:17,570
像认为这些是要放入人工的不同行一样

194
00:13:17,570 --> 00:13:22,340
神经网络之类的东西，全都只是那个值。

195
00:13:22,340 --> 00:13:28,600
每一次与该观察有关的特征或属性。

196
00:13:28,790 --> 00:13:35,030
好的，接下来我们要在这里讨论的是UPS的标志，它表示我们在这里

197
00:13:35,030 --> 00:13:38,580
这样，它们实际上都会获得分配的权重权重。

198
00:13:38,610 --> 00:13:46,760
要进一步讨论有关权重的更多信息，但总之，权重对于人工神经网络至关重要

199
00:13:46,760 --> 00:13:53,690
网络并能正常工作，因为权重是神经网络通过调整权重学习的方式

200
00:13:54,100 --> 00:13:59,900
在每种情况下，神经网络都会决定哪个信号较差而哪个信号不重要

201
00:13:59,900 --> 00:14:00,770
对某些神经元。

202
00:14:00,770 --> 00:14:05,930
传递了什么单身，没有传递出什么信号，或者在什么程度上传递了什么力量

203
00:14:05,930 --> 00:14:07,400
信号被传递。

204
00:14:07,420 --> 00:14:08,950
因此权重至关重要。

205
00:14:08,960 --> 00:14:13,220
它们是并且它们是通过学习过程进行调整的事物。

206
00:14:13,220 --> 00:14:17,840
就像在训练人工神经网络时，基本上是在调整所有权重

207
00:14:18,200 --> 00:14:20,660
在整个神经网络中，所有迹象都表明。

208
00:14:20,840 --> 00:14:26,320
这就是梯度下降和反向传播起作用的地方。

209
00:14:26,320 --> 00:14:29,160
这些是我们也讨论过的概念。

210
00:14:29,300 --> 00:14:31,070
所以基本上这些都是权重。

211
00:14:31,070 --> 00:14:32,520
这就是我们现在需要知道的。

212
00:14:32,810 --> 00:14:38,050
而且我们拥有了神经元，因此信号进入了神经元以及欧元发生了什么。

213
00:14:38,060 --> 00:14:40,160
所以这是有趣的部分。

214
00:14:40,160 --> 00:14:43,300
就像我们今天在谈论神经元一样，神经元内部发生了什么。

215
00:14:43,520 --> 00:14:49,730
所以首先发生一些事情，第一步是它获得的所有这些价值

216
00:14:49,820 --> 00:14:50,980
被加起来。

217
00:14:50,980 --> 00:14:59,750
因此，它需要加起来，所以所有输入值的加权和变得非常简单，就是

218
00:14:59,750 --> 00:15:06,080
非常非常直接的方法是将它们相加，然后相加，然后应用激活

219
00:15:06,080 --> 00:15:06,830
功能。

220
00:15:06,830 --> 00:15:10,580
现在我们将进一步讨论激活函数，但基本上它是一个

221
00:15:10,580 --> 00:15:19,060
被分配给该神经元或整个层，并应用于加权的一些。

222
00:15:19,220 --> 00:15:26,810
然后，您不了解是否需要传递信号，那就是信号

223
00:15:26,810 --> 00:15:31,910
该函数适用于某些方法。

224
00:15:31,910 --> 00:15:36,050
但是基本上取决于功能，神经元要么传递信号，要么不传递

225
00:15:36,050 --> 00:15:37,160
信号打开。

226
00:15:37,520 --> 00:15:40,970
这正是在第三步中发生的情况。

227
00:15:41,090 --> 00:15:46,580
神经元将该信号传递到线下的下一个神经元，这就是我们要讨论的

228
00:15:46,580 --> 00:15:49,580
在下一个教程中，因为它是一个非常重要的主题。

229
00:15:49,580 --> 00:15:56,000
我们想更深入地研究激活功能，但希望目前一切都应该是

230
00:15:56,000 --> 00:16:01,250
很清楚你如何知道输入值，权重，设计公司，东西

231
00:16:01,310 --> 00:16:06,050
你知道在神经元中发生的事情，你已经加权了Sarmad并应用了激活函数，然后

232
00:16:06,050 --> 00:16:10,700
是在线传递的，并且不断地在整个神经网络中不断重复，并且

233
00:16:10,790 --> 00:16:16,490
你知道成千上万次取决于你有多少神经元

234
00:16:16,490 --> 00:16:19,200
有多少仰卧起坐使您的神经网络沉默。

235
00:16:19,200 --> 00:16:19,740
所以我们去了。

236
00:16:19,740 --> 00:16:22,780
希望您喜欢今天的Tauriel Coates。

237
00:16:22,860 --> 00:16:24,660
在此之前，请享受深度学习。

