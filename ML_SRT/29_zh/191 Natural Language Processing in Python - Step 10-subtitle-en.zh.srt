1
00:00:00,270 --> 00:00:02,840
您好，欢迎来到本Python教程。

2
00:00:02,940 --> 00:00:08,970
因此，在之前的教程中，我们做了自然语言处理的主要部分，因为我们创建了

3
00:00:09,030 --> 00:00:15,840
我们的布袋词模型使我们得到了特征X的稀疏矩阵。

4
00:00:15,840 --> 00:00:24,330
自变量的矩阵，其中包含1500列，并且也在我们创建稀疏矩阵之后

5
00:00:24,330 --> 00:00:29,110
特征矩阵我们还创建了因变量向量y。

6
00:00:29,370 --> 00:00:34,970
这里的这一权利包含审查的结果，无论是正面还是负面。

7
00:00:34,980 --> 00:00:39,640
因此，对于每个评论，1表示它为正，0表示它为负。

8
00:00:39,930 --> 00:00:45,870
因此，现在我们有了第三部分中的分类，即拥有独立矩阵

9
00:00:45,870 --> 00:00:53,990
此处的变量x在笔中包含1500，并且比我们在第3部分中拥有的要多得多。

10
00:00:54,120 --> 00:00:55,870
但这在数据科学中非常普遍。

11
00:00:55,890 --> 00:01:00,270
因此，我们完全可以以此为基础建立机械模型。

12
00:01:00,390 --> 00:01:08,040
因此，这里的每一行对应一个特定的评论，对于每个评论，

13
00:01:08,040 --> 00:01:15,480
大厅生成一个单词，如果该单词未出现在评论中，则得到0，然后得到1

14
00:01:15,810 --> 00:01:18,080
如果单词出现在评论中。

15
00:01:18,080 --> 00:01:23,610
基本上，这给了我们分类模型，因为现在我们将训练机器或任何模型

16
00:01:23,610 --> 00:01:30,420
它将试图理解单词与评论之间的相关性以及

17
00:01:30,420 --> 00:01:32,700
如果是负面评价，则结果为零。

18
00:01:32,700 --> 00:01:34,400
如果是正面评价，该怎么办。

19
00:01:34,620 --> 00:01:39,150
所以这是首先要了解的，现在要注意的第二件事是，我们确实

20
00:01:39,150 --> 00:01:44,400
这里有很多零，因为对于大多数评论，大多数单词不会出现在评论中

21
00:01:44,400 --> 00:01:45,740
有很多零。

22
00:01:45,840 --> 00:01:48,170
这就是所谓的稀疏性。

23
00:01:48,360 --> 00:01:55,860
我们有两种处理稀疏性的方法，它们试图通过过滤更多作品来减少稀疏性

24
00:01:55,860 --> 00:01:58,350
在此感谢这个Max features参数。

25
00:01:58,350 --> 00:02:04,200
因此，通过减少最常用词的数量，我们可以将其减少到1000个。

26
00:02:04,320 --> 00:02:10,560
减少稀疏性的第二种方法是通过降维，我们将部分讨论

27
00:02:10,560 --> 00:02:11,160
9。

28
00:02:11,250 --> 00:02:13,230
因此，我们稍后将学习这些技术。

29
00:02:13,410 --> 00:02:17,590
而现在，我们将要建立我们的传教榜样。

30
00:02:17,640 --> 00:02:20,960
因此，让我们关闭它并开始吧。

31
00:02:21,150 --> 00:02:27,870
所以现在这基本上是令人兴奋和舒适的一步，因为我们基本上完成了所有工作。

32
00:02:27,960 --> 00:02:28,730
为什么是这样。

33
00:02:28,740 --> 00:02:34,800
这是因为我们已经有了特征矩阵，这里是稀疏矩阵，

34
00:02:34,800 --> 00:02:36,000
可变向量。

35
00:02:36,000 --> 00:02:40,590
因此，现在我们只需要做的就是，您将看到我们如何非常有效地做到这一点。

36
00:02:40,590 --> 00:02:46,080
我们只需要对部分构建的机器学习分类模型进行一些复制粘贴即可

37
00:02:46,080 --> 00:02:46,580
三。

38
00:02:46,620 --> 00:02:49,130
所以现在就开始做吧，我们要非常高效地做它。

39
00:02:49,140 --> 00:02:53,350
您将看到我们已经准备好制作模型了。

40
00:02:53,450 --> 00:03:03,230
因此，我们将在这里进入文件浏览器，我们将回到第3部分的分类，因为那是

41
00:03:03,500 --> 00:03:05,510
我们所有的分类模型都是。

42
00:03:05,510 --> 00:03:06,410
这里有。

43
00:03:06,560 --> 00:03:08,870
这就是我们拥有明显侵略性的地方。

44
00:03:08,990 --> 00:03:16,070
然后作为V.M.  V.M.上校 海军基础决策树分类和随机森林分类

45
00:03:16,610 --> 00:03:21,650
因此选择哪一个对自然语言处理最好。

46
00:03:21,890 --> 00:03:23,950
好吧，我们在这里有两个选择。

47
00:03:24,020 --> 00:03:29,300
您知道，因为我们已经有了特征矩阵和因变量矢量，并且

48
00:03:29,300 --> 00:03:35,270
为我们的机器和分类模型做好了充分的准备我们可以做的就是对其中之一进行测试

49
00:03:35,270 --> 00:03:40,460
快速然后查看准确率假阳性假阴性的数量，并查看所有假阳性

50
00:03:40,460 --> 00:03:44,270
性能标准，以决定最佳模型。

51
00:03:44,600 --> 00:03:50,450
但是，如果我们不想做得好，我们必须知道，通常您会基于经验

52
00:03:50,450 --> 00:03:51,790
在实验中。

53
00:03:52,010 --> 00:03:59,270
好吧，现在用于自然语言处理的最常见凡人就是基础和决策树

54
00:03:59,270 --> 00:04:04,790
对我们来说，分类是我们用于自然语言处理的常用模型。

55
00:04:04,790 --> 00:04:07,070
这就是我们现在要使用的。

56
00:04:07,190 --> 00:04:10,630
并且本教程将尝试朴素的基础。

57
00:04:10,670 --> 00:04:13,010
所以，让我们做吧，让我们非常有效地做吧。

58
00:04:13,010 --> 00:04:15,590
您将看到它现在将变得如此简单。

59
00:04:15,830 --> 00:04:17,010
很好的基础。

60
00:04:17,150 --> 00:04:19,800
让我们双重承担这个责任。

61
00:04:19,840 --> 00:04:20,240
真好

62
00:04:20,240 --> 00:04:24,670
基于纯购买和归档，因为这是构建模型的地方。

63
00:04:24,680 --> 00:04:25,590
所以我们开始。

64
00:04:25,650 --> 00:04:26,530
现在偏离基地。

65
00:04:26,600 --> 00:04:27,150
很好

66
00:04:27,170 --> 00:04:33,090
现在，您将看到一切都变得如此简单，就像我刚刚告诉您的那样，一切都已经做好了准备。

67
00:04:33,140 --> 00:04:38,650
您知道我们称之为特征矩阵，即自变量x矩阵，然后是

68
00:04:38,650 --> 00:04:40,290
浸入变量向量y。

69
00:04:40,340 --> 00:04:41,800
因此，一切都很好。

70
00:04:41,800 --> 00:04:50,120
因此，基本上我们需要做的就是将其从此处移至此处，因为我们不需要

71
00:04:50,120 --> 00:04:55,150
之所以进行可视化，部分原因是您知道这仅在以下情况下才有两个独立变量

72
00:04:55,150 --> 00:04:59,390
能够显示预测区域和英国边界。

73
00:04:59,470 --> 00:05:03,110
因此，我们当然不能使用它，因为我们有1500个变量。

74
00:05:03,250 --> 00:05:08,710
如果您中有人找到一种以1500个维度表示图的方法，请告诉我。

75
00:05:08,830 --> 00:05:14,530
但是对于我们的问题，可视化部分将包括查看混淆矩阵以使

76
00:05:14,530 --> 00:05:18,220
查看正确预测和错误预测的数量。

77
00:05:18,220 --> 00:05:20,160
好的，所以基本上我们只知道这一点。

78
00:05:20,290 --> 00:05:22,540
因此，我们将复制它。

79
00:05:22,540 --> 00:05:28,810
让我们回到自然语言处理代码，然后将其粘贴到此处。

80
00:05:28,810 --> 00:05:29,420
好吧。

81
00:05:29,500 --> 00:05:36,540
现在，正如我告诉您的那样，我们无需更改任何内容，因为实际上我们拥有稀疏的特征矩阵

82
00:05:36,550 --> 00:05:38,640
那是我们的矩阵非常宝贵。

83
00:05:38,830 --> 00:05:43,420
在此处以及在此代码部分的此处称为X，以创建训练集。

84
00:05:43,480 --> 00:05:44,620
设置好了。

85
00:05:44,800 --> 00:05:50,590
然后我们有我们的因变量向量Y，在这里我们也叫y来创建训练集，

86
00:05:50,590 --> 00:05:51,360
测试集。

87
00:05:51,430 --> 00:05:52,280
一样。

88
00:05:52,340 --> 00:05:59,260
基本上，由于X和Y在某种程度上是唯一训练机器的输入，或者您知道

89
00:05:59,260 --> 00:06:02,110
我们可以将其转换为某种功能。

90
00:06:02,260 --> 00:06:06,650
如果执行此操作，则函数的唯一输入将是x和y。

91
00:06:06,700 --> 00:06:11,830
因此，这意味着我们拥有了所需的一切，我们可以选择所有这些来训练模型并

92
00:06:11,830 --> 00:06:17,650
在测试集上测试其预测能力，然后查看混淆度指标以查看

93
00:06:17,650 --> 00:06:20,110
正确的预测和错误数量。

94
00:06:20,110 --> 00:06:20,730
好吧。

95
00:06:20,980 --> 00:06:27,400
因此，基本上我们已经准备好了，但最好删除该部分，因为我们并不需要

96
00:06:27,400 --> 00:06:32,530
在这里应用杀戮功能，因为您知道我们主要是零，几个和两个

97
00:06:32,530 --> 00:06:35,790
或三分之二，因此在这里不需要恶性杀戮。

98
00:06:36,010 --> 00:06:41,820
第二件事是，您知道因为我们有1000个观测值，因此我们希望拥有最大的观测值

99
00:06:41,830 --> 00:06:45,560
观察以训练我们的机器或任何分类模型。

100
00:06:45,850 --> 00:06:51,910
现在我们有25％的人要去测试，这意味着我们将要训练750个观察值

101
00:06:51,940 --> 00:06:56,160
分类道德和250观察来测试它。

102
00:06:56,350 --> 00:07:01,690
因此，我们宁愿有更多的观测值来训练模型，而要进行测试的观测值却更少。

103
00:07:01,720 --> 00:07:08,860
因此，也许我们会提高20％到20％，以便明天有100项观测值要训练，

104
00:07:08,860 --> 00:07:11,050
200个观测值对其进行测试。

105
00:07:11,050 --> 00:07:12,340
所以现在我们很好。

106
00:07:12,460 --> 00:07:16,650
我们已准备好迈出制作整个模型的最终步骤。

107
00:07:16,660 --> 00:07:23,360
因此，如果您只是想找乐子，我们可以在此处还原内核来重置所有内容。

108
00:07:23,620 --> 00:07:24,550
是。

109
00:07:24,580 --> 00:07:25,240
好吧。

110
00:07:27,170 --> 00:07:28,970
因此，它正在连接一个新女孩。

111
00:07:28,970 --> 00:07:29,860
开始了。

112
00:07:30,140 --> 00:07:34,560
现在，我将选择所有内容。

113
00:07:34,640 --> 00:07:38,170
可以避免的好专家，准备我们将要看到的东西。

114
00:07:38,420 --> 00:07:44,720
并基本上向我们按命令或控制并执行整个代码。

115
00:07:44,970 --> 00:07:47,910
现在让我们看一下混淆矩阵。

116
00:07:48,020 --> 00:07:49,550
这是CME。

117
00:07:49,560 --> 00:07:50,630
这里是。

118
00:07:50,640 --> 00:07:56,370
因此，此计算机矩阵会收集所有评论的所有正确预测和错误预测

119
00:07:56,370 --> 00:07:59,510
在包含200条评论的测试集中。

120
00:07:59,550 --> 00:08:00,480
我们在这里看到什么。

121
00:08:00,480 --> 00:08:08,910
好吧，我们看到我们的机械模型做出了55条关于负面评论的正确预测91条正确的预测

122
00:08:09,060 --> 00:08:17,790
积极评论的数量超过42的积极评论的错误预测和12的错误预测

123
00:08:18,030 --> 00:08:19,510
负面评价。

124
00:08:19,530 --> 00:08:28,850
好的，这意味着200条评论中，我们的机械模型做出了55加91等于146个正确的预测

125
00:08:29,280 --> 00:08:32,680
42加12等于54。

126
00:08:32,820 --> 00:08:34,510
错误的预测。

127
00:08:34,530 --> 00:08:40,180
因此，由于我们只有800条评论来训练模型，所以实际上还不错。

128
00:08:40,200 --> 00:08:46,050
您知道，如果我们有100万条评论，我们得到的错误预测就会少得多，因为

129
00:08:46,070 --> 00:08:53,920
海军棒球会在清理后的文字评论与结果之间发现更多，更强的相关性。

130
00:08:54,100 --> 00:08:58,900
但是在这里，只要打开800条评论模型，这实际上还不错。

131
00:08:58,920 --> 00:09:02,340
让我们看看它的准确性。

132
00:09:02,340 --> 00:09:03,840
让我们在这里继续。

133
00:09:03,840 --> 00:09:04,080
好吧。

134
00:09:04,080 --> 00:09:12,330
因此，准确度是正确操作的总数，即55加上91，然后除以总数

135
00:09:12,330 --> 00:09:15,670
测试集中的预测数为200。

136
00:09:15,840 --> 00:09:20,550
因为我们有20％的大小，即一千二百。

137
00:09:20,550 --> 00:09:25,690
好吧，让我们选择它，然后按入控件并执行。

138
00:09:25,980 --> 00:09:29,690
这就是准确性，我们得到了73％。

139
00:09:29,700 --> 00:09:34,290
因此，对于这么少的评论来说，这实际上不是更好的选择。

140
00:09:34,290 --> 00:09:38,210
好的，这就是Python中自然语言处理的结束。

141
00:09:38,220 --> 00:09:43,740
祝贺您完成了所有这些工作，从而创建了向后模型训练和分类

142
00:09:43,740 --> 00:09:45,250
模型和此数据集。

143
00:09:45,480 --> 00:09:50,580
但这还不是自然语言处理之旅的终点，因为在这段视频之后，您将

144
00:09:50,580 --> 00:09:52,530
得到一点挑战。

145
00:09:52,620 --> 00:09:54,560
所以，我会让您了解一下。

146
00:09:54,570 --> 00:09:56,570
在那之前享受机器学习

