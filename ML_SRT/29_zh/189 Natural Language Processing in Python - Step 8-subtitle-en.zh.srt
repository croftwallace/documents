1
00:00:00,360 --> 00:00:02,670
您好，欢迎来到本Python教程。

2
00:00:02,730 --> 00:00:08,490
所以在目前的辛劳中，我们进行了自然语言处理的第一步，包括清洁

3
00:00:08,700 --> 00:00:10,500
我们正在处理的文本。

4
00:00:10,500 --> 00:00:16,980
现在，我们已经准备好进行自然语言处理的重要步骤，那就是创建这组单词

5
00:00:16,980 --> 00:00:17,940
模型。

6
00:00:17,940 --> 00:00:24,450
所以首先让我解释一下单词袋模型是什么，为什么我们需要创建它并通过理解

7
00:00:24,450 --> 00:00:29,280
为什么我们明天需要创建单词袋，您将更加理解为什么我们必须清洁

8
00:00:29,280 --> 00:00:30,690
之前的文字。

9
00:00:30,690 --> 00:00:33,880
这就是为什么我们必须清理所有1000条评论。

10
00:00:34,160 --> 00:00:34,470
好。

11
00:00:34,500 --> 00:00:38,770
所以首先让我解释一下单词袋模型。

12
00:00:38,790 --> 00:00:46,020
因此，基本上，自然语言处理的第一步不仅是我们清理了所有评论，而且

13
00:00:46,080 --> 00:00:49,930
我们还创建了一个语料库，我们的语料库就在这里。

14
00:00:50,070 --> 00:00:51,570
我们可以再看看。

15
00:00:51,720 --> 00:00:55,790
那是包含1000个Kaine评论的语料库。

16
00:00:55,860 --> 00:01:00,960
因此，正是从这里的语料库中，我们将创建单词袋模型。

17
00:01:01,200 --> 00:01:03,140
因此，袋字模型是什么。

18
00:01:03,360 --> 00:01:05,810
好吧，这基本上很简单。

19
00:01:05,860 --> 00:01:11,340
我们要做的是创建此向后模型，只是采用了所有不同的词

20
00:01:11,340 --> 00:01:12,830
1000条评论。

21
00:01:12,900 --> 00:01:14,290
因此，我们将花费一段时间。

22
00:01:14,290 --> 00:01:21,960
爱地壳好测试文本令人讨厌和所有其他单词直到1000年代审查不带

23
00:01:21,960 --> 00:01:28,800
重复或三倍的两倍或三倍，我们只是采用所有不同但独特的词

24
00:01:28,890 --> 00:01:36,240
在这1000条评论中，基本上，我们要做的是为每个单词创建一列。

25
00:01:36,450 --> 00:01:41,600
所以这里当然有很多不同的词，所以我们将有很多列，然后我们将

26
00:01:41,610 --> 00:01:47,600
将所有这些列放在表格中，表格中的行不过是1000条评论而已。

27
00:01:47,760 --> 00:01:55,230
因此，基本上，我们将获得一个包含1000行的表，其中这些行对应于评论

28
00:01:55,740 --> 00:02:00,870
还有很多列，这些列对应于我们可以在这里找到的每个不同的单词

29
00:02:00,930 --> 00:02:03,470
该语料库中的所有评论。

30
00:02:03,510 --> 00:02:10,060
因此，该表的每个单元格将对应一个特定的评论和该语料库的一个特定单词。

31
00:02:10,170 --> 00:02:16,680
在这个单元格中，我们将有一个数字，这个数字将是

32
00:02:16,680 --> 00:02:20,540
与该列对应的单词出现在评论中。

33
00:02:20,790 --> 00:02:26,080
举例来说，假设您知道第一列与该单词相对应。

34
00:02:26,130 --> 00:02:26,880
哇。

35
00:02:27,090 --> 00:02:32,340
好吧，对于这个特定的第一列以及与第一条评论相对应的第一行，

36
00:02:32,340 --> 00:02:37,270
Wow在第一次评论中出现过一次，这是一口井吗？

37
00:02:37,410 --> 00:02:43,110
好吧，我们将在这个特定的单元格中找到一个，因为该单元格属于对应的列

38
00:02:43,110 --> 00:02:46,350
在评论中出现的很好。

39
00:02:46,500 --> 00:02:48,240
因此，我们将为此单元格添加一个。

40
00:02:48,390 --> 00:02:52,670
但是，如果我们留在第一列中，然后移至第二行。

41
00:02:52,830 --> 00:02:59,670
好吧，因为虽然第二次评论中没有出现任何内容，但我们将为此特定用户得到零

42
00:02:59,670 --> 00:03:03,270
单元格属于第一列和第二行。

43
00:03:03,270 --> 00:03:10,250
正如您可以想象的那样，大多数单元将为零，因为我们可以简单地看到

44
00:03:10,340 --> 00:03:16,170
哇，这是专栏文章之一，我们可以看到该词出现在第一篇评论中。

45
00:03:16,260 --> 00:03:19,150
但是，当我们快速浏览一下这里的其他评论时。

46
00:03:19,350 --> 00:03:22,120
好吧，我们可以看到它似乎并不多。

47
00:03:22,120 --> 00:03:25,700
因此，对于第一列，我们将得到很多零。

48
00:03:25,980 --> 00:03:27,600
第二个单词也一样。

49
00:03:27,600 --> 00:03:32,570
好吧，爱情应该比哇更有意义，并且应该在评论中出现得更多。

50
00:03:32,580 --> 00:03:37,800
但是，当我们快速浏览其他评论时，我们仍然可以看到“爱”这个词没有出现

51
00:03:37,800 --> 00:03:38,640
许多。

52
00:03:38,650 --> 00:03:43,940
因此，基本上，即使我们为每个单词创建一列，也会发生什么。

53
00:03:44,040 --> 00:03:48,810
好吧，这个词不会出现在大多数评论中，因此我们会得到很多零，因为当

54
00:03:48,810 --> 00:03:53,400
该单词未出现在评论中好了，对应于您加薪的单元格和该单词

55
00:03:53,470 --> 00:03:54,240
零。

56
00:03:54,480 --> 00:03:58,040
因此，我们基本上将获得一个包含很多零的表。

57
00:03:58,110 --> 00:04:05,370
实际上，此表是一个矩阵，包含很多零的矩阵称为稀疏矩阵。

58
00:04:05,490 --> 00:04:11,610
我们有很多零的事实称为稀疏性，这是机器中非常普遍的概念

59
00:04:11,610 --> 00:04:12,390
学习。

60
00:04:12,390 --> 00:04:18,210
我们使用稀疏矩阵进行大量工作，并且在工作时尽量减少稀疏性

61
00:04:18,360 --> 00:04:19,830
与机器的早期型号。

62
00:04:19,860 --> 00:04:21,840
您会很快理解为什么。

63
00:04:22,080 --> 00:04:28,740
好吧，现在您了解了我们创建道德包之后的收获，因为创建

64
00:04:28,860 --> 00:04:32,010
稀疏矩阵实际上是其本身的词袋模型。

65
00:04:32,040 --> 00:04:38,280
好词袋模型基本上是简化所有评论清洁所有评论以简化

66
00:04:38,280 --> 00:04:41,610
单词，并尽量减少单词的数量。

67
00:04:41,790 --> 00:04:48,060
这也与通过标记化过程创建稀疏矩阵有关。

68
00:04:48,060 --> 00:04:53,760
您知道将所有不同的单词放在评论中，并为每个

69
00:04:53,760 --> 00:04:54,570
这些字。

70
00:04:54,780 --> 00:05:00,300
因此，这就是道德的背后，这就是我们完成这项创作后最终得到的

71
00:05:00,300 --> 00:05:06,840
我们的后备词模型，现在我们了解了为什么我们需要创建这样的模型

72
00:05:06,840 --> 00:05:08,130
表示。

73
00:05:08,430 --> 00:05:16,690
嗯，这是因为我们最终要做的只是预测评论是正面还是负面。

74
00:05:16,980 --> 00:05:23,850
为了使我们的机械模型能够很好地预测，需要对所有这些评论进行培训

75
00:05:23,850 --> 00:05:30,440
因为对于所有这些评论，我们都有真实的结果，对于每个评论我们都知道

76
00:05:30,800 --> 00:05:37,920
如果是正数或负数，那么我们将在所有这些评论中训练我们的算法，因为我们拥有

77
00:05:37,920 --> 00:05:43,500
机器学习模型的结果将了解如何在提示之间建立关联

78
00:05:43,800 --> 00:05:49,590
可以判断评论是正面还是负面，而正面或负面则是真实的结果。

79
00:05:49,590 --> 00:05:55,410
因此，您知道它将使那里的单词和评论与真实结果之间产生某种关联。

80
00:05:55,470 --> 00:05:56,450
太好了。

81
00:05:56,520 --> 00:06:02,040
但是为了使机器受到培训，以预测评论是肯定的还是负面的。

82
00:06:02,250 --> 00:06:09,090
和往常一样，它需要有一些自变量和一个因变量，因为

83
00:06:09,090 --> 00:06:16,140
我们在这里所做的是分类，因为结果因变量是分类的

84
00:06:16,140 --> 00:06:22,060
变量二元结果是评论为肯定，如果评论为否，则为零。

85
00:06:22,230 --> 00:06:25,590
因此，我们除了分类之外什么也不做。

86
00:06:25,710 --> 00:06:28,470
正如您在第三部分中看到的那样。

87
00:06:28,680 --> 00:06:36,030
好吧，我们有一些独立的核糖，我们在模型上训练我们的任务以预测依赖

88
00:06:36,210 --> 00:06:38,650
变量，它是分类变量。

89
00:06:38,730 --> 00:06:40,990
大多数时候都是二进制结果。

90
00:06:41,190 --> 00:06:43,160
这正是我们在这里拥有的。

91
00:06:43,200 --> 00:06:49,410
当我们创建后向词模型时，因为对应于一个特定单词的每个冒号都是一个

92
00:06:49,500 --> 00:06:55,950
自变量本身，因为这些单词中的每个列都以某种方式连接

93
00:06:55,950 --> 00:07:01,590
进入评论，因为对于每个评论，我们都可以说单词是否出现。

94
00:07:01,600 --> 00:07:02,440
在审查中。

95
00:07:02,550 --> 00:07:07,350
因此，当它出现在评论中时，该列将变为1，而当它未出现在评论中时，

96
00:07:07,350 --> 00:07:08,830
列为零。

97
00:07:09,030 --> 00:07:14,220
因此，文字列与道路或其他评论之间的这种联系使我们能够创建

98
00:07:14,310 --> 00:07:20,250
一个分类模型，与我们一直在使用的分类模型完全相同

99
00:07:20,250 --> 00:07:21,190
3。

100
00:07:21,210 --> 00:07:26,490
所以这意味着一旦我们成功创建了这袋文字，

101
00:07:26,490 --> 00:07:29,420
列中的单词以及所有评论和行中的单词。

102
00:07:29,700 --> 00:07:35,780
好吧，我们赢了，因为我们只需要在某个地方复制粘贴或分类，这就是我们要做的

103
00:07:35,790 --> 00:07:41,910
在本节的最后，我们将获得特征矩阵或自变量矩阵

104
00:07:42,180 --> 00:07:47,780
这将是在所有评论中出现的不同单词，这将是矩阵的各列

105
00:07:47,940 --> 00:07:53,700
我们将得到我们的因变量向量，这将是无论评论是否为正的结果

106
00:07:54,000 --> 00:07:56,600
一或负零。

107
00:07:56,610 --> 00:08:01,680
所以这就是为什么我们只需要参加标准分类就需要创建这个道德词袋

108
00:08:01,680 --> 00:08:02,320
模型。

109
00:08:02,460 --> 00:08:06,630
这就是为什么我们在某种程度上没有在机器学习方面做任何新的事情。

110
00:08:06,690 --> 00:08:11,290
我们只是回到分类部分，但我们正在处理文本。

111
00:08:11,580 --> 00:08:16,710
因此，现在我们非常了解为什么我们必须清除所有文本的所有评论，因为

112
00:08:16,710 --> 00:08:21,160
我们为每个单词创建了一个列，该列是每个单词的一个自变量。

113
00:08:21,390 --> 00:08:26,670
好吧，我们希望清理评论并尽可能简化评论，以减少评论的总数

114
00:08:26,670 --> 00:08:32,250
语料中的单词，因此自变量的总数。

115
00:08:32,290 --> 00:08:34,350
好的，我希望这很清楚。

116
00:08:34,500 --> 00:08:35,220
不要犹豫。

117
00:08:35,220 --> 00:08:37,290
随时在问答中提问。

118
00:08:37,320 --> 00:08:43,990
如果仍然不是很清楚，现在我们将继续进行“道德”一词的创建。

119
00:08:44,040 --> 00:08:51,800
通过创建这个巨大的稀疏矩阵，将这1000条评论的每个词放在自己的列中

120
00:08:52,140 --> 00:08:54,890
我们将通过标记化过程来做到这一点。

121
00:08:55,020 --> 00:09:01,620
最后，一旦我们创建了单词袋模型，我们就可以应用我们的分类模型

122
00:09:01,800 --> 00:09:03,780
预测每个新评论。

123
00:09:03,780 --> 00:09:06,070
是正数还是负数。

124
00:09:06,090 --> 00:09:06,470
好吧。

125
00:09:06,480 --> 00:09:07,510
因此，让我们开始吧。

126
00:09:07,590 --> 00:09:12,820
让我们关闭它，现在让我们创建单词Mario。

127
00:09:13,110 --> 00:09:18,190
因此，正如我刚才所说，我们将通过标记化过程来创建此向后模型。

128
00:09:18,300 --> 00:09:23,010
当然，您可以想象，如果我们必须手工做得很好，那将是相当复杂的。

129
00:09:23,010 --> 00:09:25,950
因此，我们当然会使用可以两行执行此操作的工具。

130
00:09:25,950 --> 00:09:31,840
这个工具将成为计数向量化或分类，因此我们现在很重要。

131
00:09:32,040 --> 00:09:39,140
我们将从Skillern思想特征提取中导入此类。

132
00:09:39,140 --> 00:09:42,990
这是输入d'arte文本。

133
00:09:42,990 --> 00:09:43,910
又来了。

134
00:09:44,100 --> 00:09:54,130
输入现在，我们导入需要的类，即计数向量化或计数向量化。

135
00:09:54,240 --> 00:09:54,820
好。

136
00:09:55,060 --> 00:10:01,820
现在像往常一样，因为它是一个类，所以我们将创建该类的对象，并且这是

137
00:10:01,820 --> 00:10:07,430
将继续进行标记化，即创建我们的单词袋。

138
00:10:07,480 --> 00:10:14,510
因此，您将看到，从现在开始，我们仅需要两行即可创建对象，而另一行

139
00:10:14,510 --> 00:10:20,810
进行这种转换，即创建这个巨大的稀疏矩阵。

140
00:10:21,610 --> 00:10:25,050
好的，让我们开始创建对象。

141
00:10:25,150 --> 00:10:30,430
让我们将其称为C-v，以实现Count向量化和等于。

142
00:10:30,500 --> 00:10:37,600
因此，像往常一样，我们需要再次调用该类，复制粘贴并添加一些括号。

143
00:10:37,600 --> 00:10:44,620
好的，所以在这个阶段我们不需要在向量化中放入任何东西，但是我们应该

144
00:10:44,620 --> 00:10:49,150
查看所有参数，因为您会看到一些非常有趣的东西。

145
00:10:49,270 --> 00:10:56,160
因此，我按了它们，然后在这里检查计数向量化她，让我们看一下参数。

146
00:10:56,170 --> 00:10:56,520
好吧。

147
00:10:56,520 --> 00:11:05,280
因此，如您所见，我们有几个参数，但我想在这里重点介绍的是这一个Stoppard

148
00:11:06,270 --> 00:11:13,050
如您所见，我们有这个ups参数，您可能已经猜到了，删除了所有

149
00:11:13,050 --> 00:11:20,910
它会在语料库中找到，因为然后我们将通过转移方法将对象应用于语料库

150
00:11:20,910 --> 00:11:22,180
我们在这里创建的

151
00:11:22,350 --> 00:11:27,270
因此，如果我们在此计数矢量化中添加此向上参数，或在此处作为输入。

152
00:11:27,510 --> 00:11:33,270
好的，当我们将该对象应用于时，它将自动删除该语料库中的所有停用词。

153
00:11:33,270 --> 00:11:34,170
语料库。

154
00:11:34,200 --> 00:11:41,010
因此，这还不错，很明显，我们甚至可以查看其他参数，例如小写字母。

155
00:11:41,160 --> 00:11:46,710
嗯，如果添加一个小写字母参数，可以在此处进行矢量化处理。

156
00:11:46,860 --> 00:11:52,860
好吧，发生的是，当您将对象以转移方法应用于Corvis时，它将放置

157
00:11:52,950 --> 00:11:59,860
创建庞大的稀疏矩阵之前，语料库中的所有评论均以小写形式表示。

158
00:12:00,480 --> 00:12:03,830
因此，无论接下来是否有其他有趣的事情。

159
00:12:04,170 --> 00:12:08,800
好吧，如果我们再次上升，我也会看到一个有趣的令牌模式。

160
00:12:08,970 --> 00:12:16,410
基本上，令牌模式就是我们在这里所做的，您要取所有带有字母的单词

161
00:12:16,410 --> 00:12:22,410
从Ada Zed中删除其他字符，这是因为这是参数标记模式和

162
00:12:22,410 --> 00:12:28,530
模式实际上就是您想要保留在评论中的内容，就像Ada所说的那样。

163
00:12:28,540 --> 00:12:34,620
因此，基本上我在这里向您展示的是我们在此处手动执行的操作。

164
00:12:34,770 --> 00:12:39,120
好吧，我们可以在此方面直接做一些选择。

165
00:12:39,150 --> 00:12:43,120
通过使用这些不同的参数进行矢量化。

166
00:12:43,470 --> 00:12:46,140
但这不是最好的方法。

167
00:12:46,320 --> 00:12:50,310
我之所以喜欢这种方式，有两个原因。

168
00:12:50,310 --> 00:12:54,960
第一个原因是我们逐步了解如何清理文本。

169
00:12:54,990 --> 00:12:58,370
因此，这只是出于教学目的，但这还不是全部。

170
00:12:58,410 --> 00:13:02,540
通过手动清理评论，可为您提供更多选择。

171
00:13:02,580 --> 00:13:07,590
您知道这可以矢量化或具有多个参数，可让您自动清除评论

172
00:13:07,590 --> 00:13:13,350
通过使用参数，但有时您会需要的更多，而有时

173
00:13:13,470 --> 00:13:17,850
做进一步的清洁工作以清洁正在处理的任何文本。

174
00:13:17,970 --> 00:13:24,210
我可以举一个例子，说您正在进行自然语言处理以进行网页抓取。

175
00:13:24,360 --> 00:13:31,490
那么在这种情况下，测试将是一些HMO页面，在这些HMO页面中，您将获得一些HVM

176
00:13:31,550 --> 00:13:34,490
l例如B R的代码。

177
00:13:34,500 --> 00:13:38,970
因此，即使您在此处使用此代码也可以保留所有字母。

178
00:13:39,090 --> 00:13:42,470
好吧，不管怎样，我们还是要删除它。

179
00:13:42,540 --> 00:13:46,990
因此，您需要添加另一个选项来清除这些HMO测试。

180
00:13:47,110 --> 00:13:51,090
顺便说一句，如果你想这样做，你可以用肉眼问我一些问题，我会告诉你

181
00:13:51,210 --> 00:13:53,200
删除此税的代码。

182
00:13:53,340 --> 00:14:00,030
但基本上，在此处手动执行此操作可以使我们拥有更多的控制权和更多的选择权，

183
00:14:00,030 --> 00:14:03,930
我们可以根据需要彻底清理评论。

184
00:14:03,930 --> 00:14:05,120
因此，您可以选择。

185
00:14:05,220 --> 00:14:10,860
现在您知道通过使用Count参数在这里也有这种快捷方式

186
00:14:10,860 --> 00:14:11,860
向量化。

187
00:14:12,270 --> 00:14:12,820
好吧。

188
00:14:12,900 --> 00:14:19,580
现在，我们将不放置任何参数，我们将继续进行创建后退单词的下一步

189
00:14:19,590 --> 00:14:22,640
模型，因为现在我们刚刚创建了对象。

190
00:14:22,650 --> 00:14:31,440
所以现在我们要做的是将适合的转移方法从这个愚蠢的对象应用于我们的评论语料库

191
00:14:31,890 --> 00:14:34,550
以及这种适合的转移方法将做什么。

192
00:14:34,740 --> 00:14:38,880
好吧，除了创建这个庞大的稀疏矩阵之外，它什么也做不了。

193
00:14:38,880 --> 00:14:41,730
我一直在和你谈论，所以就做吧。

194
00:14:41,730 --> 00:14:43,770
让我们创建这个稀疏矩阵。

195
00:14:43,800 --> 00:14:50,700
所以我们要做的是，因为您了解到，这个稀疏矩阵将仅此而已

196
00:14:50,760 --> 00:14:56,850
包含不同自变量的特征矩阵，我们将使用它们来训练机器

197
00:14:56,850 --> 00:15:01,450
学习算法来预测评论的结果是正面还是负面。

198
00:15:01,680 --> 00:15:07,950
好吧，我们将这个稀疏矩阵称为X，因为当您记住三个分类时，

199
00:15:07,950 --> 00:15:10,680
用来称呼我们的特征矩阵X ..

200
00:15:10,710 --> 00:15:14,720
因此，这里的X等于，现在就创建了。

201
00:15:14,730 --> 00:15:23,250
如您所知，我们首先获取对象C-v，然后进行点运算，然后使用拟合传递

202
00:15:23,250 --> 00:15:30,500
方法，因为您知道记住我们的对象需要首先适合主体才能对其进行分析。

203
00:15:30,570 --> 00:15:31,650
看所有的话。

204
00:15:31,710 --> 00:15:37,440
看一下如何应用我们在矢量化过程中输入的不同参数。

205
00:15:37,590 --> 00:15:43,370
然后，当然，在安装好之后，我们将使用此变换部分进行变换

206
00:15:43,390 --> 00:15:45,180
这里的转移方法。

207
00:15:45,300 --> 00:15:51,540
当然，这包括创建一个庞大的稀疏矩阵，将所有不同的词放入其

208
00:15:51,600 --> 00:15:52,910
自己的专栏。

209
00:15:52,920 --> 00:15:56,620
好的，现在让我们添加一些括号。

210
00:15:56,700 --> 00:15:57,450
开始了。

211
00:15:57,570 --> 00:16:02,940
当然，在这种转移方法中，我们只需要输入需要转换的内容，即

212
00:16:03,240 --> 00:16:06,370
这就是我们需要输入的所有内容。

213
00:16:06,390 --> 00:16:11,610
我们只需要指定需要转换的内容，此方法将为我们完成全部工作。

214
00:16:11,700 --> 00:16:12,730
太好了。

215
00:16:12,960 --> 00:16:15,570
但是现在我们只需要添加一些内容。

216
00:16:15,570 --> 00:16:22,020
您知道记得在第三部分分类中，我们在这里有一个X矩阵，因为这是我们的矩阵

217
00:16:22,020 --> 00:16:22,900
功能。

218
00:16:22,920 --> 00:16:31,500
因此，为了拥有一个矩阵，我们只需要听一听然后排列即可，并添加一些括号。

219
00:16:31,530 --> 00:16:32,290
开始了。

220
00:16:32,490 --> 00:16:38,520
这就是我们仅需三行代码即可创建我们的单词袋模型，即

221
00:16:38,520 --> 00:16:41,360
创建这个稀疏矩阵。

222
00:16:41,400 --> 00:16:47,130
所以现在我只需要在这里选择这三行，然后按Command Control按Enter

223
00:16:47,130 --> 00:16:48,420
执行。

224
00:16:48,420 --> 00:16:49,160
开始了。

225
00:16:49,200 --> 00:16:51,090
创建了稀疏矩阵。

226
00:16:51,360 --> 00:16:55,890
因此，我们将对其进行分析，并在接下来的两个Tauriel中进行改进。

227
00:16:55,890 --> 00:16:57,540
在那之前享受学习

