1
00:00:00,390 --> 00:00:05,790
您好，欢迎使用本Python教程，并且我们已准备了上一篇教程，向后消除

2
00:00:05,850 --> 00:00:12,000
通过将此处的这一列添加到特征X矩阵中来实现该算法，因为这仅仅是必需的

3
00:00:12,210 --> 00:00:17,540
统计模型的库，我们稍后将使用它来获取基础的统计信息

4
00:00:17,560 --> 00:00:18,340
Voyles。

5
00:00:18,420 --> 00:00:24,330
现在期待着我们将开始向后淘汰，因此让我们开始吧。

6
00:00:24,330 --> 00:00:29,400
您说它将变得非常简单和有趣，我们将获得一张很棒的桌子，它将告诉我们所有内容

7
00:00:29,400 --> 00:00:33,390
我们需要非常轻松地完成消除木料的工作。

8
00:00:33,390 --> 00:00:39,900
所以首先让我们执行这一行，因为我们没有这样做，所以我们只是放弃了这一列

9
00:00:39,900 --> 00:00:44,490
如果只是X，就在这里到我们的矩阵，让我们也执行这一行。

10
00:00:44,520 --> 00:00:45,420
开始了。

11
00:00:45,480 --> 00:00:48,150
现在，统计数据Morell的库已导入。

12
00:00:48,330 --> 00:00:51,750
现在我们实际上正在开始向后淘汰。

13
00:00:51,750 --> 00:00:59,280
所以首先要做的是创建一个新的特征矩阵，这将是我们的最佳特征矩阵

14
00:00:59,520 --> 00:01:03,900
因此，我们将其称为X下划线表示最佳选择。

15
00:01:03,900 --> 00:01:10,320
因此，最终将是特征的最优矩阵，即包含最优矩阵的矩阵。

16
00:01:10,320 --> 00:01:16,140
仅对因变量利润具有统计意义的自变量团队

17
00:01:16,150 --> 00:01:16,350
。

18
00:01:16,530 --> 00:01:22,860
这里有一个最优矩阵X ARBT只会包含影响力很大的自变量

19
00:01:23,040 --> 00:01:24,000
在利润上。

20
00:01:24,090 --> 00:01:28,860
因此，我们将看到此矩阵的末尾位置，对于我们来说，我们现在要做的是初始化

21
00:01:28,860 --> 00:01:29,210
它。

22
00:01:29,370 --> 00:01:34,830
因此，如您所知，消除落后木材包括在以下位置包括所有自变量：

23
00:01:34,830 --> 00:01:37,310
首先，然后我们将一一删除。

24
00:01:37,470 --> 00:01:40,990
没有统计学意义的自变量。

25
00:01:41,220 --> 00:01:47,910
因此，我们实际上需要在矩阵X中使用所有独立变量

26
00:01:47,920 --> 00:01:48,190
。

27
00:01:48,450 --> 00:01:53,880
因此，现在我们采用特征X的矩阵。很好，但是我们要做的是

28
00:01:53,880 --> 00:01:58,140
专门编写列和x的所有索引。

29
00:01:58,320 --> 00:01:59,430
为什么是这样。

30
00:01:59,430 --> 00:02:03,840
这是因为我们随后将在每个步骤中删除索引。

31
00:02:04,020 --> 00:02:04,820
因此，让我们这样做。

32
00:02:04,830 --> 00:02:11,520
他将在此处使用此专栏的所有内容，然后使用逗号，然后在此处指定

33
00:02:11,580 --> 00:02:13,300
列的所有索引。

34
00:02:13,500 --> 00:02:19,380
因此，我们需要以方括号将其指定，然后只需要编写索引，并用

35
00:02:19,410 --> 00:02:20,130
逗号。

36
00:02:20,130 --> 00:02:21,460
因此，让我们看一下索引。

37
00:02:21,480 --> 00:02:28,120
如果在这里到达X，我们可以看到我们有0 1 2 3 4和5个索引。

38
00:02:28,170 --> 00:02:30,330
因此，让我们全部编写。

39
00:02:30,480 --> 00:02:36,690
0 1 2 3 4和5

40
00:02:36,690 --> 00:02:37,230
好吧。

41
00:02:37,320 --> 00:02:41,660
因此，摘录被初始化为要素的原始矩阵。

42
00:02:41,670 --> 00:02:46,310
那是一个包含我们数据集中所有自变量的矩阵。

43
00:02:46,320 --> 00:02:48,240
现在，让我们看看我们该怎么做。

44
00:02:48,240 --> 00:02:54,370
让我们跳到您在直觉教程中与Carol一起看到的向后滑动幻灯片。

45
00:02:54,720 --> 00:02:58,290
因此，第一步是选择显着性水平以保留在模型中。

46
00:02:58,290 --> 00:03:03,990
这意味着我们需要选择显着性水平，以便如果我们的自变量的p值

47
00:03:04,170 --> 00:03:09,480
低于显着性水平，则自变量将保留在模型中。

48
00:03:09,480 --> 00:03:15,330
但是，如果自变量的值是条形显着性水平，则它将不会保持不变

49
00:03:15,330 --> 00:03:17,100
在模型中，我们将其删除。

50
00:03:17,310 --> 00:03:21,030
好的，那么第一步只是选择一个重要的水平。

51
00:03:21,120 --> 00:03:22,170
我们会选择。

52
00:03:22,170 --> 00:03:25,590
SL等于0.05，即5％。

53
00:03:25,910 --> 00:03:26,420
好的。

54
00:03:26,520 --> 00:03:28,030
步骤1完成。

55
00:03:28,030 --> 00:03:30,220
现在，我们继续执行步骤2。

56
00:03:30,390 --> 00:03:35,340
因此，第二步实际上是使所有可能的预测变量都适合整个模型。

57
00:03:35,340 --> 00:03:41,390
因此，我们现在已经完成的工作是通过初始化来创建特征的光学矩阵

58
00:03:41,390 --> 00:03:47,760
它与原始指标的关系已经存在于变量X中，因此它包含所有可能的预测变量，但我们

59
00:03:47,760 --> 00:03:53,070
尚不适合我们的模型，因为记住，我们没有采用我们构建的回归器

60
00:03:53,370 --> 00:03:55,710
在以前的教程中的先前步骤中。

61
00:03:55,720 --> 00:04:01,470
我们正在使用一个新的库，因此我们需要对所有可能的独立模型进行完全拟合

62
00:04:01,470 --> 00:04:02,230
变量。

63
00:04:02,250 --> 00:04:08,100
所以现在就开始做，让我们回到Python，并在回归模型中修复倍数

64
00:04:08,130 --> 00:04:15,160
到我们未来的特征X和Y的最优矩阵，因为我们实际上需要y来拟合。

65
00:04:15,180 --> 00:04:19,950
所以让我们这样做，因为我们将使用一个新的

66
00:04:19,950 --> 00:04:25,010
库是与线性模型库相对的小型库。

67
00:04:25,050 --> 00:04:30,000
所以我们要创建一个新的回归是因为我们在这里创建的回归是回归

68
00:04:30,000 --> 00:04:31,750
来自线性模型库。

69
00:04:31,800 --> 00:04:35,620
实际上是来自普通库的线性回归类。

70
00:04:35,700 --> 00:04:41,650
现在我们要创建一个新的回归，或者它实际上是这个新类的对象

71
00:04:41,680 --> 00:04:43,090
那是金属的图书馆。

72
00:04:43,260 --> 00:04:49,150
对于普通的最小二乘，这个新类实际上称为O L S。

73
00:04:49,200 --> 00:04:56,150
让我们从此类创建一个新对象，然后将其命名为regress或下划线o

74
00:04:56,180 --> 00:05:01,060
据您所知，请指定它是上一类的回归或对象。

75
00:05:01,250 --> 00:05:06,890
因此，让我们写等于然后再调用类，以便调用我们实际需要调用的类

76
00:05:06,890 --> 00:05:12,410
库首先是S M，然后是点，然后是最后一个类。

77
00:05:12,410 --> 00:05:17,870
所以所有这些以及现在的括号是因为像往常一样，我们需要指定一些参数。

78
00:05:18,110 --> 00:05:20,600
因此，让我们看一下这些参数。

79
00:05:20,630 --> 00:05:26,420
因此，我将在此处单击评论，让我们看一下两个参数OK我们需要的两个参数

80
00:05:26,420 --> 00:05:29,380
把我们的狗放逐

81
00:05:29,690 --> 00:05:31,520
因此，实际上非常清楚。

82
00:05:31,520 --> 00:05:34,460
它指定任何狗都是因变量。

83
00:05:34,520 --> 00:05:41,120
因此，这就是为什么流亡者是这个数组，其中包含观察次数和侵略者数目

84
00:05:41,120 --> 00:05:41,240
。

85
00:05:41,240 --> 00:05:48,020
因此通常是x ARBT或矩阵特征，但随后它还指定截距不是

86
00:05:48,020 --> 00:05:49,180
默认情况下包括在内。

87
00:05:49,220 --> 00:05:55,580
这就是为什么我们需要在此处制作所有这些行，以将此处的截距附加到要素矩阵中

88
00:05:55,590 --> 00:05:55,780
。

89
00:05:55,970 --> 00:06:02,330
因此，您知道幸运的是，它提醒我们在以下情况下需要将此截距添加到特征矩阵中：

90
00:06:02,330 --> 00:06:05,180
我们将普通最小二乘模型输入该矩阵。

91
00:06:05,180 --> 00:06:09,390
您知道在此指定了截距应该由用户添加。

92
00:06:09,440 --> 00:06:11,150
用户实际上是您和我。

93
00:06:11,180 --> 00:06:12,880
因此，这在这里是一种缓解。

94
00:06:12,920 --> 00:06:17,700
您知道，这就是为什么有时查看对象检查器非常重要的原因。

95
00:06:18,110 --> 00:06:18,410
好。

96
00:06:18,410 --> 00:06:29,210
因此，现在让我们以我们理解的方式来论证，这就是为什么放逐是摘录的原因。

97
00:06:29,240 --> 00:06:29,970
好吧。

98
00:06:30,370 --> 00:06:35,790
现在我们需要做的就是拟合普通的平方算法，即您知道的倍数

99
00:06:35,790 --> 00:06:38,940
自身回归到X和Y。

100
00:06:39,170 --> 00:06:46,550
并非常简单地执行此操作，我们在此处添加fit方法，即fit方法，然后加上括号，然后

101
00:06:46,550 --> 00:06:50,710
将完美地完成通常适合摘录的工作，以及为什么。

102
00:06:50,900 --> 00:06:54,040
好吧，现在我们选择它并执行。

103
00:06:54,110 --> 00:07:02,120
因此，我们只需要选择这两行，然后按Command和Control，再按Enter即可执行和Perfect

104
00:07:02,120 --> 00:07:02,260
。

105
00:07:02,270 --> 00:07:09,410
现在，接受者已经创建好了，初始化它，然后我们将对其进行更新，直到它成为真正的最优值为止。

106
00:07:09,470 --> 00:07:10,580
功能矩阵。

107
00:07:10,730 --> 00:07:12,950
现在，我想我们已经完成了步骤2。

108
00:07:13,040 --> 00:07:16,420
让我们回到我们的向后淘汰幻灯片。

109
00:07:16,430 --> 00:07:19,980
第二步是使所有可能的预测变量都适合整个模型。

110
00:07:20,000 --> 00:07:22,320
好吧，这正是我们刚刚所做的。

111
00:07:22,430 --> 00:07:24,890
如此完美地完成了步骤2。

112
00:07:24,890 --> 00:07:26,430
现在，我们继续进行第三步。

113
00:07:26,600 --> 00:07:28,850
因此，第三步是寻找预测变量。

114
00:07:28,860 --> 00:07:31,820
这是具有最高P值的自变量。

115
00:07:31,910 --> 00:07:37,240
如果此p值高于5％的显着性水平，那么我们将转到步骤4。

116
00:07:37,370 --> 00:07:41,860
如果低于显着性水平5％，我们的模型实际上已经准备就绪。

117
00:07:41,870 --> 00:07:46,480
因此，让我们执行以下操作，以寻找具有最高P值的预测变量。

118
00:07:46,550 --> 00:07:53,240
因此，要查找此预测变量，我们将需要使用此统计信息的模型库的nostrum函数，该函数

119
00:07:53,240 --> 00:07:59,350
是摘要函数，它将返回一个包含所有统计指标的表格

120
00:07:59,360 --> 00:08:01,020
建立模型时很有趣。

121
00:08:01,100 --> 00:08:07,160
您知道这些可以帮助使道德变得更健全的指标，例如R平方，它们只是增加了R平方

122
00:08:07,160 --> 00:08:11,620
这将在评估磨床性能的部分的最后部分看到。

123
00:08:11,870 --> 00:08:14,230
当然，我们将获得所有的p值。

124
00:08:14,360 --> 00:08:17,080
因此我们将获得每个自变量。

125
00:08:17,100 --> 00:08:23,300
它是P值，因此我们将其与显着性水平进行比较，以确定是否需要将其从

126
00:08:23,360 --> 00:08:24,280
我们的模型。

127
00:08:24,320 --> 00:08:25,220
因此，让我们这样做。

128
00:08:25,220 --> 00:08:35,360
因此，像往常一样，我们需要进行或回归或对象回归或其他所有事情，现在我们需要调用

129
00:08:35,480 --> 00:08:43,010
此函数摘要，我们在此处添加一些括号，因为这是我们要选择的函数

130
00:08:43,010 --> 00:08:46,800
此行并执行抑制命令控制，然后按Enter。

131
00:08:47,120 --> 00:08:50,730
这是我刚刚跟您谈的桌子。

132
00:08:50,750 --> 00:08:57,710
因此，正如您所看到的，在我们的回归模型中包含有关您的倍数的一些非常有用的信息

133
00:08:57,710 --> 00:08:57,830
。

134
00:08:57,830 --> 00:09:02,270
因此，您可以看到我们在这里有了R平方，这是我们稍后将讨论的调整后的R平方

135
00:09:02,450 --> 00:09:06,790
您将看到我们将使用这两个指标来使我们的模型更加稳健。

136
00:09:06,980 --> 00:09:13,010
但是现在我们感兴趣的是p值，而p值实际上在此列中，因为

137
00:09:13,010 --> 00:09:15,290
你知道P值是一个概率。

138
00:09:15,590 --> 00:09:22,790
重要的是要了解，P值越低，您的独立性就越重要

139
00:09:22,790 --> 00:09:26,450
变量将与您的因变量有关。

140
00:09:26,450 --> 00:09:32,660
所以这些实际上是我们所有的自变量，这里的常数x 0等于1

141
00:09:32,870 --> 00:09:36,030
我们在此处添加内容的变量。

142
00:09:36,380 --> 00:09:43,500
然后有x y，其次是状态的两个虚拟变量，然后是x 3，这是我们的支出不足

143
00:09:43,950 --> 00:09:48,040
x4是管理费用，X5是营销支出。

144
00:09:48,380 --> 00:09:48,670
好。

145
00:09:48,680 --> 00:09:50,050
所以现在我们该怎么办。

146
00:09:50,090 --> 00:09:53,970
我们必须寻找可以很容易看到的最高P值。

147
00:09:53,990 --> 00:09:56,810
只有百分之九十九。

148
00:09:56,810 --> 00:10:00,620
因此，我们远远超出了5％的显着性水平。

149
00:10:00,620 --> 00:10:05,330
因此，我们当然需要将其删除，因为它太有价值了，因为您可以看到我们是否回到了我们的

150
00:10:05,330 --> 00:10:10,640
向后消除滑动如果P值高于5％的显着性水平，则我们需要

151
00:10:10,640 --> 00:10:14,910
转到第4步，而第4步实际上是删除此预测变量。

152
00:10:14,930 --> 00:10:17,140
因此，现在让我们实际执行此操作。

153
00:10:17,180 --> 00:10:21,870
让我们删除预测变量x 2，它是状态的第二个虚拟变量。

154
00:10:22,310 --> 00:10:26,800
因此，让我们看一下矩阵X，因为我们只需要跟踪索引即可。

155
00:10:26,840 --> 00:10:33,650
因此，让我们看看常量的索引为0 X1的索引为1，x 2的索引为2。

156
00:10:33,650 --> 00:10:35,900
这是我们要删除的自变量。

157
00:10:35,900 --> 00:10:39,680
这是具有最高P值的自变量。

158
00:10:39,800 --> 00:10:41,220
所以K指数2。

159
00:10:41,420 --> 00:10:47,450
所以现在我们要做的是复制下面粘贴的内容，现在我们要

160
00:10:47,450 --> 00:10:49,340
删除我们的索引。

161
00:10:49,430 --> 00:10:52,250
因此，我们接下来将其删除。

162
00:10:52,250 --> 00:10:57,610
因此，通过执行此操作，我们实际上删除了这个独立变量，它是哑变量

163
00:10:57,630 --> 00:11:00,640
从X开始具有最高的P值。

164
00:11:00,770 --> 00:11:05,750
实际上，如果现在返回幻灯片，我们可以看到下一步，因为实际上我们已经完成了

165
00:11:05,750 --> 00:11:09,350
下一步是拟合模型。

166
00:11:09,410 --> 00:11:10,630
没有这个变量。

167
00:11:10,790 --> 00:11:12,680
好吧，实际上它已经在这里完成了。

168
00:11:12,680 --> 00:11:18,860
这就是为什么我为您准备了一个脚本的原因，因为我们只需要复制粘贴就可以使步骤3步骤4和

169
00:11:18,860 --> 00:11:21,040
一次完成第五步。

170
00:11:21,050 --> 00:11:26,570
说到这里，我将让您独自一人做作业，但不要担心您

171
00:11:26,570 --> 00:11:28,660
在下一个教程中将有解决方案。

172
00:11:28,730 --> 00:11:34,940
但是真正尝试自己实施此完全向后消除直到最后，这将是

173
00:11:34,940 --> 00:11:36,720
看看我们是否得到相同的结果很有趣。

174
00:11:36,890 --> 00:11:40,670
实际上，在消除积压词的末尾实际上要做出一个决定。

175
00:11:40,730 --> 00:11:44,270
因此，我很想知道您是如何做出决定的。

176
00:11:44,360 --> 00:11:50,640
因为这两种解决方案实际上都很棒，所以我们将在解决方案中进行讨论。

177
00:11:50,900 --> 00:11:53,130
祝你功课顺利。

178
00:11:53,390 --> 00:11:54,470
您将看到它会很好。

179
00:11:54,470 --> 00:12:00,460
所以基本上您只需要遵循此向后消除幻灯片，然后一起

180
00:12:00,460 --> 00:12:06,590
这个故事玩具到了这里的第五步，现在您可以看到必须回到第三步

181
00:12:06,710 --> 00:12:09,000
并重做步骤三四五。

182
00:12:09,110 --> 00:12:15,800
就像我们刚才所做的一样，直到您发现最高的p值不高于有效值

183
00:12:15,800 --> 00:12:19,990
级别以及何时可以建立模型。

184
00:12:20,420 --> 00:12:20,710
好。

185
00:12:20,720 --> 00:12:23,140
因此，我期待在下一个教程中见到您。

186
00:12:23,140 --> 00:12:29,670
我期待与您比较您的结果，我相信一切都会好的。

187
00:12:29,690 --> 00:12:33,220
实际上，这很有趣且容易完成。

188
00:12:33,290 --> 00:12:38,270
因此，感谢您观看本教程，并期待在下一个教程中与您见面。

189
00:12:38,270 --> 00:12:38,610
。

190
00:12:38,630 --> 00:12:40,660
在此之前，请享受机器学习。

