1
00:00:00,900 --> 00:00:05,040
欢迎回到Dodson的终极课程，今天我感到非常兴奋。

2
00:00:05,040 --> 00:00:10,470
我已经为您准备了令人难以置信的教程，我们将结束一个非常重要的主题，

3
00:00:10,470 --> 00:00:13,710
那就是如何逐步建立模型。

4
00:00:13,710 --> 00:00:15,210
我无法忍受自己。

5
00:00:15,210 --> 00:00:21,600
我必须将额外的行逐步添加到教程的名称中，因为这正是我们所需要的

6
00:00:21,600 --> 00:00:23,020
要去看。

7
00:00:23,040 --> 00:00:28,170
我将为您提供几种不同方法的框架，所有这些都将逐步进行

8
00:00:28,190 --> 00:00:28,320
。

9
00:00:28,440 --> 00:00:31,160
因此，让我们直接进入。

10
00:00:31,200 --> 00:00:36,440
您是否还记得过去的美好时光，那时我们只有一个因变量和一个自变量。

11
00:00:36,480 --> 00:00:41,660
一切都很容易，我们只需要构建一个简单的线性回归。

12
00:00:41,670 --> 00:00:43,620
一切都很好。

13
00:00:43,770 --> 00:00:48,090
但是现在在我们的数据中，我们拥有所有这些列。

14
00:00:48,240 --> 00:00:54,390
那些轻松的日子过去了，现在所有这些列都是因变量的潜在预测因素

15
00:00:55,200 --> 00:01:01,770
它们太多了，我们需要决定我们要保留哪些，我们要保留哪些

16
00:01:01,770 --> 00:01:03,330
想扔出去。

17
00:01:03,510 --> 00:01:08,880
您会问为什么我们需要扔掉列或我们需要删除数据为什么我们不能

18
00:01:08,880 --> 00:01:11,170
使用我们模型中的所有内容。

19
00:01:11,250 --> 00:01:14,130
好吧，我可以想到两个原因。

20
00:01:14,130 --> 00:01:17,230
第一是垃圾进垃圾。

21
00:01:17,310 --> 00:01:24,030
如果您在模型中投入了很多东西，那么您的模型将不是一个好的模型，也不会是一个好的模型。

22
00:01:24,390 --> 00:01:28,510
可靠，它将不会做应该做的事。

23
00:01:28,590 --> 00:01:30,640
可以这么说垃圾模型。

24
00:01:30,750 --> 00:01:36,600
第二天结束时，您将不得不解释这些变量并理解

25
00:01:36,930 --> 00:01:43,860
不仅是其背后的数学原理，而且实际上这意味着某些变量可以预测行为

26
00:01:43,860 --> 00:01:50,580
的因变量，您将不得不向高管人员，老板向人们解释

27
00:01:50,580 --> 00:01:51,390
您正在向。

28
00:01:51,390 --> 00:01:56,100
因此，如果您有一千个变量，尝试进行解释是不切实际的。

29
00:01:56,100 --> 00:02:02,040
因此，您只想保留那些非常重要的东西，那些实际上可以预测某事的东西。

30
00:02:02,040 --> 00:02:04,500
那么我们如何构建模型。

31
00:02:04,500 --> 00:02:09,250
这是构建模型的过程，选择正确的变量。

32
00:02:09,270 --> 00:02:10,660
那么我们如何构建模型。

33
00:02:10,740 --> 00:02:15,900
好了，我们将讨论构建模型的五种方法。

34
00:02:15,900 --> 00:02:20,190
第一是马太的全部被淘汰。

35
00:02:20,190 --> 00:02:25,610
第三是正向选择，之前是双向消除。

36
00:02:25,620 --> 00:02:28,380
第五个是分数比较。

37
00:02:28,380 --> 00:02:33,450
在我们开始之前，我们将先讨论其中的每一个。我想说的是，有时您会

38
00:02:33,450 --> 00:02:42,930
听逐步回归，所以逐步回归实际上指的是数字2 3和4，因为它喜欢

39
00:02:43,070 --> 00:02:46,190
确实是真正的分步方法。

40
00:02:46,260 --> 00:02:52,020
但是有时您会听到人们说逐步回归是指

41
00:02:52,020 --> 00:02:58,980
因此将重放双向消除逐步回归，这很好，那就是

42
00:02:58,980 --> 00:03:03,680
正常情况只是因为您从我们的讨论中看到的更多。

43
00:03:03,690 --> 00:03:09,330
双向弹药是一种更通用的方法，当人们说逐步回归时

44
00:03:09,330 --> 00:03:17,070
默认情况下，它们暗示着双向消除，因此您必须推断出

45
00:03:17,090 --> 00:03:18,580
从那里。

46
00:03:18,660 --> 00:03:21,960
好的，让我们继续我们的方法。

47
00:03:22,080 --> 00:03:23,730
方法号全部。

48
00:03:23,730 --> 00:03:29,190
这不是一个技术术语，我只是将其全部称为“它只是将所有变量都放入”

49
00:03:29,190 --> 00:03:29,310
。

50
00:03:29,310 --> 00:03:31,240
我们刚刚讨论过的事情我们不应该这样做。

51
00:03:31,290 --> 00:03:32,350
你什么时候做。

52
00:03:32,370 --> 00:03:38,310
一种是如果您具有先验知识，如果您知道这些确切的变量是您的真实变量

53
00:03:38,310 --> 00:03:43,580
预测变量，您不必构建任何已知的情况。

54
00:03:43,590 --> 00:03:49,710
您可能从领域知识中知道它，或者您可能因为在某个人之前完成了此模型而知道它

55
00:03:49,710 --> 00:03:52,660
刚刚给了你这些变量，并说请建立一个模型。

56
00:03:52,770 --> 00:03:54,400
那么，您实际上没有选择。

57
00:03:54,450 --> 00:03:56,130
您只需构建模型。

58
00:03:56,340 --> 00:04:02,040
另一个是您可能必须像我在这里真的想不到很好的例子，但是也许

59
00:04:02,340 --> 00:04:08,570
公司中的某个框架说您必须正确使用这些变量。

60
00:04:08,580 --> 00:04:15,360
因此，它有点类似于先验知识，但不是，不是，不是您的决定，而是您可能

61
00:04:15,360 --> 00:04:19,100
想要做不同的事情，但是有一个框架。

62
00:04:19,110 --> 00:04:20,180
你知道可能像银行。

63
00:04:20,190 --> 00:04:26,280
并预测信用，例如某人可能违约而不得不使用的东西

64
00:04:26,280 --> 00:04:28,530
超级球再一次。

65
00:04:28,560 --> 00:04:33,420
我不确定在哪个行业会是这样，但也许会是这样。

66
00:04:33,450 --> 00:04:40,590
第三，如果您正在准备向后消除类型的构造，则可以使用此方法

67
00:04:40,590 --> 00:04:43,500
回归，这是我们的下一种类型。

68
00:04:43,500 --> 00:04:46,640
因此，让我们继续进行向后淘汰。

69
00:04:47,070 --> 00:04:52,470
好的，所以这里有逐步的内容，您可能想要拿出笔来写这些

70
00:04:52,470 --> 00:04:56,660
因为我们现在将拥有一种真正的逐步方法。

71
00:04:56,850 --> 00:04:57,510
好吧。

72
00:04:57,510 --> 00:05:00,120
向后淘汰第一件事。

73
00:05:00,120 --> 00:05:04,900
第一步，您必须选择一个显着性水平以保留在模型中。

74
00:05:05,010 --> 00:05:11,960
因此，默认情况下，我们将使用5％，即0.05，然后在下一步中使用它，因此

75
00:05:12,200 --> 00:05:15,810
从一开始，您就决定要达到这个重要水平。

76
00:05:15,810 --> 00:05:20,880
第二步，使用所有可能的预测变量拟合整个模型，以便您在所有方法中做到这一点

77
00:05:20,880 --> 00:05:27,500
我们刚刚讨论过，您将所有变量都放入了模型中，现在我们将开始

78
00:05:27,500 --> 00:05:28,720
摆脱它们。

79
00:05:28,760 --> 00:05:32,520
第三步，您认为预测值具有最高的P值。

80
00:05:32,550 --> 00:05:37,700
因此，请记住我们在Grettel或任何软件中讨论的那些P值，或者您

81
00:05:37,700 --> 00:05:38,670
可以看到他们。

82
00:05:38,660 --> 00:05:42,870
因此，在您拟合模型之后，您将看到具有最高P值的模型。

83
00:05:43,110 --> 00:05:52,340
因此，如果p大于您的显着性水平，则转到步骤4，然后必须删除步骤4

84
00:05:52,340 --> 00:05:56,400
预测将基本上删除具有最高P值的变量。

85
00:05:56,610 --> 00:06:02,720
从第4步开始，在该变量之前拟合模型，所以这里有一个星星，因为我只是想

86
00:06:02,730 --> 00:06:08,650
提醒自己告诉您，是否只是删除变量。

87
00:06:08,840 --> 00:06:11,080
显然，您可以这样说。

88
00:06:11,090 --> 00:06:16,220
好吧，现在我有了新模型，您必须对它们全部进行改装，而必须重新创建所有模型

89
00:06:16,230 --> 00:06:21,960
用较少数量的变量重建它，所以如果您有的话，也许我不知道一百个变量，

90
00:06:21,950 --> 00:06:24,570
您删除了其中之一，现在有99个。

91
00:06:24,590 --> 00:06:27,830
好吧，您必须重建它，以便系数会有所不同。

92
00:06:27,870 --> 00:06:29,710
该常数将有所不同。

93
00:06:29,840 --> 00:06:36,110
您需要执行该步骤，因为一旦删除变量，它将影响所有其他变量

94
00:06:36,140 --> 00:06:38,580
在整个回归中

95
00:06:38,780 --> 00:06:41,930
因此，在执行第5步之后，请返回到第3步。

96
00:06:41,960 --> 00:06:46,880
再次寻找新模型中具有最高值的变量。

97
00:06:47,030 --> 00:06:49,340
您将其取出并移除。

98
00:06:49,340 --> 00:06:51,290
因此，基本上，第4步将其删除。

99
00:06:51,290 --> 00:06:54,140
您再次使用较少的变量来拟合模型。

100
00:06:54,140 --> 00:07:00,410
依此类推，直到您到达最高点的偶数变量为止

101
00:07:00,410 --> 00:07:05,660
P值即p值仍小于您的显着性水平。

102
00:07:05,660 --> 00:07:11,990
因此，如果条件P大于s.l是不正确的，那么您将不再只有4。

103
00:07:12,000 --> 00:07:16,080
你去了鳍，这Feehan的情况到此为止。

104
00:07:16,090 --> 00:07:17,800
您的模型已经准备就绪。

105
00:07:17,810 --> 00:07:25,170
因此，一旦您留在模型中的所有变量都存在，p值就小于

106
00:07:25,400 --> 00:07:26,570
显着性水平。

107
00:07:26,630 --> 00:07:28,340
您的模型已准备就绪。

108
00:07:29,030 --> 00:07:31,250
这就是向后消除方法的工作方式。

109
00:07:31,380 --> 00:07:32,780
让我们继续下一个。

110
00:07:32,880 --> 00:07:35,110
下一个方法是正向选择。

111
00:07:35,270 --> 00:07:40,140
这听起来像是相反的，右上角的图片确实说明了相反的情况

112
00:07:40,150 --> 00:07:40,340
。

113
00:07:40,580 --> 00:07:46,410
但这比简单地逆转过程要复杂得多。

114
00:07:46,440 --> 00:07:50,390
您将看到它是一个更大的过程。

115
00:07:50,450 --> 00:07:52,220
我们从步骤1开始。

116
00:07:52,380 --> 00:07:55,570
选择重要性级别以输入模型。

117
00:07:55,910 --> 00:07:59,600
在这种情况下，我们将再次选择5％。

118
00:07:59,880 --> 00:08:00,920
然后转到步骤2。

119
00:08:00,920 --> 00:08:04,880
我们适合所有可能的简单回归模型。

120
00:08:04,880 --> 00:08:12,080
因此，我们采用因变量，并使用每个独立变量创建一个回归模型

121
00:08:12,080 --> 00:08:13,040
我们有。

122
00:08:13,470 --> 00:08:20,060
然后，从所有这些模型中选择一个，对于独立模型，我们选择p值最低的模型

123
00:08:20,060 --> 00:08:21,490
变量。

124
00:08:21,720 --> 00:08:25,220
因此，您已经可以看到，这本身就是很多工作。

125
00:08:25,280 --> 00:08:27,550
然后我们要做的是进入第三步。

126
00:08:27,620 --> 00:08:36,380
我们保留刚才选择的变量，并使用一个额外的预测变量拟合所有其他可能的模型

127
00:08:36,990 --> 00:08:39,440
添加到我们通常拥有的那个。

128
00:08:39,440 --> 00:08:41,250
那是什么意思。

129
00:08:41,250 --> 00:08:47,630
这意味着我们选择了带有一个变量的简单线性回归。

130
00:08:47,630 --> 00:08:55,190
现在我们需要用两个变量构造所有可能的线性回归，其中两个变量之一

131
00:08:55,190 --> 00:08:56,390
是各种各样的之一。

132
00:08:56,470 --> 00:09:03,240
因此，基本上，我们一一添加所有其他可能的变量，所以我们选择确定，让我们添加

133
00:09:03,240 --> 00:09:06,580
在此变量上，然后让我们添加下一个。

134
00:09:06,650 --> 00:09:14,280
但是单独地，所以我们构造所有可能的2个变量线性回归，并保持

135
00:09:14,270 --> 00:09:16,300
保留我们非常选择的变量。

136
00:09:16,590 --> 00:09:18,930
那之后我们该怎么办。

137
00:09:18,920 --> 00:09:26,150
在所有这两种可能的变量回归中，我们考虑一个新变量

138
00:09:26,150 --> 00:09:29,710
我们添加的P值最低。

139
00:09:29,820 --> 00:09:37,020
因此，如果p值小于我们的显着性水平，则意味着您知道变量是一个好的变量

140
00:09:37,260 --> 00:09:41,070
它是一个重要的变量，然后我们返回到步骤3。

141
00:09:41,150 --> 00:09:41,830
这意味着什么。

142
00:09:41,850 --> 00:09:47,760
意味着现在我们有两个变量的回归，现在我们将添加第三个变量，我们将尝试

143
00:09:47,750 --> 00:09:54,990
我们剩下的所有可能的变量作为我们的第三个变量，然后从所有这些模型中

144
00:09:54,990 --> 00:10:00,800
三个变量，我们将转到第4步，然后再次选择该变量的最低p值之一

145
00:10:00,810 --> 00:10:01,430
变量。

146
00:10:01,430 --> 00:10:02,160
我们添加了。

147
00:10:02,310 --> 00:10:03,230
我会继续这样做。

148
00:10:03,240 --> 00:10:08,340
因此，基本上，我们将继续增长回归模型，而不仅仅是随机选择

149
00:10:08,340 --> 00:10:15,340
一次所有可能的组合中的一个，并一次以一个变量增长

150
00:10:15,350 --> 00:10:15,530
。

151
00:10:16,360 --> 00:10:24,430
然后我们只会在添加的变量的p值大于

152
00:10:24,700 --> 00:10:25,860
我们的意义水平。

153
00:10:25,900 --> 00:10:32,590
因此，当此条件小于SL不成立时，我们就不进行步骤3，就完成了回归

154
00:10:32,590 --> 00:10:38,120
很好，因为尽管我们刚刚添加了该变量，但它不再重要了。

155
00:10:38,200 --> 00:10:43,090
而且我们也知道我们选择了P值最低的那个，所以没有其他变量

156
00:10:43,090 --> 00:10:51,910
我们可以添加其p值将大于s.l，从那时起的任何回归

157
00:10:51,910 --> 00:10:56,390
从此以后，该变量将始终是微不足道的。

158
00:10:56,410 --> 00:10:58,680
至此，我们完成了回归。

159
00:10:58,900 --> 00:11:03,480
这里的技巧是，您不保留当前模型，而是保留先前的模型。

160
00:11:03,700 --> 00:11:08,830
这是有道理的，因为您刚刚添加了一个无关紧要的变量，那有什么意义呢？

161
00:11:08,830 --> 00:11:10,760
该变量的值只向后退一步。

162
00:11:11,050 --> 00:11:12,760
这就是前向选择的工作方式。

163
00:11:12,760 --> 00:11:17,200
我知道这有点令人困惑，但是请尝试绕着头，也许通读这些说明

164
00:11:17,200 --> 00:11:18,130
再一次。

165
00:11:18,120 --> 00:11:24,720
当您确实感觉到很有意义时，就像某种图片正在发生的事情一样。

166
00:11:25,600 --> 00:11:29,290
接下来，我们将继续进行双向模仿消除。

167
00:11:29,290 --> 00:11:35,440
因此，您可以假设这一步，或者也许可以，它结合了两个第一步。

168
00:11:35,500 --> 00:11:40,780
选择要保留的重要级别或要输入，然后选择要保留的重要级别。

169
00:11:40,990 --> 00:11:47,530
因此，我们将在两种情况下都选择Phipson，但这取决于您选择要执行的操作

170
00:11:47,530 --> 00:11:54,540
正向选择的下一步意味着我们刚才讨论的那个。

171
00:11:54,550 --> 00:12:01,420
因此，在输入新变量才能输入时，新变量必须小于有效位

172
00:12:01,420 --> 00:12:03,220
级别进入。

173
00:12:03,220 --> 00:12:07,610
因此，基本上基于前向选择方法添加一个新变量。

174
00:12:07,840 --> 00:12:11,450
步骤3执行反向淘汰过程的所有步骤。

175
00:12:11,470 --> 00:12:13,720
所以现在您应该有两个变量。

176
00:12:13,780 --> 00:12:19,980
开始摆脱它们，看看是否可以摆脱它们，然后再回到步骤2，

177
00:12:19,990 --> 00:12:21,550
然后再增加一个变量。

178
00:12:21,580 --> 00:12:27,100
每次您口头说话时，假设您从五岁开始就去了六次

179
00:12:27,100 --> 00:12:31,180
到六个，您必须形成向后啮合的所有步骤，因此您不只是消除一个变量，如果

180
00:12:31,180 --> 00:12:34,480
您可以消除一到三个，但可以消除很多。

181
00:12:34,780 --> 00:12:37,420
然后从那里返回到步骤2。

182
00:12:37,450 --> 00:12:39,220
因此，这是一个非常反复的过程。

183
00:12:39,220 --> 00:12:46,000
您一直这样做，直到无法添加新变量，否则无法输入变量或没有旧变量

184
00:12:46,000 --> 00:12:48,870
只要到达那里，变量就可以退出。

185
00:12:48,880 --> 00:12:52,740
然后，您就可以进行建模了，因为模型已经可以调用了。

186
00:12:52,810 --> 00:12:57,220
您可以添加任何可以删除的内容，这意味着您已经创建了模型。

187
00:12:57,220 --> 00:13:00,520
因此，这是较为繁琐的方法之一。

188
00:13:00,550 --> 00:13:06,880
当然，您必须拥有一台计算机才能为您完成此操作，否则您会发疯

189
00:13:06,880 --> 00:13:08,430
变量放入并取出。

190
00:13:08,440 --> 00:13:11,440
但这就是双向消除的工作方式。

191
00:13:11,880 --> 00:13:17,600
有人再次称其为逐步回归。

192
00:13:17,740 --> 00:13:20,940
最后是所有可能的模型。

193
00:13:20,950 --> 00:13:29,500
因此，这是最有可能使用Saro的方法，也是最消耗资源的方法，因为

194
00:13:29,500 --> 00:13:34,960
选择一个拟合优度准则，例如，caky准则可以是R的平方

195
00:13:34,960 --> 00:13:40,470
不同的准则，那么您将构建所有可能的回归模型，因此，如果有和，

196
00:13:40,690 --> 00:13:46,420
这些变量的乘方将是2，而这些变量的总和减去1

197
00:13:46,420 --> 00:13:53,350
确切地可能有多少个模型，然后在第三步中选择这些模型之一

198
00:13:53,350 --> 00:13:56,850
以您正在寻找的最佳标准。

199
00:13:56,860 --> 00:13:58,410
到这里，您的模型已准备就绪。

200
00:13:58,420 --> 00:14:03,700
听起来很简单，但让我们看个例子，即使您女儿有10列，

201
00:14:03,700 --> 00:14:05,630
有1023个模型。

202
00:14:05,650 --> 00:14:06,760
太疯狂了

203
00:14:06,760 --> 00:14:11,050
那是疯狂的模型数量，我们不是在谈论您已经过滤掉的列

204
00:14:11,050 --> 00:14:18,110
因此称呼他们，您知道像我们的示例一样，您可能有五六列。

205
00:14:18,100 --> 00:14:24,070
现在，我们讨论的是何时获得所需的数据集几乎是一条规则，

206
00:14:24,070 --> 00:14:26,390
大概是100列

207
00:14:26,460 --> 00:14:33,940
我处理过的数据集大约有50到100也许更多的列，而不是去

208
00:14:33,940 --> 00:14:37,990
通过他们，这就是这种方法建议通过他们并挑选出那些的方法。

209
00:14:37,990 --> 00:14:39,650
您认为应该在模型中。

210
00:14:39,670 --> 00:14:40,810
你把所有东西都扔进去了。

211
00:14:40,810 --> 00:14:48,250
好吧，这不是一个好方法，因为基本上它正在以指数级增长

212
00:14:48,250 --> 00:14:49,160
呈指数增长。

213
00:14:49,360 --> 00:14:56,310
从这种方法获得结果是非常消耗资源的。

214
00:14:56,680 --> 00:15:02,010
最后，我们如何得出结论。

215
00:15:02,130 --> 00:15:07,750
我们提供了五种方法来构建模型模型，所有方法都可以消除反向词，以便按方向进行选择

216
00:15:07,780 --> 00:15:09,810
移民和分数比较。

217
00:15:10,030 --> 00:15:16,810
因此，在这些教程中将探讨一种方法，以便使我们了解如何

218
00:15:16,810 --> 00:15:22,030
逐步建立模型并获得一些实践是反向淘汰过程，因为这是

219
00:15:22,030 --> 00:15:28,010
它们中最快的一个或全部，您仍然可以准确地了解逐步方法的工作原理。

220
00:15:28,180 --> 00:15:35,590
另外，我们还将在此过程中添加一些其他技巧，以确保我们的模型非常健壮，无法等待

221
00:15:35,650 --> 00:15:36,920
开始。

222
00:15:36,970 --> 00:15:37,930
很多理论。

223
00:15:37,930 --> 00:15:40,520
让我们开始下一次强迫的练习

