1
00:00:00,360 --> 00:00:04,400
您好，欢迎来到本美术教程，您会学到很多东西。

2
00:00:04,530 --> 00:00:11,040
但是，如果不先介绍最流行的课程之一而离开本课程，那将是可耻的

3
00:00:11,310 --> 00:00:17,850
机械中的算法虽然最近很流行，但它绝对是一个非常强大的模型

4
00:00:18,030 --> 00:00:20,580
特别是在处理大型数据集时。

5
00:00:20,580 --> 00:00:24,490
它会在快速执行的同时为您提供非常高的性能。

6
00:00:24,720 --> 00:00:30,930
在谈到性能和执行速度时，重要的是要提醒您，前GI加速是最重要的。

7
00:00:30,930 --> 00:00:37,490
在模型性能和执行速度方面，梯度提升的强大实现。

8
00:00:37,530 --> 00:00:41,320
因此，将其包含在工具箱中对您而言非常重要。

9
00:00:41,400 --> 00:00:43,230
因此，让我们实施额外的提升。

10
00:00:43,230 --> 00:00:48,970
这只是一个介绍，因此我们将对实际上boost做一个简单的实现。

11
00:00:49,050 --> 00:00:54,810
但是您必须考虑一下计算机，然后才能尝试解决自己的问题。

12
00:00:54,810 --> 00:01:00,090
数据集，您将看到，即使通过这种简单的实现，它也一定会给您一些

13
00:01:00,150 --> 00:01:01,740
完美的表现。

14
00:01:01,990 --> 00:01:06,900
现在，我们要做的是解决我们在本课程中要处理的业务问题之一。

15
00:01:06,900 --> 00:01:11,850
这实际上将是我们在深度学习部分中解决的问题。

16
00:01:11,850 --> 00:01:17,400
请记住，这是客户流失模型问题，我们需要预测银行的客户

17
00:01:17,400 --> 00:01:18,390
会离开银行。

18
00:01:18,540 --> 00:01:24,240
所以这是一个分类问题，我们将客户分为两类

19
00:01:24,240 --> 00:01:26,860
银行和那些不会离开银行的人。

20
00:01:26,880 --> 00:01:31,140
因此请记住，对于该问题，我们获得了86％的准确度。

21
00:01:31,260 --> 00:01:38,070
但这花了相当长的时间，因为我们训练了一个带有许多错误的人工神经网络，因此

22
00:01:38,070 --> 00:01:40,170
执行花了很多时间。

23
00:01:40,200 --> 00:01:42,850
因此，在本节中，我们将做同样的事情。

24
00:01:42,850 --> 00:01:46,480
我们将对这种流失建模问题应用极端观点。

25
00:01:46,500 --> 00:01:47,850
该数据集包含。

26
00:01:47,850 --> 00:01:51,710
如果我还记得13个功能，那不是很大的数据集。

27
00:01:51,780 --> 00:01:57,750
需要强调的是，即使这是一个很大的数据集，也非常好

28
00:01:57,810 --> 00:02:01,650
就性能而言，额外的提升将是最好的模型之一。

29
00:02:01,710 --> 00:02:05,810
那是为了获得良好的准确性和执行速度。

30
00:02:05,820 --> 00:02:12,530
因此，例如，如果您使用的是大型数据集，我强烈建议您测试前GI增强功能。

31
00:02:12,530 --> 00:02:17,100
好的，我们现在要进行预处理阶段。

32
00:02:17,180 --> 00:02:21,910
因此，这只是第1部分，因为第2部分将实现人工神经网络。

33
00:02:22,040 --> 00:02:27,740
因此，我们只想针对与该客户流失相关的客户流失建模问题预处理数据

34
00:02:27,740 --> 00:02:29,540
在我们跌倒时进行建模。

35
00:02:29,990 --> 00:02:34,850
但是实际上，我们不会在此预处理阶段进行所有操作。

36
00:02:34,850 --> 00:02:41,360
原因是对于人工神经网络来说，缩放功能完全是强制性的。

37
00:02:41,360 --> 00:02:45,610
所问问题的功能必须应用于深度学习。

38
00:02:45,800 --> 00:02:51,440
但好消息是，对于真正推动世界的人们，因为真正推动是一个伟大而不断发展的模式

39
00:02:51,440 --> 00:02:56,960
具有决策树的特征就完全不需要杀死，这就是

40
00:02:56,960 --> 00:03:02,520
关于前GI嘘声的好处在于它的高性能和快速执行速度。

41
00:03:02,590 --> 00:03:08,210
您可以保留对数据集问题的解释，而结果却只有一半

42
00:03:08,210 --> 00:03:15,170
在建立模型之后得到，所以我们现在可以理解为什么前GI布鲁斯之所以如此受欢迎是因为它具有

43
00:03:15,170 --> 00:03:22,520
这三个品质是第一品质，高性能，第二品质，第一执行速度和第三品质。

44
00:03:22,520 --> 00:03:26,240
您可以保留对问题和模型的所有解释。

45
00:03:26,300 --> 00:03:29,320
因此，绝对是拥有您的工具包的模型。

46
00:03:29,330 --> 00:03:35,630
好的，因此在这里必须杀死功能，因此，我们将从这里获取所有信息

47
00:03:35,840 --> 00:03:42,360
像这样的副本顶部，我们会将其粘贴到我们的极端使用文件中。

48
00:03:42,380 --> 00:03:42,870
开始了。

49
00:03:42,890 --> 00:03:45,850
现在我们可以实施极端主义了。

50
00:03:45,860 --> 00:03:54,710
因此，首先让我们为火车设置一个适合ex-GI的新部分。

51
00:03:54,840 --> 00:03:55,670
好吧。

52
00:03:55,670 --> 00:03:58,540
首先让我们开始吧。

53
00:03:58,820 --> 00:04:01,180
所以像往常一样有一个包装。

54
00:04:01,180 --> 00:04:05,870
它是额外的Boost程序包，使我们可以非常有效地实施额外的豪饮。

55
00:04:05,870 --> 00:04:07,560
所以让我们像往常一样停在这里。

56
00:04:07,600 --> 00:04:17,380
安装该软件包，并在额外软件包的名称内安装，该名称就是这样的X G boost。

57
00:04:17,390 --> 00:04:22,910
然后选择此行，然后按Command Control Enter执行。

58
00:04:22,910 --> 00:04:25,900
那就是安装额外的增强包。

59
00:04:26,190 --> 00:04:26,540
对。

60
00:04:26,540 --> 00:04:28,720
我们可以看到它在进动。

61
00:04:28,970 --> 00:04:33,690
在这里，我们可以在此包文件夹中找到下载的二进制包。

62
00:04:33,770 --> 00:04:36,160
已安装所有良好的前GI助推器。

63
00:04:36,410 --> 00:04:41,150
因此，让我们在该部分中添加注释。

64
00:04:41,270 --> 00:04:46,150
现在，让我们导入专业知识包，因为确实已经安装了它。

65
00:04:46,340 --> 00:04:51,580
但是，如果我们走到最底端，极端安装就不重要了。

66
00:04:51,680 --> 00:04:58,540
我们希望使其浪漫，就像您的音乐库和X G boost内部一样。

67
00:04:58,990 --> 00:05:01,540
那就是我们将导入包。

68
00:05:01,580 --> 00:05:02,350
好吧。

69
00:05:02,350 --> 00:05:04,890
现在让我们实现实际的提升。

70
00:05:05,000 --> 00:05:10,100
实际上这将占用一行，因为我们将使分类器额外

71
00:05:10,090 --> 00:05:11,640
为自己提高班级。

72
00:05:11,870 --> 00:05:18,530
因此，基本上，我们只需要创建一个新变量，就可以像通常的分类器一样调用它，然后等于

73
00:05:18,740 --> 00:05:22,570
然后我们使用这个额外包中的额外提升功能。

74
00:05:22,700 --> 00:05:32,440
因此，请额外点击一下并加上括号，让我们单击此处并按一下，以获得实际上有关此信息的一些信息。

75
00:05:32,450 --> 00:05:33,680
使用功能。

76
00:05:33,680 --> 00:05:39,520
因此，信息是我们对参数以及此处需要的参数感兴趣。

77
00:05:39,890 --> 00:05:46,460
好，所以首先我们看到我们有这个harams参数，它实际上是参数列表

78
00:05:46,460 --> 00:05:49,340
参数是您可以在此处看到的所有参数。

79
00:05:49,370 --> 00:05:55,250
例如，用于控制学习率的操作数是最小的伽玛参数

80
00:05:55,250 --> 00:05:56,320
减少损失。

81
00:05:56,450 --> 00:05:58,480
好吧，您有很多这样的参数。

82
00:05:58,610 --> 00:06:02,810
但是本教程只是对X的介绍。

83
00:06:02,870 --> 00:06:08,870
因此，我们不会在模型和本课程上做任何尝试，但是我相信在以后的课程中，我会

84
00:06:08,870 --> 00:06:14,720
对一些更复杂的问题进行一些更复杂的实现，从而进一步提高效率。

85
00:06:14,720 --> 00:06:19,460
但是在本课程中，仅以实际增强的简单介绍结束，以便您可以

86
00:06:19,460 --> 00:06:23,350
至少对它有一些了解，并以您为目标。

87
00:06:23,360 --> 00:06:29,900
因此，让我们现在不再专注于此，而继续讲当然是

88
00:06:29,900 --> 00:06:31,220
第一个是数据。

89
00:06:31,220 --> 00:06:36,870
因此，当然有您想要训练非常小的角色的训练集数据集。

90
00:06:36,890 --> 00:06:38,600
因此，让我们现在就说。

91
00:06:38,630 --> 00:06:44,260
因此，第一个参数数据等于训练集。

92
00:06:44,330 --> 00:06:45,240
开始了。

93
00:06:45,410 --> 00:06:50,810
实际上，我们只需要转换中的特征，因此我们将删除因变量

94
00:06:50,810 --> 00:06:56,200
之所以选择此培训集，是因为培训集既包含功能，又具有不可估量的价值。

95
00:06:56,390 --> 00:07:00,550
但是此数据参数期望的只是功能。

96
00:07:00,560 --> 00:07:05,030
因此，在这里我们添加了一些括号，并删除了之前非常宝贵的内容。

97
00:07:05,170 --> 00:07:06,390
以及该索引有什么作用。

98
00:07:06,500 --> 00:07:11,510
为此，我们需要先导入数据集，然后再导入数据集。

99
00:07:11,510 --> 00:07:14,510
让我们快速将步枪设置为工作目录。

100
00:07:14,510 --> 00:07:18,380
因此，现在我们分部分讨论，然后是第49节的额外提要。

101
00:07:18,590 --> 00:07:19,600
那是正确的。

102
00:07:19,610 --> 00:07:24,940
确保必须打开Mullings文件，然后单击此处的sads工作目录。

103
00:07:25,040 --> 00:07:25,850
现在我们开始。

104
00:07:25,880 --> 00:07:27,780
现在我们可以导入数据集。

105
00:07:28,010 --> 00:07:29,170
让我们导入它。

106
00:07:29,240 --> 00:07:30,820
那就是数据集。

107
00:07:30,920 --> 00:07:35,840
但是请记住，在此数据集中我们没有获取所有自变量，因为我们不感兴趣

108
00:07:35,840 --> 00:07:42,990
在行号客户ID和姓氏中，我们知道这些复兴对因变量没有影响。

109
00:07:43,070 --> 00:07:49,790
因此我们将其删除，这就是我们在此行数据集中所做的等于14的数据集。

110
00:07:49,790 --> 00:07:55,790
这意味着我们从数据集的第四个变量即信用评分中获取所有变量

111
00:07:56,030 --> 00:07:58,950
直到最后一个变量退出了家属复兴。

112
00:07:58,950 --> 00:08:00,540
那就是因变量。

113
00:08:00,650 --> 00:08:04,430
因此，让我们采取这一行并执行。

114
00:08:04,520 --> 00:08:11,420
现在，如果您查看我们的数据集，那么它将包含所有相关功能并加深可逆

115
00:08:11,480 --> 00:08:17,990
退出，因此挑战在于所有这些自变量在这里我们要预测客户

116
00:08:17,990 --> 00:08:20,210
将离开或留在银行。

117
00:08:20,210 --> 00:08:26,930
这就是我们考虑用来训练模型和测试其性能的数据集，因此

118
00:08:26,930 --> 00:08:32,360
我们必须删除的因变量的索引现在，在data参数的极端损失函数中

119
00:08:32,520 --> 00:08:35,460
是退出列的最后一个索引。

120
00:08:35,570 --> 00:08:40,960
而且由于我们有11个变量，因此索引为11。

121
00:08:40,970 --> 00:08:47,510
因此，让我们回到真正的增强功能，让我们回到函数，因此这里有2个输入

122
00:08:47,780 --> 00:08:49,910
负11。

123
00:08:49,910 --> 00:08:54,930
好吧，所以我们有完整的培训课程，但是没有部门竞争对手。

124
00:08:54,930 --> 00:08:55,920
我们想要的。

125
00:08:55,920 --> 00:09:00,590
因此，现在让我们回过头来看看我们是否需要有关此第一个参数的更多信息。

126
00:09:00,840 --> 00:09:06,810
好吧，确实有一些非常重要的信息我们需要在这里考虑的是，这些数据

127
00:09:06,810 --> 00:09:10,650
set必须是X才能成为矩阵。

128
00:09:10,650 --> 00:09:18,420
所以这基本上是Matrix的一种类型，但我们还可以看到，除了data之外，data参数

129
00:09:18,420 --> 00:09:20,340
接受Matrix。

130
00:09:20,340 --> 00:09:22,400
但这不是矩阵。

131
00:09:22,400 --> 00:09:28,830
这是一个数据框，因此如果我们以这种方式输入要素，则将无法正常工作，因此我们可以将其转换为

132
00:09:29,070 --> 00:09:33,690
打成矩阵或简单矩阵。

133
00:09:33,690 --> 00:09:39,780
因此，让我们采用最简单的解决方案，将这个数据框特征转换为矩阵，您就会知道

134
00:09:39,780 --> 00:09:40,750
这个怎么做。

135
00:09:40,800 --> 00:09:48,850
我们只需要将其用作点矩阵函数并放在圆括号内，因为它是一个函数。

136
00:09:48,900 --> 00:09:51,180
此数据框的功能。

137
00:09:51,180 --> 00:09:51,800
开始了。

138
00:09:51,870 --> 00:09:55,650
现在这成为一个矩阵，这正是我们所需要的。

139
00:09:55,650 --> 00:09:58,380
好的，然后下一个论点。

140
00:09:58,380 --> 00:10:03,810
所以在这里您还有很多其他论点，但是这些不是强制性的，我们将不再关注它们

141
00:10:03,810 --> 00:10:10,940
现在，但是下一个强制性参数是这个标签参数，因为确实在这里我们可以将矩阵

142
00:10:10,940 --> 00:10:16,290
特征，但当然要训练分类模型，我们不仅需要特征矩阵，还需要

143
00:10:16,290 --> 00:10:18,230
也是无价之宝。

144
00:10:18,360 --> 00:10:21,870
这就是我们在此label参数中添加的内容。

145
00:10:21,870 --> 00:10:28,680
就像您可能期望的那样，因为我们将要素放入矩阵中，所以我们需要将此标签参数放入

146
00:10:29,190 --> 00:10:37,530
作为向量并将我们的因变量作为向量，我们需要输入标签等于我们的训练集

147
00:10:38,370 --> 00:10:39,250
比美元。

148
00:10:39,360 --> 00:10:45,160
然后，我们使用已存在的因变量的名称，这将为我们提供一个向量。

149
00:10:45,180 --> 00:10:48,380
因此训练说美元退出是因变量。

150
00:10:48,420 --> 00:10:55,810
但是给出的正是我们所需要的向量，因为正如您所看到的，label应该是向量。

151
00:10:55,830 --> 00:11:02,010
响应值的向量，响应值当然是因变量的值。

152
00:11:02,460 --> 00:11:03,120
好吧。

153
00:11:03,240 --> 00:11:06,700
现在下一个参数是下一个参数。

154
00:11:06,780 --> 00:11:13,510
好吧，我们这里需要输入第三种强制性论点，而实际上是在上面。

155
00:11:13,530 --> 00:11:18,540
但我想将标签放在有意义的功能矩阵后面。

156
00:11:18,870 --> 00:11:25,080
现在，我们需要输入第三个参数，这是and的参数和round的参数。

157
00:11:25,110 --> 00:11:28,330
arguments是最大迭代次数。

158
00:11:28,350 --> 00:11:33,990
因此，由于我们无法很好地解决过于复杂的问题，因此最多可以进行10次迭代

159
00:11:33,990 --> 00:11:34,880
足够。

160
00:11:35,070 --> 00:11:43,620
因此，我们将在此处输入，大约等于10，最多将训练十次迭代。

161
00:11:43,620 --> 00:11:44,340
完善。

162
00:11:44,490 --> 00:11:52,430
现在，实际上这行代码已准备就绪，可以执行以训练额外的boost分类器。

163
00:11:52,470 --> 00:11:58,740
因此，即使有了额外的软件包，额外的提升也是一个非常高级的机器学习问题

164
00:11:59,070 --> 00:12:05,040
您只需要简单的一行代码即可非常有效地实现它。

165
00:12:05,040 --> 00:12:05,700
好吧。

166
00:12:05,700 --> 00:12:10,740
我们现在不打算执行这一行，因为首先我们需要运行数据预处理阶段，然后

167
00:12:10,740 --> 00:12:15,830
那么我想添加一些代码段来评估我们极小的性能。

168
00:12:15,870 --> 00:12:18,660
因此，我们将最终执行整个操作。

169
00:12:18,810 --> 00:12:25,190
但是现在，让我们添加最后的部分进行评估，以实际提高性能，当然

170
00:12:25,200 --> 00:12:29,620
我们将采用我们全面的交叉验证技术对其进行评估。

171
00:12:29,700 --> 00:12:36,190
因此，在这里，我将进行仔细的交叉验证部分，它就在这里，

172
00:12:36,210 --> 00:12:39,540
要使用它很小。

173
00:12:39,540 --> 00:12:45,460
因此，在这里，我只需要复制此部分，回到我的执行小文件，然后将其粘贴到此处。

174
00:12:45,810 --> 00:12:47,730
并注意其中的内容。

175
00:12:47,730 --> 00:12:52,250
我们需要更改分类器，因为此处就是作为分类器的内核。

176
00:12:52,380 --> 00:13:00,700
因此，基本上，我们只需要通过额外的boost分类器将此内核替换为VM分类器即可。

177
00:13:00,870 --> 00:13:08,010
所以我只是将其复制到此处，然后返回到我的K4交叉验证部分，并粘贴代码以尝试

178
00:13:08,010 --> 00:13:11,980
实际上是在这里进行气化炉培训。

179
00:13:12,330 --> 00:13:16,000
好的，然后我们需要在此部分中添加另一行代码。

180
00:13:16,020 --> 00:13:22,710
这与以下事实有关：这个额外的小值会将预测作为概率返回。

181
00:13:22,710 --> 00:13:28,830
您知道它将返回类别1的概率，因此您知道此技巧可将

182
00:13:28,830 --> 00:13:32,340
真实预测中的概率0 0 1。

183
00:13:32,520 --> 00:13:34,940
好吧，我们需要添加这一行代码。

184
00:13:35,080 --> 00:13:39,430
Pred等于，然后加上括号。

185
00:13:39,430 --> 00:13:41,100
完全正确。

186
00:13:41,370 --> 00:13:44,780
大于0.5。

187
00:13:44,880 --> 00:13:49,860
因此，如果概率大于0.5，则我们将为1。

188
00:13:50,010 --> 00:13:54,700
如果概率低于0.5，那么为什么散布为零。

189
00:13:54,890 --> 00:13:57,850
这样便可以得到二进制结果0或1。

190
00:13:57,920 --> 00:14:01,960
这正是S.可以期望的增加。

191
00:14:02,020 --> 00:14:07,860
最终在执行全部操作之前，我们仍然需要首先更改两件事。

192
00:14:07,890 --> 00:14:12,830
事实是，由于期望训练集是一个矩阵，所以所有这些都将是相同的

193
00:14:12,830 --> 00:14:13,910
测试集。

194
00:14:13,910 --> 00:14:21,840
因此，在这里我们还需要添加点矩阵，并在括号内将测试充满。

195
00:14:22,220 --> 00:14:23,600
这是第一个变化。

196
00:14:23,600 --> 00:14:29,750
现在，第二个变化当然与深端Roybal的指数有关，因为这里有三个

197
00:14:29,750 --> 00:14:35,860
是我们实施仔细辐射的上一个问题中因变量的索引。

198
00:14:35,870 --> 00:14:41,600
因此我们需要在新问题中将这三个索引替换为因变量的索引

199
00:14:41,600 --> 00:14:45,830
不是3，而是11，这里也一样。

200
00:14:45,830 --> 00:14:48,940
在混淆矩阵中为11。

201
00:14:49,340 --> 00:14:49,910
好吧。

202
00:14:49,970 --> 00:14:53,910
现在一切就绪，我们可以执行整个代码。

203
00:14:53,960 --> 00:14:57,530
因此，让我们做一下，看看我们获得哪种精度。

204
00:14:57,680 --> 00:14:59,950
因此，让我们回到顶部。

205
00:15:00,170 --> 00:15:02,120
我们已经导入了数据集。

206
00:15:02,120 --> 00:15:07,030
因此，现在让我们将分类变量编码为因素。

207
00:15:07,040 --> 00:15:07,910
开始了。

208
00:15:07,910 --> 00:15:08,820
做完了

209
00:15:08,870 --> 00:15:12,800
现在，让我们将数据集分为训练集和测试集。

210
00:15:12,800 --> 00:15:13,540
开始了。

211
00:15:13,550 --> 00:15:14,880
也做。

212
00:15:14,900 --> 00:15:18,260
现在，让我们将额外的内容与训练集结合起来。

213
00:15:18,470 --> 00:15:20,650
因此，实际的包已经被导入了。

214
00:15:20,840 --> 00:15:26,540
因此，基本上我们只需要选择这一行并执行即可。

215
00:15:26,540 --> 00:15:27,330
开始了。

216
00:15:27,380 --> 00:15:31,090
我们获得每轮均方根误差的信息。

217
00:15:31,100 --> 00:15:34,880
因此，基本的平方误差基本上是误差的相关计算。

218
00:15:35,000 --> 00:15:36,670
您可以将其描述为错误。

219
00:15:36,680 --> 00:15:40,590
当然，误差越小，您的MO越好。

220
00:15:40,730 --> 00:15:45,250
我们可以看到，从第一次迭代到最后一个迭代都有所困扰。

221
00:15:45,270 --> 00:15:49,910
误差从0.14 1降低到0.20 9。

222
00:15:50,000 --> 00:15:56,150
此外，我们可以看到最大10次迭代是一个不错的选择，因为我们可以看到

223
00:15:56,240 --> 00:16:00,350
它大约在0.30左右收敛。

224
00:16:00,560 --> 00:16:05,750
随时尝试进行更多的迭代，并尝试查看其是否收敛于较小的数字

225
00:16:05,750 --> 00:16:06,510
超过30。

226
00:16:06,650 --> 00:16:11,290
如果您得到的数字接近0.30，那么10次迭代是一个不错的选择。

227
00:16:11,360 --> 00:16:15,610
因此，在火车上实施并训练了完美的额外助推器。

228
00:16:15,650 --> 00:16:22,110
现在，让我们应用K4交叉加法，以精度指标评估其性能。

229
00:16:22,220 --> 00:16:27,260
实际上，我注意到还有一件事情需要更改，那就是此处可用的DIP名称。

230
00:16:27,350 --> 00:16:32,330
恭喜您，发现我们需要用真实姓名替换此处购买的商品，

231
00:16:32,330 --> 00:16:37,940
问题中的因变量（不是购买而是退出）。

232
00:16:38,090 --> 00:16:42,110
因此，让我们在这里意外地替换购买。

233
00:16:42,500 --> 00:16:43,290
现在我们开始。

234
00:16:43,310 --> 00:16:45,130
现在，一切都会好起来的。

235
00:16:45,320 --> 00:16:51,680
让我们做最后一次检查，将其作为训练集的矩阵，并将其作为转换为测试集的白面包的矩阵

236
00:16:51,680 --> 00:16:53,890
二进制结果0或1。

237
00:16:53,900 --> 00:16:56,450
索引对于因变量是正确的。

238
00:16:56,450 --> 00:16:57,580
一切看起来都很好。

239
00:16:57,680 --> 00:17:05,260
让我们在这里选择整个部分，并获得最终精度。

240
00:17:05,390 --> 00:17:06,540
开始了。

241
00:17:06,890 --> 00:17:13,700
所有步骤都正确且非常快速地执行，最终精度达到88％。

242
00:17:13,700 --> 00:17:19,850
因此，这不仅非常有效，而且我们成功地以和击败了10的精度。

243
00:17:19,880 --> 00:17:26,650
除了这些值以外，我们还可以相信Brousseau的实际准确性为88％。

244
00:17:26,800 --> 00:17:27,850
这样很好。

245
00:17:27,920 --> 00:17:32,360
实际上不仅速度非常快，而且还给我们带来了惊人的准确性。

246
00:17:32,360 --> 00:17:36,230
这是我们在本课程中实施的所有模型中最好的。

247
00:17:36,230 --> 00:17:37,870
因此，这是一项了不起的工作。

248
00:17:38,630 --> 00:17:43,870
现在该说再见了，因为这实际上是本课程的最后部分。

249
00:17:43,910 --> 00:17:48,410
这是一种感觉，因为这是我介绍的机器学习之旅的终点

250
00:17:48,410 --> 00:17:50,760
在本课程的第一个教程中。

251
00:17:50,780 --> 00:17:53,570
是的，那是对的，这就是旅程的终点​​。

252
00:17:53,600 --> 00:17:57,290
但是，我确信这不是最后的机器学习旅程。

253
00:17:57,440 --> 00:17:59,510
这是您的第一次宣教之旅。

254
00:17:59,510 --> 00:18:01,930
我很高兴与您一起进行这次冒险。

255
00:18:01,940 --> 00:18:04,770
我真的很喜欢这次旅行，希望您也是如此。

256
00:18:04,850 --> 00:18:09,930
我会很高兴制作一些新机器，并使课程开始一些新的机器旅程。

257
00:18:10,100 --> 00:18:11,750
所以我希望很快能见到你。

258
00:18:11,750 --> 00:18:13,610
直到那时学习。

