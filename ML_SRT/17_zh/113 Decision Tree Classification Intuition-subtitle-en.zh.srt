1
00:00:01,140 --> 00:00:03,490
您好，欢迎回到机器学习课程。

2
00:00:03,570 --> 00:00:06,100
今天，我们谈论的是决策树。

3
00:00:06,120 --> 00:00:07,920
好吧，让我们开始吧。

4
00:00:08,130 --> 00:00:10,240
您可能听说过购物车一词。

5
00:00:10,320 --> 00:00:17,040
它代表分类和回归树，该术语是两种类型的

6
00:00:17,040 --> 00:00:21,450
我们看到的是钙化树和回归树。

7
00:00:21,450 --> 00:00:26,430
现在的区别是分类树可以帮助您对数据进行分类，因此它们不会

8
00:00:26,430 --> 00:00:31,260
分类变量，例如雄性或雌性苹果或橙色，或不同类型的颜色和变量

9
00:00:31,350 --> 00:00:32,410
这样的。

10
00:00:32,520 --> 00:00:37,740
攻击树旨在帮助您预测可能是实数的结果，因此

11
00:00:38,100 --> 00:00:43,380
例如一个人的薪水或外界的温度等

12
00:00:43,380 --> 00:00:43,410
。

13
00:00:43,410 --> 00:00:48,030
所以这是两种不同的类型，我们将在此讨论分类树

14
00:00:48,030 --> 00:00:49,300
课程部分。

15
00:00:49,350 --> 00:00:54,540
因此，这里我们在二维散点图中有很多点的示例。

16
00:00:54,720 --> 00:00:56,360
现在，决策树如何工作。

17
00:00:56,370 --> 00:01:00,950
因此，它要做的就是将它分成几次迭代切成片，让我们来看一下。

18
00:01:01,110 --> 00:01:02,670
因此，它们将被一分为二。

19
00:01:02,790 --> 00:01:11,690
他们将一分为二，所以一一拆分我们在X处的数据等于60拆分以拆分我们的女儿X1等于

20
00:01:11,700 --> 00:01:13,730
50分裂三。

21
00:01:13,740 --> 00:01:14,490
但是我们。

22
00:01:14,580 --> 00:01:19,660
Exotic或70，然后拆分以将数据拆分为x 2。

23
00:01:19,710 --> 00:01:21,810
此处未显示，大约为20。

24
00:01:21,810 --> 00:01:26,520
这就是决策树的工作方式以及拆分的基础。

25
00:01:26,520 --> 00:01:32,860
那么如何选择这些分割，算法如何知道在何处选择分割。

26
00:01:33,050 --> 00:01:39,300
好吧，基本上，如果您现在看一下，然后以最大程度地增加

27
00:01:39,300 --> 00:01:47,310
每个拆分中的子类别的数量，以便最大化例如，我们想要最大的Reade's

28
00:01:47,310 --> 00:01:52,680
这就是这里的类别，这就是为什么它仍然相同，但是接下来的拆分最大程度地增加了

29
00:01:52,680 --> 00:01:54,430
这里是绿色，这里是红色。

30
00:01:54,450 --> 00:01:56,590
这是一种非常基本的解释方式。

31
00:01:56,610 --> 00:01:59,780
实际上，背景中发生了一些复杂的数学运算。

32
00:01:59,790 --> 00:02:03,120
分裂试图使熵最小化。

33
00:02:03,350 --> 00:02:08,310
因此，它是信息熵，这是一个非常有趣的术语。

34
00:02:08,420 --> 00:02:12,440
我们现在需要花费数小时和数小时才能完成所有这些工作。

35
00:02:12,600 --> 00:02:18,450
因此，如果您想深入了解该算法背后的数学知识，那么您当然可以进行研究

36
00:02:18,450 --> 00:02:18,620
那。

37
00:02:18,630 --> 00:02:23,400
但是对于我们来说，只需要寻找最佳分割或将要使用的算法就足够了

38
00:02:23,400 --> 00:02:30,450
找到最佳分割，这些分割将最大化每个这些新点中不同点的数量

39
00:02:30,480 --> 00:02:35,490
口袋实际上叫做树叶，所以我们有一个初始散点图，然后在最后

40
00:02:35,490 --> 00:02:39,230
得到这些叶子，最后的叶子实际上称为终端租赁。

41
00:02:39,300 --> 00:02:40,360
这样便会发生这种情况。

42
00:02:40,360 --> 00:02:44,480
现在，让我们倒带一点，然后再次执行整个过程。

43
00:02:44,490 --> 00:02:49,410
但是，当我们执行拆分时，我们将开始构建决策树和实际决策

44
00:02:49,410 --> 00:02:49,610
树。

45
00:02:49,620 --> 00:02:50,280
我们来看一下。

46
00:02:50,460 --> 00:02:52,190
因此，这是我们的第一名。

47
00:02:52,470 --> 00:02:56,620
它正在做的是将我们的女儿拆分为60岁的孩子。

48
00:02:56,700 --> 00:03:00,710
因此，现在让我们构建一个决策树来提出这个问题。

49
00:03:00,720 --> 00:03:07,410
X是否大于60或小于60，所以如果X大于6，则X落入一个分支

50
00:03:07,410 --> 00:03:09,590
超过60，它将落入下一个范围。

51
00:03:09,600 --> 00:03:12,160
因此，X-2小于60。

52
00:03:12,260 --> 00:03:13,930
不，不，不。

53
00:03:13,950 --> 00:03:21,300
接下来是将两个仅拆分60 x以上的Dahdah拆分为口头。

54
00:03:21,600 --> 00:03:22,960
因此，让我们看一下。

55
00:03:22,980 --> 00:03:29,100
我们仅处理X-2以上的数据，因此已经结束，现在我们正在检查。

56
00:03:29,100 --> 00:03:30,010
所以我现在回去。

57
00:03:30,010 --> 00:03:34,340
现在，我们检查X1变量是否拆分为50。

58
00:03:34,590 --> 00:03:37,710
因此，这里x x1小于50是或否。

59
00:03:37,710 --> 00:03:43,080
因此，如果在这里和您可以立即看到此拆分，您可以告诉我们是否有绿色

60
00:03:43,110 --> 00:03:43,740
或红色。

61
00:03:43,920 --> 00:03:51,840
因此，如果它较小，那么如果我们已经高于60，然后低于50，则为绿色，我们可以在此处看到

62
00:03:52,560 --> 00:03:55,710
如果我们高于50，那么它是红色的，我们可以在这里看到。

63
00:03:55,710 --> 00:03:59,430
这就是这种分类的工作方式，剩下的就让我们处理。

64
00:03:59,430 --> 00:04:02,520
因此，我们在70处发生了拆分3。

65
00:04:02,730 --> 00:04:07,500
如果您低于70岁，那么您显然会变成红色，否则我们将需要再次拆分

66
00:04:07,500 --> 00:04:07,770
。

67
00:04:07,800 --> 00:04:10,580
因此低于70则为红色。

68
00:04:10,590 --> 00:04:16,200
现在我们要对拆分进行拆分，因为如果拆分拆分大于20，则为绿色。

69
00:04:16,200 --> 00:04:22,740
如果低于20，则为红色；如果高于20，则为绿色；如果低于，则为“否”。

70
00:04:22,740 --> 00:04:23,190
。

71
00:04:23,190 --> 00:04:27,410
因此，有了这些决策树，构造它们的一种好方法就是始终保持。

72
00:04:27,410 --> 00:04:28,890
是是是是

73
00:04:28,920 --> 00:04:35,010
一方面，如果您正在寻找是，就像他们一直在向左寻找

74
00:04:35,010 --> 00:04:36,740
那些是正确的，反之亦然。

75
00:04:36,810 --> 00:04:37,920
只是不要混淆它们。

76
00:04:37,980 --> 00:04:45,680
然后终端叶子将准确地预测剩下的颜色或克劳斯的颜色。

77
00:04:45,930 --> 00:04:49,950
但是同时，即使您没有到达末端leef，因为它是一棵非常简单的树。

78
00:04:49,950 --> 00:04:51,930
树木可能非常长。

79
00:04:52,050 --> 00:04:57,000
因此，有时您甚至可能无法深入研究，因此如果您想对新的观察结果进行分类，

80
00:04:57,000 --> 00:05:03,640
例如，该观察结果落入此处的此部分，然后沿着这条路走，

81
00:05:03,660 --> 00:05:05,000
你到这里去那里。

82
00:05:05,090 --> 00:05:08,830
但是，假设您甚至还没有走到尽头，就在这里。

83
00:05:08,920 --> 00:05:14,220
然后，在这些还没有静止的盒子中，它们在这里仍然有混合物，例如绿色或绿色的混合物。

84
00:05:14,220 --> 00:05:14,870
红色。

85
00:05:14,940 --> 00:05:21,450
那么这里的规则是现实的分类发生，所以我们知道而不是检查

86
00:05:21,450 --> 00:05:25,490
最后一种情况，我们只检查绿色和红色的可能性。

87
00:05:25,500 --> 00:05:30,000
因此，在这里我们看到有更多的绿色和红色，所以如果我们将其保留在此框中，那么我们将

88
00:05:30,000 --> 00:05:36,720
只是说这是一个绿色的点，而如果我们只是将其保留在此保留框中

89
00:05:36,720 --> 00:05:39,970
因此，我们只检查第一个条件，然后将其保留在此处。

90
00:05:40,080 --> 00:05:42,820
然后我们会自动说这是一个红点。

91
00:05:42,840 --> 00:05:48,030
如果我们不去做决策树并检查更多条件，那是使用字典的另一种方式

92
00:05:48,360 --> 00:05:50,520
而不是尽力而为。

93
00:05:50,520 --> 00:05:55,620
您可以随时停下来，然后仅使用概率来预测分类。

94
00:05:55,620 --> 00:05:59,670
另一件事是，不必总是两个变量。

95
00:05:59,670 --> 00:06:05,490
因此，例如在决策树中，就像其他任何机器学习算法一样，您可以拥有多维

96
00:06:05,520 --> 00:06:10,500
数据集有很多不同的列，所以在我们的情况下，我们只有两个，但是您可能有很多

97
00:06:10,500 --> 00:06:16,200
以及很多专栏，然后您可能会在这里提出一些混合问题，这取决于

98
00:06:16,200 --> 00:06:18,290
提出这些问题的算法。

99
00:06:18,450 --> 00:06:23,550
最后，我想谈一谈决策树的历史

100
00:06:23,550 --> 00:06:30,810
已经存在了很长时间，实际上它们已经很老了，以至于最近

101
00:06:30,810 --> 00:06:31,790
开始死亡。

102
00:06:31,800 --> 00:06:38,250
大约23年前仍然有流行音乐，但最近已取代了更复杂的方法

103
00:06:38,250 --> 00:06:38,730
他们。

104
00:06:38,850 --> 00:06:41,750
很快她就如此受欢迎。

105
00:06:42,090 --> 00:06:43,820
并持续了一段时间。

106
00:06:43,830 --> 00:06:52,560
直到最近，他们才有了新的升级，可以这么说，这些升级是其他方法

107
00:06:52,650 --> 00:07:00,180
建立在决策树之上的方法是随机森林梯度提升和其他方法

108
00:07:00,180 --> 00:07:00,570
。

109
00:07:00,570 --> 00:07:05,150
在本课程的这一部分中，我们至少会研究其他方法之一。

110
00:07:05,160 --> 00:07:10,030
关键是决策树虽然是一个非常简单的工具。

111
00:07:10,110 --> 00:07:16,860
它们本身并不是很强大，但是可以用于其他利用其简单性的方法中

112
00:07:17,160 --> 00:07:23,340
并创建一些非常强大的机器学习算法，这些算法甚至被用于执行

113
00:07:23,340 --> 00:07:29,070
面部识别，例如在iPhone上，您将获得面部识别以及一些游戏，例如

114
00:07:29,070 --> 00:07:34,620
作为Kinect，有点像我们，但您无需实际控制即可播放它。

115
00:07:34,620 --> 00:07:40,840
因此，就像您将游戏添加到X-Box中一样，您可以玩游戏，因为我们遵循任何控制权

116
00:07:40,850 --> 00:07:46,430
控制您的手，以便它可以识别您正在移动的手臂和腿以及该方法

117
00:07:46,440 --> 00:07:53,750
微软决定在森林周围使用这种方法，并在Boak决策树中为我们运行它。

118
00:07:53,820 --> 00:07:55,690
所以希望这是今天的教程。

119
00:07:55,750 --> 00:08:01,440
这是一个非常简单的方法，但同时它也位于一些更现代的方法的基础上

120
00:08:01,500 --> 00:08:04,410
以及机器学习中更强大的方法。

121
00:08:04,410 --> 00:08:05,730
期待下次见到您。

122
00:08:05,730 --> 00:08:07,440
直到那时，在您的机器学习中

