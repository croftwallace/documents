1
00:00:00,390 --> 00:00:05,760
您好，欢迎立即阅读此Python教程，我非常激动，因为我们将要处理一个

2
00:00:05,760 --> 00:00:07,440
非常强大的穿越火线。

3
00:00:07,650 --> 00:00:13,470
您将看到它将如何对我们的用户和社交网络进行分类

4
00:00:13,740 --> 00:00:20,490
以及如何捕获那些您认识的反叛用户，这些用户使我们的数据集不是线性的

5
00:00:20,490 --> 00:00:21,390
可分离。

6
00:00:21,420 --> 00:00:22,620
这就是您需要了解的内容。

7
00:00:22,620 --> 00:00:24,480
我们的数据集不是线性可分离的。

8
00:00:24,480 --> 00:00:29,530
这意味着我们不能通过直线将数据集中的两类用户分开。

9
00:00:29,670 --> 00:00:35,340
因此，我们需要功能更强大的分类器来克服用户中非线性的不可分离性

10
00:00:35,340 --> 00:00:36,250
我们的数据集。

11
00:00:36,360 --> 00:00:40,410
我们将以通用的V.M. 我迫不及待地向您展示结果。

12
00:00:40,590 --> 00:00:45,780
因此，这里首先让我们快速设置Reifler，因为我们可以将目录转到File Explorer。

13
00:00:45,810 --> 00:00:50,250
我在这里在桌面上，因此我将转到我的计算机或任何文件夹。

14
00:00:50,580 --> 00:00:54,300
第3部分分类和将内核作为VM。

15
00:00:54,420 --> 00:00:55,350
那是正确的文件夹。

16
00:00:55,350 --> 00:00:58,210
确保将社交网络文件作为C。

17
00:00:58,230 --> 00:01:02,790
如果是这种情况，您只需单击此处的小按钮即可将此文件夹设置为正常工作

18
00:01:02,850 --> 00:01:05,080
目录，您就可以开始了。

19
00:01:05,550 --> 00:01:10,830
好，所以我们现在要做的是采用我们制作的分类模板

20
00:01:11,220 --> 00:01:17,640
在逻辑回归部分的模板中，我们只需要创建分类器，然后创建所有内容

21
00:01:17,640 --> 00:01:23,490
一旦创建了分类器，就可以准备好了，然后我们就可以舒适地坐在椅子上喝一杯好咖啡了，

22
00:01:23,490 --> 00:01:24,690
观看结果。

23
00:01:24,930 --> 00:01:30,110
因此，让我们将所有内容从这里带到最底层。

24
00:01:30,390 --> 00:01:36,070
复制此模板并将其作为VM粘贴到内核中。

25
00:01:36,120 --> 00:01:37,250
好吧。

26
00:01:37,680 --> 00:01:40,530
因此，快速提醒一下，这是数据预处理步骤。

27
00:01:40,530 --> 00:01:43,800
这是我们在此处创建分类器的地方。

28
00:01:43,830 --> 00:01:49,890
我们在这里预测了测试结果，我们制作了混淆矩阵来查看不正确的数量

29
00:01:49,890 --> 00:01:53,280
预测是分类的，您将在测试集中进行。

30
00:01:53,580 --> 00:01:56,220
最后但并非最不重要的一点是我最喜欢的部分。

31
00:01:56,250 --> 00:01:57,930
可视化结果。

32
00:01:57,990 --> 00:02:02,750
因此，我们将可视化两个结果来训练结果和测试结果。

33
00:02:03,020 --> 00:02:03,380
好。

34
00:02:03,390 --> 00:02:05,470
因此，让我们这样做。

35
00:02:05,560 --> 00:02:08,620
我们将从预处理步骤开始。

36
00:02:08,700 --> 00:02:14,850
因此，一旦我们要选择它并按命令控制并按Enter来执行，我们将全部完成

37
00:02:14,850 --> 00:02:15,930
。

38
00:02:15,930 --> 00:02:19,990
一切都很好，您可以看到代码已正确执行。

39
00:02:20,010 --> 00:02:26,970
一切都很好，我们可以去void导出以在此处获得我们的数据集X火车和y火车

40
00:02:26,970 --> 00:02:31,280
训练集，以X检验和Y检验为检验集。

41
00:02:31,320 --> 00:02:32,920
这是可行的。

42
00:02:33,030 --> 00:02:35,060
这是功能矩阵。

43
00:02:35,280 --> 00:02:36,300
一切看起来很棒。

44
00:02:36,300 --> 00:02:40,890
如果您想进行x训练，我们可以快速浏览一下，因为您可以在秤中看到更多。

45
00:02:40,890 --> 00:02:42,150
一切看起来很棒。

46
00:02:42,300 --> 00:02:48,570
我们准备继续下一步，将内核创建为VM分类器。

47
00:02:48,780 --> 00:02:49,990
因此，让我们这样做。

48
00:02:50,340 --> 00:02:56,130
像往常一样，这很容易，我们将使用psychic的知识来导入相应的类

49
00:02:56,130 --> 00:03:01,680
对于内核SVM，您将看到它与Vienne几乎相同，然后我们将创建一个对象

50
00:03:01,680 --> 00:03:07,200
该类的分类将对您的对象进行分类，然后我们将该分类器对象适合我们的

51
00:03:07,200 --> 00:03:08,340
训练集。

52
00:03:08,400 --> 00:03:13,840
因此，让我们执行此操作，我们将删除此命令，该命令正是模板用于创建您的

53
00:03:13,850 --> 00:03:15,000
在这里上课。

54
00:03:15,270 --> 00:03:15,720
好。

55
00:03:15,720 --> 00:03:17,790
现在让我们创建分类器。

56
00:03:18,000 --> 00:03:21,790
因此，我们将转到第二个三分之一并进行导入。

57
00:03:21,990 --> 00:03:31,010
因此，正如我们所看到的，让我们在scaler和Dot中执行此操作。

58
00:03:31,380 --> 00:03:32,460
好。

59
00:03:32,460 --> 00:03:42,500
接下来，我们创建分类器，使分类器等于现在看到的括号。

60
00:03:42,710 --> 00:03:45,160
但是在输入参数之前，让我们看一下。

61
00:03:45,300 --> 00:03:51,810
因此，我们将进行检查，并执行命令和控制以及II

62
00:03:51,810 --> 00:03:53,490
如我们所见进行检查。

63
00:03:53,550 --> 00:03:57,780
所以这里我们有一些信息，但我们最感兴趣的是晴雨表，以了解

64
00:03:57,780 --> 00:03:58,860
我们必须输入。

65
00:03:59,040 --> 00:04:00,290
让我们来看看。

66
00:04:00,390 --> 00:04:00,880
好。

67
00:04:00,960 --> 00:04:03,410
第一个参数是惩罚参数。

68
00:04:03,440 --> 00:04:09,660
我们将在分类部分的末尾讨论这一点，以了解您可以使用

69
00:04:09,660 --> 00:04:13,060
一些惩罚参数正在使用一些正则化技术。

70
00:04:13,200 --> 00:04:18,250
但是现在，让我们关注模型本身并获取基本参数。

71
00:04:18,450 --> 00:04:22,680
说到基本参数，只有一个内核。

72
00:04:22,830 --> 00:04:29,880
记住，当我们使用SVM时，我们使用了线性内核，这使我们的VM分类器成为线性分类器。

73
00:04:29,880 --> 00:04:34,290
记住，我们有一条直线将两类用户分开。

74
00:04:34,290 --> 00:04:39,930
在这里，我们将继续前进，因为我们将使用更复杂的内核，

75
00:04:39,930 --> 00:04:43,870
将是RDF内核，就像高​​斯内核一样。

76
00:04:43,950 --> 00:04:51,120
因此，我们将使用此内核将数据提升到一个新的维度，以便在这个新维度中

77
00:04:51,450 --> 00:04:57,900
我们的数据可以线性分离并由一个超平面分开，然后我们投影回去

78
00:04:57,900 --> 00:05:01,720
分离内核时获取内核的尺寸。

79
00:05:02,490 --> 00:05:06,150
所以是的，我们肯定要输入内核参数。

80
00:05:06,330 --> 00:05:10,250
因此，让我们在此相等。

81
00:05:10,270 --> 00:05:18,430
所以将是R B F上校，顺便说一句，您可以看到默认

82
00:05:18,430 --> 00:05:24,540
当我们谈论V.M. 在Python中，默认情况下它是VM

83
00:05:24,550 --> 00:05:27,400
当我们谈论VM时，它是线性或SVM。

84
00:05:27,500 --> 00:05:33,900
因此，这是默认设置，我们实际上不必指定此名称，但是我们这样做当然是为了提醒我们

85
00:05:33,900 --> 00:05:35,580
高斯核。

86
00:05:35,680 --> 00:05:38,140
因此，接下来是下一个参数。

87
00:05:38,830 --> 00:05:41,250
好，那我们接下来的学位是什么。

88
00:05:41,330 --> 00:05:47,350
好，所以这就是多项式内核函数的阶数，如果我们选择多项式内核，则可以尝试

89
00:05:47,350 --> 00:05:47,900
那你自己。

90
00:05:47,900 --> 00:05:50,450
您还将获得非常有趣的结果。

91
00:05:50,620 --> 00:05:55,340
但是在这里，我们将重点放在最流行的内核上校。

92
00:05:55,440 --> 00:05:58,120
因此，我们不需要为程序做同样的色域。

93
00:05:58,140 --> 00:06:04,480
我们会知道您可以使用gamma参数来改善我们的模型，但是我们不想使用它，因为

94
00:06:04,530 --> 00:06:08,310
再次，我们将看到我们的内核SBM将做得很棒。

95
00:06:08,940 --> 00:06:09,810
就是这样。

96
00:06:09,810 --> 00:06:12,520
因此，我们有了基本参数。

97
00:06:12,510 --> 00:06:16,780
那是内核参数，然后我们将添加一个随机状态参数。

98
00:06:16,770 --> 00:06:20,800
您知道这是我认为的一个参数。

99
00:06:20,810 --> 00:06:22,120
是的，这里是随机状态。

100
00:06:22,120 --> 00:06:28,120
Ranum状态在这里是因为内核作为VM算法基于随机因素。

101
00:06:28,120 --> 00:06:33,090
所以我们想在这里使用随机状态来获得相同的结果，因此我们将输入随机

102
00:06:33,100 --> 00:06:34,210
静态零。

103
00:06:34,690 --> 00:06:35,490
就是这样。

104
00:06:35,500 --> 00:06:37,680
我们的内核作为VM分类器已准备就绪。

105
00:06:37,690 --> 00:06:40,130
做到这一点有多容易。

106
00:06:40,140 --> 00:06:41,540
所以这很酷。

107
00:06:41,860 --> 00:06:47,940
因此，在这一行中，我们将分类器内核创建为VM对象，现在需要将该对象拟合为

108
00:06:47,940 --> 00:06:49,230
我们的训练集。

109
00:06:49,240 --> 00:06:56,860
这就是我们的内核SVM算法可以学习我们训练集的数据，以学习如何对

110
00:06:56,860 --> 00:06:58,480
社交网络的用户。

111
00:06:58,690 --> 00:07:07,990
因此，让我们执行此操作，我们将在此处进行类触发，然后应用此处适合的fit方法

112
00:07:08,880 --> 00:07:10,350
训练集。

113
00:07:10,420 --> 00:07:17,770
因此，我们需要输入我们的训练集X训练和因变量的自变量矩阵

114
00:07:17,760 --> 00:07:20,060
我们训练集的变量向量。

115
00:07:20,110 --> 00:07:23,550
为什么要训练那是我们在python中要做的事情。

116
00:07:23,820 --> 00:07:29,690
并将此学习者内核作为VM类来完成，它将适合火车组。

117
00:07:29,740 --> 00:07:37,530
因此，让我们执行此操作，让我们选择它，然后按Command和Control按Enter执行。

118
00:07:37,620 --> 00:07:42,020
在这里，我们很完美，您可以看到代码正确执行。

119
00:07:42,040 --> 00:07:47,160
我们拥有默认情况下选择的所有默认参数以及此分类器，因为我们

120
00:07:47,160 --> 00:07:49,750
没有指定它们，但这很好。

121
00:07:49,750 --> 00:07:51,110
一切都很好。

122
00:07:51,430 --> 00:07:56,920
因此，既然我们的分类器已经准备就绪，就可以预测测试结果了。

123
00:07:57,030 --> 00:08:01,720
因此，让我们执行此操作，然后按命令控制百分比执行。

124
00:08:01,720 --> 00:08:02,410
开始了。

125
00:08:02,440 --> 00:08:06,560
为什么要准备Pretis，让我们看一下，否则我们将探索您的pred。

126
00:08:06,780 --> 00:08:09,460
这是我们的预测向量。

127
00:08:09,730 --> 00:08:13,160
因此，让我们将其与Whitehead的乐趣进行比较。

128
00:08:13,150 --> 00:08:15,480
这就是为什么测试，这就是为什么弗雷德。

129
00:08:15,690 --> 00:08:16,970
这就是事实。

130
00:08:17,070 --> 00:08:18,670
这是预测。

131
00:08:18,660 --> 00:08:19,480
让我们来看一下。

132
00:08:19,480 --> 00:08:25,200
我们已经可以看到，您所知道的一切看起来都正确预测了，这里有所有零

133
00:08:25,380 --> 00:08:26,490
零到六。

134
00:08:26,500 --> 00:08:27,150
同样在这里。

135
00:08:27,150 --> 00:08:33,900
从零到一，这里的六到七，这意味着该社交网络的七个用户是

136
00:08:33,930 --> 00:08:40,230
被测试购买了SUV，我对您的预测是该用户会购买此SUV。

137
00:08:40,240 --> 00:08:47,550
用户可以在SUV中使用它，或对您预测的用户未购买SUV进行分类

138
00:08:47,760 --> 00:08:48,510
再次伟大。

139
00:08:48,510 --> 00:08:50,140
但是，我们现在不庆祝胜利。

140
00:08:50,130 --> 00:08:52,500
我们有一个错误的预测。

141
00:08:52,530 --> 00:08:54,780
那么没有模型是完美的。

142
00:08:54,780 --> 00:09:00,180
机器学习中实际上有一句话说所有模型都是错误的，但有些模型是有用的

143
00:09:00,180 --> 00:09:00,440
。

144
00:09:00,580 --> 00:09:01,660
虽然这里就是这种情况。

145
00:09:01,650 --> 00:09:08,400
此模型绝对有用，因为您可以知道将用户分为两类以帮助

146
00:09:08,460 --> 00:09:12,640
市场营销活动的目标是用户销售他们的产品。

147
00:09:12,630 --> 00:09:17,550
因此，此模型绝对有用，但当然是错误的，因为总会有一些正确的东西

148
00:09:17,560 --> 00:09:23,460
预测，您会发现一些道德观念，对饮料组没有不正确的预测，但要等待

149
00:09:23,520 --> 00:09:25,290
当您在测试仪上进行测试时。

150
00:09:25,380 --> 00:09:31,400
您会发现对新观测值总是有一些不正确的预测，从而使它们成为错误的。

151
00:09:31,600 --> 00:09:33,470
就是这样。

152
00:09:33,490 --> 00:09:37,300
现在您可以看到我们可以检查最后一个只是为了好玩。

153
00:09:37,440 --> 00:09:40,090
好1 1 1 1 1 1 0 0

154
00:09:40,200 --> 00:09:46,980
可以在九十五分做出一个错误的预测，但是在这里和这里都可以纠正。

155
00:09:46,990 --> 00:09:48,080
看起来不错。

156
00:09:48,100 --> 00:09:53,430
我等不及要找出混淆矩阵上不正确的预测数。

157
00:09:53,430 --> 00:09:55,130
现在就开始做吧。

158
00:09:55,140 --> 00:10:02,290
记住，我们有11个直接预测与Logistic回归，将作为比较的参考

159
00:10:02,290 --> 00:10:02,620
。

160
00:10:02,620 --> 00:10:08,160
现在，让我们来看一下M内核SVM的错误预测数。

161
00:10:08,580 --> 00:10:14,310
所以我刚刚选择了这个，我们现在要找出Commander控制百分比执行

162
00:10:14,410 --> 00:10:15,130
。

163
00:10:15,340 --> 00:10:16,470
在这里。

164
00:10:16,500 --> 00:10:17,870
我们已经可以看到了吗？

165
00:10:18,120 --> 00:10:19,320
我们可以。

166
00:10:19,330 --> 00:10:22,380
实际上在这里，我们这里有四个和三个。

167
00:10:22,380 --> 00:10:25,600
因此，这意味着我们有七个错误的预测。

168
00:10:25,750 --> 00:10:26,700
大。

169
00:10:26,740 --> 00:10:35,240
因此，比逻辑回归更好的是，您肯定知道测试集上的七个错误预测。

170
00:10:35,360 --> 00:10:38,350
那肯定是一种改进的模型，让我们单击OK。

171
00:10:38,540 --> 00:10:43,840
现在您可以将水煮沸并煮咖啡，或者我不知道您是如何煮咖啡的，但是您可以

172
00:10:43,840 --> 00:10:51,160
得到您的咖啡，因为现在我们只需要做的就是选择所有这些内容并可视化趋势

173
00:10:51,160 --> 00:10:52,310
结果。

174
00:10:52,330 --> 00:10:53,940
你准备好了吗

175
00:10:54,040 --> 00:10:55,150
我们开工吧。

176
00:10:55,150 --> 00:11:00,970
您将看到我们的上校SVM转移如何管理我们的类别以及他如何管理

177
00:11:00,970 --> 00:11:04,340
来吸引那些用户并将其放在正确的类别中。

178
00:11:04,390 --> 00:11:08,690
那些使我们的数据集非线性可分离的用户。

179
00:11:08,980 --> 00:11:13,120
因此，让我们来看看指挥官控制我们并执行。

180
00:11:13,240 --> 00:11:14,010
在这里。

181
00:11:14,020 --> 00:11:15,000
哇好漂亮。

182
00:11:15,010 --> 00:11:15,930
非常漂亮。

183
00:11:16,090 --> 00:11:17,510
看看这个

184
00:11:18,170 --> 00:11:18,420
好。

185
00:11:18,430 --> 00:11:23,080
因此，对于那些首次发现该图并迅速提醒您的人

186
00:11:23,080 --> 00:11:24,030
即将。

187
00:11:24,040 --> 00:11:29,420
因此，这里的所有点都是观察点或这些火车的集合是我们火车的用户

188
00:11:29,450 --> 00:11:29,980
。

189
00:11:30,280 --> 00:11:31,760
这些都是真实的结果。

190
00:11:31,780 --> 00:11:33,760
因此，所有观点都代表了事实。

191
00:11:33,760 --> 00:11:34,730
到底发生了什么。

192
00:11:34,870 --> 00:11:40,080
那些红色用户没有购买SUV，而那些绿色用户则拥有绿色积分。

193
00:11:40,150 --> 00:11:41,300
但是，你可以的。

194
00:11:41,470 --> 00:11:44,910
然后我们得到的区域就是预测区域。

195
00:11:45,100 --> 00:11:52,160
因此，这里的红色区域是分类预测用户不会购买SUV的区域。

196
00:11:52,210 --> 00:11:58,420
因此，这意味着分类人员预计该地区的所有用户都不会购买SUV

197
00:11:58,900 --> 00:12:04,420
绿色区域是分类预测用户将购买Wii的区域

198
00:12:04,420 --> 00:12:10,010
根据交火法，预计该地区的任何用户都将购买SUV。

199
00:12:10,480 --> 00:12:14,920
因此，重要的是要了解要点是事实，而区域是预测

200
00:12:15,460 --> 00:12:21,340
接下来要了解的是，此分隔符将绿色的两个区域分开

201
00:12:21,340 --> 00:12:25,690
区域和红色区域称为预测边界。

202
00:12:25,930 --> 00:12:31,960
所以在进行逻辑回归和线性核支持向量机之前，我们有一条直线

203
00:12:32,290 --> 00:12:33,790
这两个类别。

204
00:12:33,800 --> 00:12:40,000
但是在这里，因为我们跳到更高的层次，所以我们建立了一个非线性分类器，我们可以在这里清楚地看到

205
00:12:40,000 --> 00:12:46,730
因为边界不再是一条直线，而是一条曲线，如您所见，

206
00:12:46,730 --> 00:12:47,720
数据。

207
00:12:48,100 --> 00:12:54,880
因此，现在肯定要归功于此分类器，该低薪的老用户现在已经成为

208
00:12:54,880 --> 00:13:01,460
归类在正确的区域中，并且对于这些年轻用户来说也是如此，他们的估计薪水很高。

209
00:13:01,480 --> 00:13:03,450
他们现在已经安抚了。

210
00:13:03,490 --> 00:13:08,000
当然，我们有一些不正确的预测，例如此处或此处。

211
00:13:08,150 --> 00:13:14,770
那是因为内核SVM不会像没有过度拟合那样进行分类

212
00:13:14,770 --> 00:13:18,740
一路走到这里并抓住所有要点。

213
00:13:18,790 --> 00:13:21,490
那是因为它是一个明智的非通用的窄播变量。

214
00:13:21,490 --> 00:13:27,070
例如，如果我们四处游玩并学习具有很多阶数的多项式类，您就会知道

215
00:13:27,070 --> 00:13:33,080
如果选择多项式内核，请选择度数和内核SVM的参数

216
00:13:33,150 --> 00:13:37,460
您将看到这一天将过时，因为它将尝试赶上。

217
00:13:37,460 --> 00:13:43,140
此外，这里的绿点可以通过在此处绘制多条曲线来尝试。

218
00:13:43,230 --> 00:13:50,320
实际上可能很有趣，但内核为V.M. 是一个明智的分类器，实际上这是一个

219
00:13:50,320 --> 00:13:55,870
在这种情况下可以找到最适合的班级，因为您不希望它过分适合您

220
00:13:55,870 --> 00:13:56,460
数据因为。

221
00:13:56,470 --> 00:13:57,070
好。

222
00:13:57,160 --> 00:14:01,480
火车上有这个绿点，但它告诉您我们将拥有相同的绿点

223
00:14:01,480 --> 00:14:03,320
测试集中的绿点。

224
00:14:03,520 --> 00:14:07,870
最后但并非最不重要的一点是，我要说的是，这条曲线将

225
00:14:07,940 --> 00:14:14,650
达特（Dator）但不要忘了这样做是一项了不起的工作，因为如果您走到幕后

226
00:14:14,890 --> 00:14:22,630
您可以看到内核SVM跳到了更高的维度，您知道找到了更高的维度空间

227
00:14:22,630 --> 00:14:29,710
它设法以线性分离的方式分布我们的数据

228
00:14:29,710 --> 00:14:31,070
高维空间。

229
00:14:31,120 --> 00:14:36,100
这就是Kirill与您谈论的使用此映射功能的过程，您知道此映射功能

230
00:14:36,100 --> 00:14:43,120
那种将原来在二维空间中的数据带入三维

231
00:14:43,120 --> 00:14:49,360
现在可以线性分离数据的空间比找到线性边界超平面的空间要大

232
00:14:49,360 --> 00:14:56,470
高维空间，即3D维空间，然后投影回二维空间

233
00:14:56,470 --> 00:15:02,700
最终获得此曲线的空间，该曲线在此很好地分开。

234
00:15:03,150 --> 00:15:03,490
好。

235
00:15:03,520 --> 00:15:05,230
这就是训练集。

236
00:15:05,240 --> 00:15:07,020
现在让我们看一下测试集。

237
00:15:07,040 --> 00:15:08,430
所以我要结束那个。

238
00:15:09,040 --> 00:15:15,080
然后非常然后非常有效地选择此命令程序控制我们并执行。

239
00:15:15,310 --> 00:15:17,720
这是测试结果。

240
00:15:17,860 --> 00:15:19,450
因此，让我们看一下。

241
00:15:19,450 --> 00:15:20,470
我们到了。

242
00:15:20,750 --> 00:15:22,210
哇

243
00:15:22,210 --> 00:15:23,500
是的，那也很棒。

244
00:15:23,500 --> 00:15:25,370
这也是很好的结果。

245
00:15:25,390 --> 00:15:31,570
我们可以清楚地看到，几乎我们所有的新观测值都被归为正确的类别。

246
00:15:31,600 --> 00:15:36,730
上校为V.M. 做出了很多预测，预测用户不会购买SUV

247
00:15:36,730 --> 00:15:39,970
在这个红色区域，他们将在这个绿色区域购买SUV。

248
00:15:39,970 --> 00:15:41,000
就是这样。

249
00:15:41,200 --> 00:15:42,850
当然，我们有错误的预测。

250
00:15:42,850 --> 00:15:47,500
请记住，我们在混淆矩阵中计算了七个不正确的预测。

251
00:15:47,530 --> 00:15:49,500
因此，在这里我们可以计算它们的速度。

252
00:15:49,510 --> 00:15:50,450
我们这里有一个。

253
00:15:50,560 --> 00:15:54,310
一二三四五。

254
00:15:54,310 --> 00:16:00,890
您知道在绿色预测区域中的其他这些红色真相观察点。

255
00:16:01,090 --> 00:16:02,680
所以这些是不正确的。

256
00:16:02,710 --> 00:16:08,410
所以是四个和五个，然后六个，最后是七个。

257
00:16:08,440 --> 00:16:11,890
是的，我们有七个错误的预测。

258
00:16:11,930 --> 00:16:12,510
那也行。

259
00:16:12,510 --> 00:16:16,740
我是说，我在训练中告诉您的那部分完全没问题。

260
00:16:16,750 --> 00:16:22,930
我们不希望没有合适的分类器来对两个数据进行很好的分类，如果我们

261
00:16:22,930 --> 00:16:24,320
有一些间接的补充。

262
00:16:24,700 --> 00:16:27,120
营销活动不是到最近的美元。

263
00:16:27,220 --> 00:16:28,740
因此，这是一个很好的分类器。

264
00:16:28,750 --> 00:16:35,620
现在，由于分类良好，因此我们可以用新的观察结果确认这一点。

265
00:16:36,100 --> 00:16:38,110
好，就是这样。

266
00:16:38,110 --> 00:16:43,780
我强烈建议您使用这些参数，以便您已经可以尝试改善自己

267
00:16:43,780 --> 00:16:44,620
模型。

268
00:16:44,850 --> 00:16:49,780
现在，我迫不及待地向您展示下一个分类器，它们将是决策树气化器和

269
00:16:49,780 --> 00:16:55,670
随机森林气化炉，您将看到我们将发现全新的结果风格

270
00:16:55,670 --> 00:16:55,690
。

271
00:16:55,690 --> 00:16:58,720
不再是这种曲线了。

272
00:16:58,840 --> 00:17:00,050
这将是另外一回事。

273
00:17:00,100 --> 00:17:05,050
我不会告诉您它是什么，而让您等待惊喜，但这对机器来说真是太酷了

274
00:17:05,050 --> 00:17:09,120
学习是道德和图形结果存在多样性。

275
00:17:09,340 --> 00:17:13,050
我们总是发现一些新事物，并且对此没有限制。

276
00:17:13,090 --> 00:17:19,630
我的意思是机器学习的科学家和研究人员一直在思考机械模型的新思想

277
00:17:19,630 --> 00:17:24,970
因此，您将始终找到有关机器学习模型的一些新技巧和一些创新思想

278
00:17:24,970 --> 00:17:25,230
。

279
00:17:25,240 --> 00:17:29,220
这就是传教士的奇妙之处，没有终点。

280
00:17:29,260 --> 00:17:33,660
就像宇宙膨胀到无限的极限。

281
00:17:33,730 --> 00:17:34,090
好。

282
00:17:34,090 --> 00:17:36,320
因此，非常感谢您观看本教程。

283
00:17:36,400 --> 00:17:38,830
希望您喜欢上校SBM分类器。

284
00:17:39,010 --> 00:17:42,700
现在，等待查看决策树并运行其气化炉。

285
00:17:42,820 --> 00:17:45,820
这将是我们下一节的主题。

286
00:17:46,090 --> 00:17:47,710
感谢您观看本教程。

287
00:17:47,710 --> 00:17:49,780
我希望在接下来的一个小时内能见到您。

288
00:17:49,840 --> 00:17:52,100
在那之前享受机器学习

