1
00:00:00,510 --> 00:00:06,360
您好，欢迎来到Tauriel，今天我们将实现支持向量机或更多

2
00:00:06,360 --> 00:00:08,220
通常称为SVM。

3
00:00:08,220 --> 00:00:13,080
在python上实现它之后，我们将在我们上执行它。

4
00:00:13,140 --> 00:00:20,580
因此，让我们快速将文件夹设置为工作目录机械简易部分3分类SBM。

5
00:00:20,640 --> 00:00:25,800
在这里，我们确保您拥有此社交网络并启动文件，然后可以单击

6
00:00:25,800 --> 00:00:29,250
单击此处的更多按钮，将文件夹设置为工作目录。

7
00:00:29,250 --> 00:00:30,120
都好。

8
00:00:30,180 --> 00:00:35,010
现在，我们将转到在逻辑回归部分中创建的分类模板

9
00:00:35,020 --> 00:00:35,250
。

10
00:00:35,490 --> 00:00:46,360
因此，让我们将所有内容从这里带到最终副本，并在我们的SVM文件中进行处理。

11
00:00:46,410 --> 00:00:46,930
开始了。

12
00:00:46,960 --> 00:00:50,160
现在，我们需要更改的东西很少。

13
00:00:50,190 --> 00:00:56,250
所以首先我们需要在这里创建或分类，这当然是SVM的转移，然后

14
00:00:56,250 --> 00:01:01,380
我们只需要在这里更改标题即可搜索搜索结果。

15
00:01:01,380 --> 00:01:08,680
因此，我们将指定到目前为止，这里的测试集的SVM分类器是否相同。

16
00:01:08,910 --> 00:01:13,210
现在让我们回到创建分类器的位置。

17
00:01:13,540 --> 00:01:13,860
好。

18
00:01:13,860 --> 00:01:20,070
因此，像往常一样，我们将导入一个库，这次库将是最受欢迎的

19
00:01:20,070 --> 00:01:23,750
用于SVM的库是库。

20
00:01:23,750 --> 00:01:25,330
变成了71岁。

21
00:01:25,440 --> 00:01:31,140
因此，实际上有两个非常流行的图书馆广告，每个人和当前的实验室都很密集。

22
00:01:31,230 --> 00:01:32,990
实际上我都尝试过它们。

23
00:01:33,000 --> 00:01:34,570
我们有10种相同的结果。

24
00:01:34,680 --> 00:01:36,880
因此，您可以尝试另一个到她的实验室。

25
00:01:36,930 --> 00:01:38,810
它实际上几乎是相同的参数。

26
00:01:38,820 --> 00:01:45,370
您只需要按F1键即可查看K SVM上的参数，因为当前函数

27
00:01:45,420 --> 00:01:48,080
是k SVM，您可以签出。

28
00:01:48,090 --> 00:01:51,390
但是现在我们将对每个71进行处理。

29
00:01:51,420 --> 00:01:54,580
因此，让我们先做一下，看看这里的软件包。

30
00:01:54,690 --> 00:02:01,050
看看您是否拥有Eton 71库，因为我以前经常做很多工作

31
00:02:01,050 --> 00:02:05,010
有时，但是如果您是首次安装我们的产品，则可能没有。

32
00:02:05,160 --> 00:02:08,280
因此，我将在此处编写此命令。

33
00:02:08,280 --> 00:02:13,720
对于需要安装它的人，请安装软件包。

34
00:02:13,770 --> 00:02:14,550
这里是。

35
00:02:14,550 --> 00:02:20,360
然后在括号中用双引号将库的名称命名为这种方式。

36
00:02:20,580 --> 00:02:23,100
十七十71。

37
00:02:23,130 --> 00:02:23,490
好吧。

38
00:02:23,490 --> 00:02:28,440
因此，如果选择此行的出口，它将安装该软件包。

39
00:02:28,440 --> 00:02:31,740
我不会这样做，因为它已经安装了，但是会这样做。

40
00:02:31,770 --> 00:02:32,460
我答应你。

41
00:02:32,710 --> 00:02:35,920
因此，我将在此处发表评论。

42
00:02:36,120 --> 00:02:39,330
现在让我们开始创建或分类。

43
00:02:39,330 --> 00:02:48,660
首先，我们需要知道该行库已变为71，并且如果您想进行一些自动化

44
00:02:48,660 --> 00:02:54,110
脚本会在此处自动选择您的媒体库，因为不会一直选择该脚本。

45
00:02:54,120 --> 00:02:56,050
因此，您要确保选择它。

46
00:02:56,280 --> 00:02:59,320
现在，我们将创建我们的SVM类。

47
00:02:59,430 --> 00:03:04,230
因此，与往常一样，我们将分类器称为等号。

48
00:03:04,680 --> 00:03:08,760
然后在这里我们将非常简单地将函数用作V。

49
00:03:08,770 --> 00:03:10,820
因此，让我们将此作为V。

50
00:03:10,830 --> 00:03:17,870
然后按F1键查看参数F1，这是参数的参数。

51
00:03:17,880 --> 00:03:19,880
因此，第一个参数是公式。

52
00:03:20,160 --> 00:03:21,230
因此，让我们添加它。

53
00:03:21,230 --> 00:03:28,830
因此，与往常一样，公式是使用Attila表示的因变量

54
00:03:28,830 --> 00:03:31,920
您用点表示的独立变量。

55
00:03:31,920 --> 00:03:34,750
因此，这里我们将添加公式等于。

56
00:03:34,800 --> 00:03:42,630
因此，首先我们使用我们已购买的因变量，现在我们习惯了

57
00:03:42,720 --> 00:03:49,160
然后是一个点，这表示我们正在获取数据集的所有自变量。

58
00:03:49,430 --> 00:03:50,030
好。

59
00:03:50,130 --> 00:03:55,850
现在来看下一个参数，然后下一个参数是数据。

60
00:03:55,860 --> 00:04:01,230
因此，当然是要在其上训练您的分类器的数据

61
00:04:01,230 --> 00:04:05,230
供您学习数据以进行将来的分类。

62
00:04:05,360 --> 00:04:08,960
当然，这些数据是我们的训练集。

63
00:04:09,420 --> 00:04:11,890
好吧，完美来。

64
00:04:12,030 --> 00:04:18,100
让我们转到下一个参数，下一个参数是X和Y，但是我们并不在乎。

65
00:04:18,270 --> 00:04:26,370
但是我们确实关心这里的时间和内核，因为您注意到的类型有两个

66
00:04:26,370 --> 00:04:33,090
V.M.的类型 有V和分类，还有我们回归的

67
00:04:33,090 --> 00:04:38,790
一种与支持向量机算法相同的算法，但是有一种用于分类，一种用于回归

68
00:04:38,790 --> 00:04:38,820
。

69
00:04:38,820 --> 00:04:45,830
因此，在此使用此参数类型，您可以选择分类，然后选择分类的默认类型

70
00:04:45,830 --> 00:04:47,490
正在查看分类。

71
00:04:47,490 --> 00:04:50,220
这就是我们要选择的类型。

72
00:04:50,270 --> 00:05:02,050
因此，让我们在这里添加类型等于C的分类是正确的，现在最后一个非常重要的参数是

73
00:05:02,050 --> 00:05:03,200
内核。

74
00:05:03,250 --> 00:05:09,580
所以我们从最简单的基础开始，例如Linnie are V M

75
00:05:09,620 --> 00:05:10,430
作为V.M.

76
00:05:10,450 --> 00:05:15,610
所以在这里，我们将选择线性内核，然后选择线性内核

77
00:05:15,610 --> 00:05:19,400
具有一些高斯内核的更复杂的SVM。

78
00:05:19,450 --> 00:05:24,070
因此，我将让您了解实际上可以尝试自己练习并玩耍的惊喜

79
00:05:24,070 --> 00:05:28,450
如果您想了解不同的内核，请提前完成本课程。

80
00:05:28,450 --> 00:05:33,280
事先做一个课程，然后观看课程来比较

81
00:05:33,370 --> 00:05:35,270
处理课程内容。

82
00:05:35,380 --> 00:05:42,650
但是现在我们要选择一个线性核，这样他就可以将核数提高到Linna。

83
00:05:42,850 --> 00:05:44,440
好的，仅此而已。

84
00:05:44,440 --> 00:05:49,290
有了这四个参数，当VM分类器准备就绪时，我们的CROSSFIRE就准备就绪了。

85
00:05:49,570 --> 00:05:54,490
因此，我们将在代码的各个部分中选择不同的步骤。

86
00:05:54,520 --> 00:05:58,600
因此，让我们首先选择该数据进行预处理。

87
00:05:58,960 --> 00:05:59,950
这里是。

88
00:06:00,230 --> 00:06:02,710
好的，太完美了，我们有一个数据集。

89
00:06:02,710 --> 00:06:05,940
这是火车和我们的测试仪。

90
00:06:06,070 --> 00:06:12,520
因此，数据集包含400个观测值，这些观测值是有关社交网络用户的信息，其中包括

91
00:06:12,520 --> 00:06:16,240
在此购买的此列中的年龄和估计薪水会告诉您。

92
00:06:16,270 --> 00:06:24,250
如果是或否，则当用户收到汽车公司在社交网络上投放的广告时，用户购买了汽车

93
00:06:24,550 --> 00:06:26,730
用于营销活动。

94
00:06:26,880 --> 00:06:32,560
因此，像往常一样，我们将尝试将用户分为两类。

95
00:06:32,650 --> 00:06:35,230
是的，他们买了车，不，他们不买车。

96
00:06:35,530 --> 00:06:38,030
好的，让我们回到分类器。

97
00:06:38,290 --> 00:06:43,010
至此，预处理已经完成，现在我们将创建一个分类器。

98
00:06:43,090 --> 00:06:48,120
一切都会顺利进行，因为已经选择了我，所以我必须选择它。

99
00:06:48,430 --> 00:06:52,140
因此，让我们按下命令控件并按Enter即可执行。

100
00:06:52,150 --> 00:06:55,340
这就是造成铸造失败的原因。

101
00:06:55,380 --> 00:06:56,240
都好。

102
00:06:56,590 --> 00:06:57,010
好。

103
00:06:57,000 --> 00:07:02,470
现在我们可以对新观测值做出一些预测，以测试该观测值，因此让我们

104
00:07:02,470 --> 00:07:03,490
做吧。

105
00:07:03,500 --> 00:07:05,320
白面包白面包。

106
00:07:05,320 --> 00:07:07,810
好吧，让我们来看看为什么要面包。

107
00:07:07,900 --> 00:07:10,420
为什么选择KRAD。

108
00:07:11,230 --> 00:07:16,470
因此，这些都是针对测试集的每个用户的测试观测值的所有预测或

109
00:07:16,470 --> 00:07:19,700
分类或预测用户是否购买了SUV。

110
00:07:19,990 --> 00:07:24,910
因此，让我们将预测与集合中包含的真相进行比较。

111
00:07:25,180 --> 00:07:29,960
因此，本专栏讲述了有关用户是否同意的事实。

112
00:07:29,970 --> 00:07:35,890
SUV，这是预测，因为VM转移预测每个用户是否购买

113
00:07:36,050 --> 00:07:37,270
一个或SUV。

114
00:07:37,260 --> 00:07:39,660
因此，让我们看一下例如Web。

115
00:07:39,820 --> 00:07:40,030
好。

116
00:07:40,030 --> 00:07:43,560
因此，所有这些最初的家伙都被预测不会购买SUV。

117
00:07:43,570 --> 00:07:46,070
直到103为止，我们全为零。

118
00:07:46,420 --> 00:07:53,830
但是在这里您可以看到，实际上有一些人购买了SUV，实际上是18 19 20

119
00:07:53,830 --> 00:07:56,970
22有分类的预测，他们没有买。

120
00:07:57,160 --> 00:07:59,540
事实是他们实际上买了它。

121
00:07:59,560 --> 00:08:01,510
因此，这是不正确的预测。

122
00:08:01,500 --> 00:08:07,700
但是，通过查看混淆矩阵，我们可以更有效地查看错误的预测

123
00:08:07,700 --> 00:08:08,000
。

124
00:08:08,050 --> 00:08:11,240
因此，让我们在这里选择这一行并执行。

125
00:08:11,350 --> 00:08:15,530
现在让我们找出错误预测的真实数量。

126
00:08:15,790 --> 00:08:18,660
所以我们停止C.M. 在理事会中，然后按Enter。

127
00:08:19,120 --> 00:08:23,630
哇，这实际上是大量不正确的预测。

128
00:08:23,830 --> 00:08:28,600
顺便说一下，我们不希望获得与Python相同的结果，因为我们在其中存在一些随机因素

129
00:08:28,600 --> 00:08:32,690
模型，此处我们未指定座位，因此您实际上可能会有一些不同的结果。

130
00:08:32,710 --> 00:08:34,930
但是这里的想法。

131
00:08:34,960 --> 00:08:38,740
好了，现在让我们看一下图表，看看它的运行情况。

132
00:08:38,760 --> 00:08:44,980
因此，对于那些实际上没有看过有关V.M的Python教程的人。 一个好的锻炼是

133
00:08:45,040 --> 00:08:47,110
尝试猜测会发生什么。

134
00:08:47,110 --> 00:08:51,930
那就是我们将要成为预测边界的预测区域

135
00:08:51,940 --> 00:08:52,240
。

136
00:08:52,240 --> 00:08:54,730
你猜你会看到什么。

137
00:08:54,750 --> 00:08:56,230
因此，我将让您考虑一下。

138
00:08:56,230 --> 00:08:59,370
您可以传递视频，现在我要告诉您。

139
00:08:59,380 --> 00:09:07,180
如您所见，我们选择了线性核，这意味着被归类为线性分类器

140
00:09:07,190 --> 00:09:07,370
。

141
00:09:07,540 --> 00:09:14,690
因此，正如我在Python的逻辑回归教程中所解释的那样，它在

142
00:09:14,680 --> 00:09:17,620
2D维空间是一条直线。

143
00:09:17,620 --> 00:09:23,010
所以在这里我告诉你现在我们要获得一条直线，不要让任何失望

144
00:09:23,290 --> 00:09:28,890
因为我知道我们用拐杖证明了我们的模型，并且在获得良好的预测边界之前，

145
00:09:29,020 --> 00:09:35,760
切，您知道右下角的用户实际上是SUV，但不正确

146
00:09:35,800 --> 00:09:38,160
逻辑回归的捕食。

147
00:09:38,160 --> 00:09:43,830
好吧，这对线性化虚拟机也将是间接保护，因为它实际上是

148
00:09:43,960 --> 00:09:45,520
线性分类器。

149
00:09:45,660 --> 00:09:48,000
因此，让我们现在来看一下结果。

150
00:09:48,010 --> 00:09:51,940
选择此选项并按命令控制按Enter键执行

151
00:09:54,040 --> 00:09:56,180
。

152
00:09:56,590 --> 00:09:58,670
这是结果。

153
00:09:58,990 --> 00:09:59,290
好。

154
00:09:59,290 --> 00:10:01,510
如您所见，这正是我刚刚告诉您的。

155
00:10:01,510 --> 00:10:08,830
您知道这里的那些薪水较低，年龄较大的用户实际上是在实际购买SUV的

156
00:10:09,250 --> 00:10:15,490
因为这些点是绿色的，所以这些点是真实的观察结果，但它们落入红色区域

157
00:10:15,520 --> 00:10:21,390
这里是因为Lynbrook证明者是一条直线，所以您不知道会做出某种曲线

158
00:10:21,390 --> 00:10:28,840
在这里将所有红色点捕获到正确的位置，因此它捕获了一些绿色点

159
00:10:28,840 --> 00:10:30,410
他们在红色区域。

160
00:10:30,420 --> 00:10:33,520
是的，这与逻辑回归完全相同。

161
00:10:33,660 --> 00:10:39,860
抱歉让您失望，但不要担心，在下一节中，我们将介绍一种新型的Caspari

162
00:10:39,880 --> 00:10:44,510
作为VM的内核将具有与Lindauer内核不同的颜色。

163
00:10:44,560 --> 00:10:49,510
这将是一个高斯内核，或者甚至我们可以尝试更多的内核，在那里您可以练习

164
00:10:49,510 --> 00:10:50,190
你自己

165
00:10:50,200 --> 00:10:52,040
对您来说这可能是一个很好的做法。

166
00:10:52,260 --> 00:10:57,810
让我们听一听，它是一个线性证明子，所以基本上它与逻辑回归相同。

167
00:10:58,270 --> 00:11:04,390
如果我们看一下测试，结果将是相同的，那就让我们看一下。

168
00:11:07,030 --> 00:11:08,400
这是测试集。

169
00:11:08,530 --> 00:11:08,850
好。

170
00:11:08,860 --> 00:11:17,040
是的，在同一件事上，我们有一些年龄较高且估计工资较低的用户，但是是U-V，但是

171
00:11:17,200 --> 00:11:22,990
再次落入红色区域，因为我们班级非常直线，那是你所能做到的最好

172
00:11:22,990 --> 00:11:28,360
确实要对这两点进行分类并将它们放入相应的类别。

173
00:11:28,620 --> 00:11:32,360
所以这里有一些不正确的预测，这里有一些不正确的补充。

174
00:11:32,500 --> 00:11:36,550
而且，如果您愿意，您可以计算不正确操作的数量，即其中的绿点数量

175
00:11:36,550 --> 00:11:42,330
红色区域加上绿色区域中红色点的数量，您将计算出不正确的数量

176
00:11:42,340 --> 00:11:48,260
我们在混淆矩阵中发现了20个错误的预测。

177
00:11:48,310 --> 00:11:54,170
取得这个结果真是太好了，因为这给了我们改进分类器的动力

178
00:11:54,260 --> 00:11:55,600
改善我们的模型。

179
00:11:55,720 --> 00:11:57,690
这就是我们在下一节中要做的。

180
00:11:57,700 --> 00:12:03,610
因此，我期待在下一节中与您见面，并向您展示我们如何能够大幅改善我们的

181
00:12:03,610 --> 00:12:05,140
分类模型。

182
00:12:05,160 --> 00:12:07,260
因此，我希望向您展示新的水平。

183
00:12:07,270 --> 00:12:09,170
在那之前享受机器学习

