
****************************关于神经网络、机器学习等的资料收录及自己理解**********************************

-1- 训练神经网络解决现实中的问题时之间的联系
	.达到最终目的前 统计一些影响最终目的先决条件-对应神经网络的特征
	.根据实践得出的结果 作为最终确定的结果，然后网络依据一些先决条件(对应神经元)和中间隐藏层(参数或其他调整)得出的一个关于结果的一个概率值，这个值越大说明此网络作出的判断 越贴近最终确定的结果
	.神经元和隐藏层之间连接(突触)，起思考的作用，影响最终的走向和最终偏离最终确定结果的概率

-2-	.神经网络将得出 0 到 1 之间的概率，与真值 1 对比，表示准确率
	.神经网络通过概率和试错学习方法，渐进式地改进下一次预测的结果

-3- 准确率提高的步骤
    前馈：想象一下 1960 年的 IBM 计算机，大到填满整个房间，穿孔卡片从一端输入，答案从另一端输出。上文提到的神经网络以前三个调查问题的数据作为输入，得出预测结果；

    全局最小值：想象一下桌子上有一个黄色的碗（如上图所示）。桌子表面表示几乎零误差的完美预测结果，那么很显然碗底是最接近完美预测结果的位置，具备最小的误差。与碗整个表面（即「全局表面」（global surface））相比，碗底最接近完美，它具备全局最小误差值。


    反向传播：想象一位杂技表演者，他能向空中抛接 16 个不同大小和重量的保龄球瓶，并使它们同时悬在空中，甚至可以神奇地调整保龄球瓶的大小和重量。网络在执行预测后，会返回到上一次预测的过程中，查看是否可以做一些调整，以便在下一次预测中缩小误差，推动小球向碗底前进。

    梯度下降：想象粉色乒乓球沿着碗侧向碗底滚落，碗底即全局最小值（见上图）。网络就像那个球，碗的表面由网络的每一次预测构成。梯度下降就是球沿着碗侧滚落向碗底（即具备全局最小误差的预测）的过程。



    梯度下降是网络在达到准确预测（即全局最小误差）前的试错过程，就像乒乓球滚落碗底的过程；

    前馈即执行预测。预测就像给定时刻球停留在碗表面某个位置的定格图像；

    全局最小值即预测几乎没有误差的完美位置（碗底）。我们的目标是到达碗底。网络将预测结果与真值进行对比，来衡量球目前位置与碗底的距离（误差）；

    反向传播即返回到上一次预测，找出错误并修正。反向传播衡量球现在位置到其下桌面的距离（即误差），并找出推动球向碗底前进的方法。



Hopfield 网络是理论神经科学的一个模型，为分布式、内容可寻址的内存存储和检索提供了统一的框架，也启发了 Boltzmann 机的提出以及许多满足弱约束的分布作为 AI 计算模型的想法。


# 什么是神经网络
. 神经网络是一套特定的算法，它彻底改变了机器学习领域。他们受到生物神经网络的启发，目前深度神经网络已经被证实效果很好。
  神经网络本身是一般的函数逼近，这就是为什么它们几乎可以应用于任何从输入到输出空间复杂映射的机器学习问题


# 神经网络类型
  https://blog.csdn.net/weixin_40519315/article/details/104514507
. 前馈神经网络：第一层是输入，最后一层是输出。如果有多个隐藏层，我们称之为「深度」神经网络，前馈神经网络可以看做是一个
		函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射。前馈神经网络包括全连接前馈神经网络
		(FC)和卷积神经网络(CNN)
. 反馈型神经神经网络:和前馈神经网络相比，反馈神经网络中的神经元具有记忆功能，在不同时刻具有不同的状态。反馈神经网络中的
		    信息传播可以是单向也可以是双向传播，因此可以用一个有向循环图或者无向图来表示。常见的反馈神经网络包括
		    循环神经网络(RNN)、Hopfield网络和玻尔兹曼机
. 图网络: 前馈神经网络和反馈神经网络的输入都可表示为向量或者向量序列，但实际应用中很多数据都是图结构的数据，比如知识图谱、
          社交网络和分子网络等。这时就需要用到图网络来进行处理。图网络是定义在图结构数据上的神经网络，图中每个结点都由
          一个或者一组神经元组成。结点之前的连接可以是有向的，也可以是无向的。每个结点可以收到来自相邻结点或自身的信息


# 神经网络架构
  https://www.jiqizhixin.com/articles/2018-02-07-4
. 感知器
. 卷积神经网络
. 循环神经网络
. 长短期记忆网络
. Hopfield 网络
. 玻尔兹曼机网络
. 深度信念网络
. 深度自动编码器


# numpy 数学工具库
  import numpy as np

# matplotlib.pyplot 制作图表的库
  import matplotlib.pyplot as plt

# pandas 导入数据集和管理数据集的库
  import pandas as pd
  Pandas 基于两种数据类型，series 和 dataframe
  . series 是一种一维的数据类型，其中的每个元素都有各自的标签
  . dataframe 是一个二维的、表格型的数据结构。Pandas 的 dataframe 可以储存许多不同类型的数据，并且每个轴都有标签。
    你可以把它当作一个 series 的字典

# Feature Scaling
from sklearn.preprocessing import StandardScaler

# 交叉验证库  scikit-learn 安装时全名
from sklearn.cross_validation import train_test_split


# sklearn是机器学习中一个常用的python第三方模块，对常用的机器学习算法进行了封装，其中包括：
1.分类（Classification）
2.回归（Regression）
3.聚类（Clustering）
4.数据降维（Dimensionality reduction）
5.常用模型（Model selection）
6.数据预处理（Preprocessing）

# sklearn自带了很多数据集，可以用来对算法进行测试分析，免去了自己再去找数据集的烦恼，其中包括：
鸢尾花数据集:load_iris()
手写数字数据集:load_digitals()
糖尿病数据集:load_diabetes()
乳腺癌数据集:load_breast_cancer()
波士顿房价数据集:load_boston()
体能训练数据集:load_linnerud()


# statsmodels 估算许多不同统计模型以及进行统计测试和统计数据探索
  import statsmodels.api


# 过度学习，也称为过拟合，指在训练阶段

# 虚拟变量，也叫哑变量和离散特征编码，注意虚拟变量陷阱(完全多重共线性)...待理解
可用来表示分类变量、非数量因素可能产生的影响,不应包含所有虚拟变量的列
https://wiki.mbalib.com/wiki/虚拟变量陷阱
https://zhuanlan.zhihu.com/p/26640957
共线性=矩阵非满秩

在模型中引入多个虚拟变量时，虚拟变量的个数应按下列原则确定：
（1）如果回归模型有截距项
有m种互斥的属性类型，在模型中引入（m-1）个虚拟变量。
（2）如果回归模型无截距项，有m个特征，设置m个虚拟变量


# Splitting the dataset into the Training set and Test set
//cross_validatio sklearn v0.2 不再使用了，划分到了model_selection这个包中
//from sklearn.cross_validation import train_test_split  
from sklearn.model_selection import train_test_split


# X特征矩阵一直被视为矩阵，不能是向量


https://zh.wikipedia.org/wiki/%E5%B8%8C%E8%87%98%E5%AD%97%E6%AF%8D%E7%9A%84%E8%8B%B1%E8%AA%9E%E7%99%BC%E9%9F%B3
小写希腊字母
α   β   γ   δ   ϵ   ζ   η   θ   ι   κ   λ   μ   ν   ξ   ο   π   ρ   σ   τ   υ   ϕ   χ   ψ   ω
大写希腊字母
Α   Β   Γ   Δ   Ε   Ζ   Η   Θ   Ι   Κ   Λ   Μ   Ν  Ξ   Ο   Π   Ρ   Σ   Τ   Υ   Φ   Χ   Ψ   Ω

--------------------------------------------回归--------------------------------------------

# LinearRegression 线性回归类
from sklearn.linear_model import LinearRegression

. 假设函数 h(x)=θ1x+θ0; 真实数据点 y(i) 
. 代价函数(或者损失函数) J(θ0,θ1)=（h(x(i))-y(i))2 ， 是一个三维碗状曲面，等高线确定最小值，找出相应θ0，θ1参数
. 优化函数是尝试不同的参数θ0，θ1描绘出 h(xi) -y(i)误差平方和，使之最小

# 梯度下降
. 使用于代价函数更加复杂，参数更多，图像无法描绘情况
. θ0,θ1,θ2,θ3...θn，参数需要同时更新
. 阿尔法学习率，如果太小，收敛很慢； 如果太大代价函数可能不不会在每次迭代都下降
. 代价函数正常是逐渐下降，趋于平缓则收敛了

# 正规方程求解最优参数
. 不需要学习率
. 不需要迭代
. 但是对于特征n 很大时计算量是O(nxn3) 3次方倍  n>10000 就算大了,更倾向与梯度下降


# logistic分类器
. 不是用训练集来定义决策边界，而是用训练集拟合θ参数,有了θ参数就决定了决策边界
  假设函数形式是Logistic函数(也称Sigmoid函数)的变化
  https://zhuanlan.zhihu.com/p/28408516
. 假设函数形式是Logistic函数 不能用误差平方和这个优化函数，会是一个非凸函数，无法收敛到全局最小


# SVM与logistic分类器类似，也是一种二类分类模型
. from sklearn.svm import SVR
. SVR是SVM的一种运用，基本思路是一致，有细微的区别。使用SVR作回归分析，与SVM一样，我们需要找到一个超平面


# regression & classification 回归和分类的区别
. 回归: 目的 找到最优拟合
. 分类: 目的 寻找决策边界 属于监督学习

  

# https://zhuanlan.zhihu.com/p/33692660  //相关介绍
# SVR 可以选择不同内核
'linear' //线性
'poly',  //多项式
'rbf',   //高斯
'sigmoid', //
'precomputed' 
or a callable

# Dummy Variables 034 课程编号
																																																																																																																																																									

# LogisticRegression 逻辑回归类 本质是线性的,二分类,分割两个类别 (取逻辑回归这个名字由于历史传承原因)
from sklearn.linear_model import LogisticRegression


# KNeighborsClassifier 最临近算法类  非线性
from sklearn.neighbors import KNeighborsClassifier

# SVC 是 SVM支持向量机的对象分类器
from sklearn.svm import SVC

# DecisionTreeClassifier 是决策树的类
from sklearn.tree import DecisionTreeClassifier

# RandomForestClassifier 是随机森林的类
from sklearn.ensemble import RandomForestClassifier

--------------------------------------------回归--------------------------------------------





--------------------------------------------分类--------------------------------------------

# KMeans 均值聚类
  from sklearn.cluster import KMeans
  https://zh.wikipedia.org/wiki/K-%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95
  https://blog.csdn.net/m0_37490039/article/details/88943513
  . k-平均聚类要把n个观测划分到k个集合中(k≤n),使得组内平方和（WCSS within-cluster sum of squares）最小
  - k-均值算法效率很高的两个关键特征同时也被经常被视为它最大的缺陷
    . 聚类数目k是一个输入参数。选择不恰当的k值可能会导致糟糕的聚类结果,这也是为什么要进行特征检查来决定数据集的聚类数目了
    . 收敛到局部最优解，可能导致“反直观”的错误结果
    . k-均值算法的一个重要的局限性即在于它的聚类模型。这一模型的基本思想在于：得到相互分离的球状聚类，在这些聚类中，
      均值点趋向收敛于聚类中心。 一般会希望得到的聚类大小大致相当，这样把每个观测都分配到离它最近的聚类中心（即均值点）
      就是比较正确的分配方案
    . K-means 算法迭代步骤中的 每次确认中心点以后重新进行标记 对应 EM 算法中的 E 步 求当前参数条件下的 Expectation 
      而 根据标记重新求中心点 对应 EM 算法中的 M 步 求似然函数最大化时（损失函数最小时）对应的参数
      
# KMeans类的主要参数
  . n_clusters: 即我们的k值，一般需要多试一些值以获得较好的聚类效果, 用手肘法选择(需要人工看)，Gap statistic 方法
  . init： 即初始值选择的方式，可以为完全随机选择'random',优化过的'k-means++'或者自己指定初始化的k个质心。
          一般建议使用默认的k-means++：
          1.从样本点中随机选一个作为第一个聚类中心μ1
	  2.对于尚未选择的每个数据点x，计算D(x)，即x与已经选择的最接近中心之间的距离
	  3.使用加权概率分布随机选择一个新的数据点作为新的中心，原则是D(x)越大，概率越大。
	  4.重复2，3 直到挑选出K个中心点
  . n_init：用不同的初始化质心运行算法的次数。由于K-Means是结果受初始值影响的局部最优的迭代算法，因此需要多跑几次
            以选择一个较好的聚类效果，默认是10，一般不需要改。如果你的k值较大，则可以适当增大这个值。默认等于10
  . algorithm：有auto, full, elkan三种选择，full就是最大期望算法(EM算法)能够快速收敛于一个局部最优解，如果我们的优化
               目标是凸数据集，则EM算法可以保证收敛到全局极大值，这点和梯度下降法这样的迭代算法相同
               默认的auto则会根据数据值是否是稀疏的，来决定如何选择，一般数据是稠密的，那么就是 elkan，否则就是full

  . max_iter： 最大的迭代次数，一般如果是凸数据集的话可以不管这个值，如果数据集不是凸的，可能很难收敛，
              此时可以指定最大的迭代次数让算法可以及时退出循环，默认等于300
              
# 凸数据集与凸函数
  凸数据集: 通俗的讲，就是数据集D中任意两点的连线上的点，也在数据集D内，那么数据集D就是一个凸集 
  凸函数: 几何上，任意两点的连线在这两点函数部分的上面    

# hierarchy 分层聚类的寻找最优分类数的 dendrogram 工具
  import scipy.cluster.hierarchy as sch

# AgglomerativeClustering 聚合分层聚类
from sklearn.cluster import AgglomerativeClustering

--------------------------------------------分类--------------------------------------------




# Naive Bayes 天真的贝叶斯 (假设变量之间完全独立 没有关联)
基础算法，应用于机器学习 (即使变量或特征不是独立或非独立，通常也会应用它，仍能提供良好效果)


# 运行chatbot
python chat.py models/cornell_movie_dialog/trained_model_v1/best_weights_training.ckpt



    
--------------------------------------------TensorFlow--------------------------------------------

# tensorflow 中 tf.strided_slice 函数理解
'''
https://www.jianshu.com/p/78db75bd91ca
'''
t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],py
                 [[5, 5, 5], [6, 6, 6]]])


result = tf.strided_slice(t, [1, 0, 0], [2, 1, 3], [1, 1, 1])  # [[[3, 3, 3]]]
result = tf.strided_slice(t, [1, 0, 0], [2, 2, 3], [1, 1, 1])  # [[[3, 3, 3],
                                                      #   [4, 4, 4]]]
result = tf.strided_slice(t, [1, -1, 0], [2, -3, 3], [1, -1, 1])  # [[[4, 4, 4],
                                                         #   [3, 3, 3]]]


# DLSS（深度学习超级采样）


# C-ordered contiguous C连续数组
  https://blog.csdn.net/shinetzh/article/details/72782835
  https://blog.csdn.net/itnerd/article/details/105127984
  . C contiguous 指的是数组的行存储在内存中的地址是连续的
  . 与之对应的是 F contiguous，指的是数组的列存储在内存中的地址是连续的
  . Pascal, C，C++，Python 都是行优先存储，即内存中同行的元素存在一起
    而 Fortran，MatLab 是列优先存储，即内存中同列的元素存在一起
  . arr = np.arange(12).reshape(3,4)是 C连续数组(行连续数组)，arr.T转置后不再是行连续
  . arr2 = arr.T; arr2.reshape(12) NumPy会将arr2的值复制到新的内存块中（因为它无法将视图返回到该形状的原始数据）





































--------------------------------------------TensorFlow--------------------------------------------


    
    













