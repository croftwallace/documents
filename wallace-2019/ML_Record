
****************************关于神经网络、机器学习等的资料收录及自己理解**********************************

-1- 训练神经网络解决现实中的问题时之间的联系
	.达到最终目的前 统计一些影响最终目的先决条件-对应神经网络的特征
	.根据实践得出的结果 作为最终确定的结果，然后网络依据一些先决条件(对应神经元)和中间隐藏层(参数或其他调整)得出的一个关于结果的一个概率值，这个值越大说明此网络作出的判断 越贴近最终确定的结果
	.神经元和隐藏层之间连接(突触)，起思考的作用，影响最终的走向和最终偏离最终确定结果的概率

-2-	.神经网络将得出 0 到 1 之间的概率，与真值 1 对比，表示准确率
	.神经网络通过概率和试错学习方法，渐进式地改进下一次预测的结果

-3- 准确率提高的步骤
    前馈：想象一下 1960 年的 IBM 计算机，大到填满整个房间，穿孔卡片从一端输入，答案从另一端输出。上文提到的神经网络以前三个调查问题的数据作为输入，得出预测结果；

    全局最小值：想象一下桌子上有一个黄色的碗（如上图所示）。桌子表面表示几乎零误差的完美预测结果，那么很显然碗底是最接近完美预测结果的位置，具备最小的误差。与碗整个表面（即「全局表面」（global surface））相比，碗底最接近完美，它具备全局最小误差值。


    反向传播：想象一位杂技表演者，他能向空中抛接 16 个不同大小和重量的保龄球瓶，并使它们同时悬在空中，甚至可以神奇地调整保龄球瓶的大小和重量。网络在执行预测后，会返回到上一次预测的过程中，查看是否可以做一些调整，以便在下一次预测中缩小误差，推动小球向碗底前进。

    梯度下降：想象粉色乒乓球沿着碗侧向碗底滚落，碗底即全局最小值（见上图）。网络就像那个球，碗的表面由网络的每一次预测构成。梯度下降就是球沿着碗侧滚落向碗底（即具备全局最小误差的预测）的过程。



    梯度下降是网络在达到准确预测（即全局最小误差）前的试错过程，就像乒乓球滚落碗底的过程；

    前馈即执行预测。预测就像给定时刻球停留在碗表面某个位置的定格图像；

    全局最小值即预测几乎没有误差的完美位置（碗底）。我们的目标是到达碗底。网络将预测结果与真值进行对比，来衡量球目前位置与碗底的距离（误差）；

    反向传播即返回到上一次预测，找出错误并修正。反向传播衡量球现在位置到其下桌面的距离（即误差），并找出推动球向碗底前进的方法。



Hopfield 网络是理论神经科学的一个模型，为分布式、内容可寻址的内存存储和检索提供了统一的框架，也启发了 Boltzmann 机的提出以及许多满足弱约束的分布作为 AI 计算模型的想法。


#python 中索引从 0开始


#anaconda中启动spyder
在anaconda/bin目录下运行终端，输入./spyder即可启动

# numpy 数学工具库
import numpy as np

# matplotlib.pyplot 制作图表的库
import matplotlib.pyplot as plt

# pandas 导入数据集和管理数据集的库
import pandas as pd

# 交叉验证库  scikit-learn 安装时全名
from sklearn.cross_validation import train_test_split


#sklearn是机器学习中一个常用的python第三方模块，对常用的机器学习算法进行了封装，其中包括：
1.分类（Classification）
2.回归（Regression）
3.聚类（Clustering）
4.数据降维（Dimensionality reduction）
5.常用模型（Model selection）
6.数据预处理（Preprocessing）

#sklearn自带了很多数据集，可以用来对算法进行测试分析，免去了自己再去找数据集的烦恼，其中包括：
鸢尾花数据集:load_iris()
手写数字数据集:load_digitals()
糖尿病数据集:load_diabetes()
乳腺癌数据集:load_breast_cancer()
波士顿房价数据集:load_boston()
体能训练数据集:load_linnerud()

#LinearRegression 线性回归类
from sklearn.linear_model import LinearRegression


#statsmodels 估算许多不同统计模型以及进行统计测试和统计数据探索
import statsmodels.api


#过度学习，也称为过拟合，指在训练阶段

#虚拟变量，也叫哑变量和离散特征编码，注意虚拟变量陷阱(完全多重共线性)...待理解
可用来表示分类变量、非数量因素可能产生的影响,不应包含所有虚拟变量的列
https://wiki.mbalib.com/wiki/虚拟变量陷阱
https://zhuanlan.zhihu.com/p/26640957
共线性=矩阵非满秩

在模型中引入多个虚拟变量时，虚拟变量的个数应按下列原则确定：
（1）如果回归模型有截距项
有m种互斥的属性类型，在模型中引入（m-1）个虚拟变量。
（2）如果回归模型无截距项，有m个特征，设置m个虚拟变量


#Splitting the dataset into the Training set and Test set
//cross_validatio sklearn v0.2 不在使用了，划分到了model_selection这个包中
//from sklearn.cross_validation import train_test_split  
from sklearn.model_selection import train_test_split


# X特征矩阵一直被视为矩阵，不能是向量


#SVM与logistic分类器类似，也是一种二类分类模型
SVR是SVM的一种运用，基本的思路是一致，有细微的区别。使用SVR作回归分析，与SVM一样，我们需要找到一个超平面
from sklearn.svm import SVR

#https://zhuanlan.zhihu.com/p/33692660  //相关介绍
#SVR 可以选择不同内核
'linear' //线性
'poly',  //多项式
'rbf',   //高斯
'sigmoid', //
'precomputed' 
or a callable

# Dummy Variables 034 课程编号



# LogisticRegression 逻辑回归类 本质是线性的,二分类,分割两个类别
from sklearn.linear_model import LogisticRegression

# KNeighborsClassifier 最临近算法类  非线性
from sklearn.neighbors import KNeighborsClassifier

# SVC 是 SVM支持向量机的对象分类器
from sklearn.svm import SVC











